<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on CloudBees Technologists</title>
        <link>https://cb-technologists.github.io/posts/</link>
        <description>Recent content in Posts on CloudBees Technologists</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sat, 27 Jul 2019 10:50:46 -0400</lastBuildDate>
        <atom:link href="https://cb-technologists.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Technologists Lightning Talks for DevOps World | Jenkins World 2019 San Francisco</title>
            <link>https://cb-technologists.github.io/posts/lightning-talks-dw-jw-2019/</link>
            <pubDate>Sat, 27 Jul 2019 10:50:46 -0400</pubDate>
            
            <guid>https://cb-technologists.github.io/posts/lightning-talks-dw-jw-2019/</guid>
            <description>The Technologists will be giving a bunch of lightning talks in the DevOps Theater at this years DevOps World | Jenkins World in San Francisco. Some of the topics we will be covering include (many of which we have already blogged about):
 Self-Updating Jenkins: GitOps for Jenkins Configuration  This Lightning Talk will explore using GitOps to automate config updates for the CloudBees Jenkins Distribution.  Jenkins Plugin Management as Code  Let’s admit it, Jenkins Plugin management can be a pain.</description>
            <content type="html"><![CDATA[<p>The Technologists will be giving a bunch of lightning talks in the DevOps Theater at this years <a href="https://www.cloudbees.com/devops-world/san-francisco/">DevOps World | Jenkins World in San Francisco</a>. Some of the topics we will be covering include (many of which we have <a href="https://cb-technologists.github.io/posts/">already blogged about</a>):</p>

<ul>
<li><a href="https://cb-technologists.github.io/posts/cjd-casc/"><strong>Self-Updating Jenkins: GitOps for Jenkins Configuration</strong></a>

<ul>
<li>This Lightning Talk will explore using GitOps to automate config updates for the CloudBees Jenkins Distribution.</li>
</ul></li>
<li><a href="https://cb-technologists.github.io/posts/jenkins-plugins-good-bad-ugly/"><strong>Jenkins Plugin Management as Code</strong></a>

<ul>
<li>Let’s admit it, Jenkins Plugin management can be a pain. In this talk we will explore using CasC for Jenkins Plugin management. We will also get a preview of some exciting improvements around Plugin Management being built by CloudBees.</li>
</ul></li>
<li><strong>Multi-Cluster/Multi-Cloud/Hybrid Cloud</strong>

<ul>
<li>A brief overview and demonstration of up and coming Kubernetes multi-cluster capabilities in CloudBees Core.</li>
</ul></li>
<li><a href="https://cb-technologists.github.io/posts/jenkins-x-flow-integration/"><strong>Traditional Deployments with Jenkins X</strong></a>

<ul>
<li>Jenkins X isn’t just for Kubernetes deployments. In this talk, we’ll discuss how traditional deployments to non Kubernetes environments can be accomplished using Jenkins X and the power of CloudBees Flow.</li>
</ul></li>
<li><strong>CloudBees Jenkins X Distribution Means Stability for Native Kubernetes CD</strong>

<ul>
<li>Stability has been an issue for Jenkins X as it evolves quickly. This talk will provide an overview of how the CloudBees Jenkins X Distribution provides the stability that companies expect for their CD solution.</li>
</ul></li>
<li><a href="https://cb-technologists.github.io/posts/gitops-series-part-1/"><strong>GitOps for Jenkins Infrastructure</strong></a>

<ul>
<li>Overview of using GitOps to manage your Jenkins infrastructure as code. Using some popular open source tools, we can create a process for provisioning and managing the underlying infrastructure for running Jenkins on Kubernetes.</li>
</ul></li>
<li><strong>Multi Cluster Deployments with Jenkins X</strong>

<ul>
<li>When building and deploying applications in a Kubernetes based environment, a common requirement is the ability to isolate Development, Staging, and Production clusters which may have different security policies configured.  Jenkins X now has the ability to build in your development cluster but deploy your application to a separate cluster.</li>
</ul></li>
<li><strong>Safety First with Snyk and Jenkins</strong>

<ul>
<li>Take advantage of Synk’s dependency and docker vulnerability scanning in your pipelines. Fail builds for critical vulnerabilities. Go from insecure to informed in 15 minutes.</li>
</ul></li>
<li><strong>GitOps for Blogging, Why Not?</strong>

<ul>
<li>Jenkins X isn’t just for deploying micro-services to Kubernetes. The Technologists leverage Jenkins X to provide a GitOps approach to reviewing and deploying their Hugo based blog site.</li>
</ul></li>
</ul>

<p>The Technologists will also always be available in the CloudBees booth. If you are headed to DevOps World | Jenkins World in San Francisco, and we hope you are, we would love to discuss any of these topics, any thing that we have blogged about on this site or any emerging technologies related to continuous delivery with you. Just stop by the CloudBees booth and ask for a CloudBees Technologist.</p>

<p>We are looking forward to talking with you in San Francisco!</p>
]]></content>
        </item>
        
        <item>
            <title>Introduction to GitOps - Part 1</title>
            <link>https://cb-technologists.github.io/posts/gitops-series-part-1/</link>
            <pubDate>Tue, 16 Jul 2019 15:00:00 -0400</pubDate>
            
            <guid>https://cb-technologists.github.io/posts/gitops-series-part-1/</guid>
            <description>GitOps is a concept that was first coined by Weaveworks in their GitOps - Operations by Pull Request post. The idea itself wasn&amp;rsquo;t anything particularly new, people had been doing automated operations with infrastructure-as-code for years. But now that there was a descriptive new name for this concept, the DevOps community has really started to embrace it. Especially with the ever growing prevalence of Kubernetes.
If you haven&amp;rsquo;t already done so, I&amp;rsquo;d recommend reading that Weaveworks post since it is always good to understand the origination of a concept.</description>
            <content type="html"><![CDATA[

<p>GitOps is a concept that was first coined by Weaveworks in their <a href="https://www.weave.works/blog/gitops-operations-by-pull-request">GitOps - Operations by Pull Request</a> post. The idea itself wasn&rsquo;t anything particularly new, people had been doing automated operations with infrastructure-as-code for years. But now that there was a descriptive new name for this concept, the DevOps community has really started to embrace it. Especially with the ever growing prevalence of Kubernetes.</p>

<p><img src="/img/gitops-series/part-1/trend.png" alt="GitOps Trend" /></p>

<p>If you haven&rsquo;t already done so, I&rsquo;d recommend reading that Weaveworks post since it is always good to understand the origination of a concept. But simply put, GitOps is a way for you to manage your operations from a source code repo. With GitOps you won&rsquo;t be doing any manual steps when you want to change something. On a commit to your master branch, a job will get kicked off and will make the necessary changes.</p>

<h2 id="why-would-you-want-this">Why would you want this?</h2>

<p>If you have any experience on an operations team with little or no automation, you no doubt know the frustration of manual configuration and snowflake servers. In this sort of environment it is easy to quickly get overwhelmed. And when things go wrong, they can spin out of control.</p>

<p>As you move to infrastructure-as-code by using configuration management tools you&rsquo;re able to get away from most of that headache. Now that you have code which describes your desired environment state you have an easy way to manage and maintain the environment. When you need to change something, submit a pull request and have someone review it. Once the change is merged, go ahead and run the automation and the change will propogate. Should something disastrous happen to your environment, you can get your environment back up in no time by rerunning the automation.</p>

<p>But you are still missing something if these commits to master don&rsquo;t automatically kick off a job to make the change. You should always want your environment to 100% match the configuration in your repo. If the automation isn&rsquo;t automatically run, you will drift away from the target state.</p>

<p>I&rsquo;ve personally been guilty of putting off running automation due to fear of something breaking, and I know I&rsquo;m not alone in this. Knowing that merging your pull request is going to trigger a job makes you more careful in your review of pull requests but also gives you confidence in knowing that your environment is always up-to-date.</p>

<h2 id="objective-of-this-series">Objective of this series</h2>

<p>To explore the idea of GitOps I am writing a 3-part series where we&rsquo;ll be building out a fully-functional GitOps process.</p>

<p>In this first part we will take a look at building out the infrastructure automation piece. This will involve provisioning a Kubernetes cluster, setting up certificates and DNS, and more. From here we will fork into two different directions.</p>

<p>In the second part we will add the automation of our <a href="https://www.cloudbees.com/products/cloudbees-jenkins-distribution">CloudBees Jenkins Distribution</a>. This will include plugin management, configuration, and more.</p>

<p>In the final part we will look at <a href="https://www.cloudbees.com/products/cloudbees-core">CloudBees Core</a> and some cool stuff we can do with custom Operations Center and Managed Master images.</p>

<p>Overall, while the goal of this series is educational, I hope it is also useful and that the assets are useable. As I write this I am using this automation daily to make my life easier as I play around with new features and try new configurations.</p>

<p>By necessity the resulting assets are based on my configuration and preferences. While I am trying to keep things as generic as possible, some things like my use of GKE might differ from your situation. The ideas and processes should be transferable to your environment.</p>

<h1 id="time-to-get-down-to-business">Time to get down to business</h1>

<p>With that background out of the way, let&rsquo;s dive right in.</p>

<h2 id="what-s-the-plan">What&rsquo;s the plan?</h2>

<p><img src="/img/gitops-series/part-1/plan.svg" alt="Plan of attack" /></p>

<ol>
<li>Pull latest changes - we&rsquo;ll leave this to Jenkins</li>
<li>Kubernetes cluster - for this I have decided to use <a href="https://www.terraform.io/">Terraform</a> to provision/manage the Kubernetes cluster. We&rsquo;ll be using <a href="https://cloud.google.com/kubernetes-engine/">(GKE) Google Kubernetes Engine</a> since it is my favorite managed Kubernetes platform</li>
<li>Namespace and permissions - will use <code>kubectl</code> to handle this</li>
<li>Ingress controller - Will use the recommended nginx ingress controller</li>
<li>Set DNS record - Will take advantage of the Ansible role to do this easily</li>
<li>Cert manager - Will use Kubectl to install</li>
<li>Ensure CJD/Core are running - This is where we will fork into the next 2 posts</li>
</ol>

<h3 id="already-there-is-an-issue">Already there is an issue</h3>

<p>In order to have a GitOps process that works, we need to have something to actually kick off the jobs. In this case we&rsquo;re going to be using Jenkins, but we don&rsquo;t have it up yet.</p>

<p>There are a couple of ways we could handle this:</p>

<ol>
<li>Have a Jenkins server running outside of the Kubernetes cluster (where is the fun in that?)</li>
<li>Create a seed script which will run everything the first time and setup the Jenkins server on the cluster we just created</li>
</ol>

<p>Since I am trying to minimize the number of things we need to manage, we&rsquo;ll be going with #2.</p>

<h2 id="creating-the-repo">Creating the repo</h2>

<p>In order to keep the separations of concerns pretty straightforward, I&rsquo;ve got the structure of the repo looking like this:</p>

<pre><code>.
├── ansible &lt;- Our ansible playbook will live here
├── cert-manager &lt;- The cert manager configuration lives here
├── scripts &lt;- All other scripts (including our seed script) live here
└── terraform &lt;- The terraform configuration lives here
</code></pre>

<h2 id="provisioning-the-kubernetes-cluster">Provisioning the Kubernetes cluster</h2>

<p>The first obvious step in building out our project is to be able to spin up the Kubernetes cluster, since after all, that is the platform everything will be running on. For this task I&rsquo;ve chosen to use Terraform for it&rsquo;s quick and easy way to provision cloud resources in an <a href="https://en.wikipedia.org/wiki/Idempotence">idempotent</a> fashion.</p>

<p>Specifically we&rsquo;ll use the Google Kubernetes Engine (GKE) <a href="https://www.terraform.io/docs/providers/google/r/container_cluster.html">provisioner</a>.</p>

<p>If you don&rsquo;t have any experience with Terraform, they have a good <a href="https://learn.hashicorp.com/terraform/">learning site</a> where you can get started. The scope of what we&rsquo;ll be doing is rather limited, so if you don&rsquo;t have any prior experience don&rsquo;t worry, it should be simple enough to follow and understand.</p>

<p>At a minimum you will want to have Terraform <a href="https://learn.hashicorp.com/terraform/getting-started/install">installed locally</a>.</p>

<h3 id="variables-file">Variables file</h3>

<p>When using Terraform I like to split out all of the variables into a separate variables file. This makes it easier when making changes to see all settings at once.</p>

<p>Inside of our <code>terraform/</code> directory will start by creating a <code>variables.tf</code> file.</p>

<p>In Terraform, a variable looks like this:</p>

<pre><code class="language-terraform">variable &quot;cluster_name&quot; {
  default = &quot;ld-cluster-1&quot;
}
</code></pre>

<p>This can then be referenced like this: <code>&quot;${var.cluster_name}&quot;</code></p>

<p>The <code>terraform/variables.tf</code> file is going to look like this:</p>

<pre><code class="language-terraform">variable &quot;project&quot; {
  default = &quot;myproject&quot;
}

variable &quot;region&quot; {
  default = &quot;us-east1-b&quot;
}

variable &quot;cluster_name&quot; {
  default = &quot;my-cluster-name&quot;
}

variable &quot;cluster_zone&quot; {
  default = &quot;us-east1-b&quot;
}

variable &quot;cluster_k8s_version&quot; {
  default = &quot;1.13.6-gke.13&quot;
}

variable &quot;initial_node_count&quot; {
  default = 1
}

variable &quot;autoscaling_min_node_count&quot; {
  default = 1
}

variable &quot;autoscaling_max_node_count&quot; {
  default = 5
}

variable &quot;disk_size_gb&quot; {
  default = 100
}

variable &quot;disk_type&quot; {
  default = &quot;pd-standard&quot;
}

variable &quot;machine_type&quot; {
  default = &quot;n1-standard-2&quot;
}
</code></pre>

<p>Since this is where we are setting the environment specific variables, go ahead and replace those with your own desired state. You&rsquo;ll most likely want to adjust <code>project</code>, <code>region</code>, <code>cluster_zone</code>, and <code>cluster_name</code>. There is also a chance that as you read this the <code>cluster_k8s_version</code> I have listed here is no longer available, so you may need to update that.</p>

<h3 id="cluster-definition-file">Cluster definition file</h3>

<p>Now with the variables out of the way, it&rsquo;s time to build out the actual definition of what the cluster is going to look like. This is the stuff that isn&rsquo;t likely to change as much. If you&rsquo;re following along you shouldn&rsquo;t need to make any changes except for one specific spot I&rsquo;ll point out.</p>

<p>We&rsquo;re going to create a <code>cluster.tf</code> file.</p>

<p>It&rsquo;s going to look like this: <code>terraform/cluster.tf</code></p>

<pre><code class="language-terraform">provider &quot;google&quot; {
  project = &quot;${var.project}&quot;
  region  = &quot;${var.region}&quot;
}

# Change this section
terraform {
  backend &quot;gcs&quot; {
    bucket  = &quot;my-unique-bucket&quot;
    prefix  = &quot;terraform/state&quot;
    project = &quot;my-project&quot;
  }
}

resource &quot;google_container_cluster&quot; &quot;cluster&quot; {
  name               = &quot;${var.cluster_name}&quot;
  location           = &quot;${var.cluster_zone}&quot;
  min_master_version = &quot;${var.cluster_k8s_version}&quot;

  addons_config {
    network_policy_config {
      disabled = true
    }

    http_load_balancing {
      disabled = false
    }

    kubernetes_dashboard {
      disabled = false
    }
  }

  node_pool {
    name               = &quot;default-pool&quot;
    initial_node_count = &quot;${var.initial_node_count}&quot;

    management {
      auto_repair = true
    }

    autoscaling {
      min_node_count = &quot;${var.autoscaling_min_node_count}&quot;
      max_node_count = &quot;${var.autoscaling_max_node_count}&quot;
    }

    node_config {
      preemptible  = false
      disk_size_gb = &quot;${var.disk_size_gb}&quot;
      disk_type    = &quot;${var.disk_type}&quot;

      machine_type = &quot;${var.machine_type}&quot;

      oauth_scopes = [
        &quot;https://www.googleapis.com/auth/devstorage.read_only&quot;,
        &quot;https://www.googleapis.com/auth/logging.write&quot;,
        &quot;https://www.googleapis.com/auth/monitoring&quot;,
        &quot;https://www.googleapis.com/auth/service.management.readonly&quot;,
        &quot;https://www.googleapis.com/auth/servicecontrol&quot;,
        &quot;https://www.googleapis.com/auth/trace.append&quot;,
        &quot;https://www.googleapis.com/auth/compute&quot;,
        &quot;https://www.googleapis.com/auth/cloud-platform&quot;
      ]

    }
  }
}

output &quot;client_certificate&quot; {
  value     = &quot;${google_container_cluster.cluster.master_auth.0.client_certificate}&quot;
  sensitive = true
}

output &quot;client_key&quot; {
  value     = &quot;${google_container_cluster.cluster.master_auth.0.client_key}&quot;
  sensitive = true
}

output &quot;cluster_ca_certificate&quot; {
  value     = &quot;${google_container_cluster.cluster.master_auth.0.cluster_ca_certificate}&quot;
  sensitive = true
}

output &quot;host&quot; {
  value     = &quot;${google_container_cluster.cluster.endpoint}&quot;
  sensitive = true
}
</code></pre>

<p>This may look complicated, but really all we are doing is defining the configuration of the cluster we want to provision. As you can see we are taking full use of the variables we listed in the <code>variables.tf</code> file.</p>

<p>There is one section you will need to modify, it is the following block:</p>

<pre><code class="language-terraform">terraform {
  backend &quot;gcs&quot; {
    bucket  = &quot;my-unique-bucket&quot;
    prefix  = &quot;terraform/state&quot;
    project = &quot;my-project&quot;
  }
}
</code></pre>

<p>By default, when you are using Terraform it stores the state of your environment to the local system. Since we are going to be running this from Jenkins in an ephemeral agent, we don&rsquo;t want this. Instead, this block tells Terraform to store the state to a GCS storage bucket so the state will persist between runs.</p>

<p>If you&rsquo;re following along, you can follow these <a href="https://cloud.google.com/storage/docs/creating-buckets">instructions</a> to create a GCS bucket here.</p>

<h3 id="testing-it-out">Testing it out</h3>

<p>With this configuration all set, we are ready to test it out and see if we can provision a GKE cluster using terraform.</p>

<p>If you <code>cd terraform</code> to change to that directory, you can initialize the Terraform project and pull the requisite plugins by running <code>terraform init</code>. You can then run <code>terraform plan</code> to see the plan that gets generated by Terraform.</p>

<p>If all looks good, you can go ahead and run <code>terraform apply</code>. Unless you specify a specific flag, it is going to prompt you whether you want to perform the actions or not.</p>

<p>Go ahead and type <code>yes</code> when you&rsquo;re ready, then the provisioning process will begin. This should take a few minutes to complete since it has to spin up and configure quite a few resources.</p>

<p>Once the cluster is up and ready to go, we can move on to the next steps.</p>

<h2 id="setting-up-namespace-and-permissions">Setting up namespace and permissions</h2>

<p>These steps we&rsquo;re performing will be put into a script since automation is our goal, but I&rsquo;m going to run through them manually the first time so we can understand what is going on.</p>

<p>First we need to connect to the Kubernetes cluster we created. This is easiest done by running (with your specific parameters):</p>

<p><code>gcloud container clusters get-credentials MYCLUSTER --zone MYZONE --project MYPROJECT</code></p>

<p>You can verify that you&rsquo;re connected by running a kubectl command like <code>kubectl get nodes</code>.</p>

<h3 id="assigning-cluster-admin-role">Assigning cluster-admin role</h3>

<p>Certain components of our setup will need cluster-admin role access so we can easily set that up by running:</p>

<p><code>kubectl create clusterrolebinding cluster-admin-binding  --clusterrole cluster-admin  --user $(gcloud config get-value account)</code></p>

<h3 id="create-the-namespaces">Create the namespaces</h3>

<p>Next we will want to create a namespace for Core or CJD to live in.</p>

<pre><code class="language-bash">kubectl create namespace core
kubectl label namespace core name=core
kubectl config set-context $(kubectl config current-context) --namespace=core
</code></pre>

<p>If you&rsquo;re familiar with kubectl you might be aware that we are going to get an error on subsequent runs of the <code>kubectl create</code> command since it will already exist. We will need to take care of that as part of the Jenkinsfile.</p>

<h2 id="setup-the-ingress-controller">Setup the ingress controller</h2>

<p>In order to get traffic into an application running in Kubernetes we will need to create ingresses for each application. It turns out that manually doing this is a bit of a pain, so the Kubernetes community created the <a href="https://kubernetes.github.io/ingress-nginx/">NGINX Ingress Controller</a> which will do most of the work for us.</p>

<p>There are several different ways to install this, including a simple helm install, all of which can be found <a href="https://kubernetes.github.io/ingress-nginx/deploy/">here</a>.</p>

<p>To avoid having to manage anything else (i.e. helm), I&rsquo;ve opted to just use the yaml file install.</p>

<pre><code class="language-bash">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.24.1/deploy/mandatory.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.24.1/deploy/provider/cloud-generic.yaml
</code></pre>

<p>An important note about this is that it takes several seconds to provision and attach the public ip address to the service. We will need to handle this in the Jenkinsfile.</p>

<h2 id="set-dns-record">Set DNS Record</h2>

<p>Now that we have a public ip address, we can point our domain at it. This is one of those problems that you can tackle 100 different ways. The simplest way is probably to make an api call to your DNS host to update a particular record with the ip address.</p>

<p>I&rsquo;m going to make it a little more complicated in order to make it easier to switch between different DNS hosts.</p>

<p>I&rsquo;ve setup an <a href="https://github.com/ansible/ansible">Ansible</a> playbook which takes advantage of the pre-built DNS provider modules.</p>

<p>Here are the <a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html">Ansible install instructions</a>. If you&rsquo;ve got <code>pip</code> on your system you can simply run <code>pip install ansible</code>.</p>

<p>I created a file <code>ansible/dns.yml</code> which contains the following:</p>

<pre><code class="language-yaml">- hosts: localhost
  tasks:
  - name: Create a managed zone
    gcp_dns_managed_zone:
      name: &quot;my-zone-name&quot;
      dns_name: &quot;my-domain-name.com&quot;
      description: My playground
      project: my-project
      state: present
      auth_kind: serviceaccount
    register: managed_zone

  - name:  Create an A record to point to the Core instance
    gcp_dns_resource_record_set:
      managed_zone: &quot;{{ managed_zone }}&quot;
      name: &quot;core.my-domain-name.com.&quot;
      type: A
      target: 
        - &quot;{{ target_ip }}&quot;
      project: my-project
      auth_kind: serviceaccount
</code></pre>

<p>What we are doing here is taking advantage of the gcp_dns modules (<a href="https://docs.ansible.com/ansible/latest/modules/gcp_dns_managed_zone_module.html"><code>gcp_dns_managed_zone</code></a> &amp; <a href="https://docs.ansible.com/ansible/latest/modules/gcp_dns_resource_record_set_module.html"><code>gcp_dns_resource_record_set</code></a>) to easily set the DNS.</p>

<p>The nice thing about this is should you need to use another DNS host like <a href="https://www.cloudflare.com/">Cloudflare</a> you can easily transition over using the right <a href="https://docs.ansible.com/ansible/latest/modules/cloudflare_dns_module.html#cloudflare-dns-module">module</a>.</p>

<p>Once that is configured, you can run the playbook with (setting the TARGET_IP according to your cluster&rsquo;s ip):</p>

<p><code>ansible-playbook ansible/dns.yml -e target_ip=${TARGET_IP}</code></p>

<h2 id="setting-up-cert-manager">Setting up Cert Manager</h2>

<p>Now we have our ingress controller which has given us a public ip address and we have setup a DNS record to point to the address. We could go ahead and install Core or CJD at this point if we wanted, but we might as well setup SSL certificates to make things more secure.</p>

<p>We&rsquo;re going to use <a href="https://letsencrypt.org/">Let&rsquo;s Encrypt</a> Certificate Authority in order to generate the certs. To do this in an easy and automated fashion, we&rsquo;ll use <a href="https://github.com/jetstack/cert-manager">cert-manager</a>.</p>

<p>Installing it is pretty easy, and you can find the most update instructions <a href="https://docs.cert-manager.io/en/latest/getting-started/install/kubernetes.html">here</a>.</p>

<pre><code class="language-bash">kubectl create namespace cert-manager
kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true
kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.8.0/cert-manager.yaml
</code></pre>

<p>The above commands create a namespace for cert-manager and then deploy the cert-manager application in there.</p>

<p>Next we&rsquo;ll need to create some cert issuers to allow us to actually generate the certs. We&rsquo;ll create two of them in the cert-manager directory.</p>

<p><code>cert-manager/staging-issuer.yaml</code>:</p>

<pre><code class="language-yaml">   apiVersion: certmanager.k8s.io/v1alpha1
   kind: Issuer
   metadata:
     name: letsencrypt-staging
   spec:
     acme:
       # The ACME server URL
       server: https://acme-staging-v02.api.letsencrypt.org/directory
       # Email address used for ACME registration
       email: myemail@example.com # change this
       # Name of a secret used to store the ACME account private key
       privateKeySecretRef:
         name: letsencrypt-staging
       # Enable the HTTP-01 challenge provider
       http01: {}
</code></pre>

<p><code>cert-manager/production-issuer.yaml</code>:</p>

<pre><code class="language-yaml">   apiVersion: certmanager.k8s.io/v1alpha1
   kind: Issuer
   metadata:
     name: letsencrypt-prod
   spec:
     acme:
       # The ACME server URL
       server: https://acme-v02.api.letsencrypt.org/directory
       # Email address used for ACME registration
       email: myemail@example.com
       # Name of a secret used to store the ACME account private key
       privateKeySecretRef:
         name: letsencrypt-prod
       # Enable the HTTP-01 challenge provider
       http01: {}
</code></pre>

<p>The reason we have two of these is because Let&rsquo;s Encrypt has a rate-limiter on how often you can generate certificates. So while you are experimenting with things, it is safer to use the staging issuer. When things are all sorted, you can switch to the production-issuer.</p>

<p>Go ahead and apply these two issuers with:</p>

<pre><code class="language-bash">kubectl apply -f cert-manager/staging-issuer.yaml
kubectl apply -f cert-manager/production-issuer.yaml
</code></pre>

<p>Now when we create an ingress for our applications, we can add some metadata to the ingress definition and the certificates will automatically be generated and stored as <a href="https://kubernetes.io/docs/concepts/configuration/secret/">K8s Secrets</a>.</p>

<h2 id="setting-up-cloudbees-core-or-cjd">Setting up CloudBees Core or CJD</h2>

<p>The final step in this flow is to either setup Core or CJD. We will add this portion, and more in the following posts.</p>

<p>But before concluding this post, let&rsquo;s take a look at how we can put all of these steps into a Jenkinsfile.</p>

<h2 id="putting-it-all-together">Putting it all together.</h2>

<p>Since the whole objective here was to automate this process, it makes sense to use Jenkins to run the process. We can easily achieve GitOps by having every commit to master kick off our pipeline here.</p>

<p>You&rsquo;ll note that we&rsquo;ve added a couple of dependencies along the way that we&rsquo;ll need to make sure Jenkins will have access to. Thankfully, since we&rsquo;ll be running on Kubernetes, we can take advantage of the ephemeral, container-based agents. We can define a <a href="https://jenkins.io/doc/pipeline/steps/kubernetes/">pod template</a> which will describe all of the containers we will need.</p>

<p>In the root directory of my repo, I have created a <code>pod-template.yml</code> file:</p>

<pre><code class="language-yaml">kind: Pod
metadata:
  name: gitops-pod
spec:
  containers:
  - name: terraform
    image: hashicorp/terraform:light
    command:
    - cat
    tty: true
    volumeMounts:
      - name: gcp-credential
        mountPath: /root/
    env:
      - name: GOOGLE_CLOUD_KEYFILE_JSON
        value: &quot;/root/gcp-service.json&quot;
      - name: GCP_SERVICE_ACCOUNT_FILE
        value: &quot;/root/gcp-service.json&quot;
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: &quot;/root/gcp-service.json&quot;
  - name: ansible
    image: ldonleycb/ansible-ci:new
    command:
    - cat
    tty: true
    volumeMounts:
      - name: gcp-credential
        mountPath: /root/
    env:
      - name: GOOGLE_CLOUD_KEYFILE_JSON
        value: &quot;/root/gcp-service.json&quot;
      - name: GCP_SERVICE_ACCOUNT_FILE
        value: &quot;/root/gcp-service.json&quot;
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: &quot;/root/gcp-service.json&quot;
      - name: GCP_PROJECT
        value: &quot;my_project&quot;
      - name: GCP_CLUSTER_NAME
        value: &quot;my_cluster_name&quot;
  - name: kubectl
    image: google/cloud-sdk:252.0.0-slim
    command:
    - cat
    tty: true
    volumeMounts:
      - name: gcp-credential
        mountPath: /home/
    env:
      - name: GOOGLE_CLOUD_KEYFILE_JSON
        value: &quot;/home/gcp-service.json&quot;
      - name: GCP_SERVICE_ACCOUNT_FILE
        value: &quot;/home/gcp-service.json&quot;
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: &quot;/home/gcp-service.json&quot;
      - name: GCP_PROJECT
        value: &quot;my_project&quot;
      - name: GCP_CLUSTER_NAME
        value: &quot;my_cluster_name&quot;
  volumes:
    - name: gcp-credential
      secret:
        secretName: gcp-credential
</code></pre>

<p>This looks complicated, but it is mostly just bloated by the array of environment variables we need for Google Cloud operations.</p>

<p>The <code>Jenkinsfile</code> in the root directory will look something like this:</p>

<pre><code class="language-groovy">pipeline {
  agent {
    kubernetes {
      label 'gitops'
      yamlFile 'pod-template.yml'
    }
  }
  stages {
    stage('Terraform') {
      steps {
        container('terraform'){
          sh '''
            cd terraform
            terraform init
            terraform apply -input=false -auto-approve
            cd ..
          '''
        }
      }
    }
    stage('Setup ingress controller and namespace') {
      steps {
        container('kubectl'){
          script {
            sh '''
              gcloud auth activate-service-account --key-file=$GCP_SERVICE_ACCOUNT_FILE
              gcloud container clusters get-credentials $GCP_CLUSTER_NAME --zone us-east1-b --project $GCP_PROJECT
            '''
            try {
              sh '''
                kubectl create clusterrolebinding cluster-admin-binding  --clusterrole cluster-admin  --user $(gcloud config get-value account)
              '''
            }
            catch(error) {
              sh &quot;echo ''&quot;
            }

            sh '''
              kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.24.1/deploy/mandatory.yaml
              kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.24.1/deploy/provider/cloud-generic.yaml
              sleep 60s
            '''
            try {
              sh '''
                kubectl create namespace core
                kubectl label namespace core name=core
              '''
            }
            catch(error) {
              sh &quot;echo ''&quot;
            }
            sh '''
              kubectl config set-context $(kubectl config current-context) --namespace=core
            '''
            env.TARGET_IP = sh(returnStdout: true, script: 'kubectl get service ingress-nginx -n ingress-nginx | awk \'END {print $4}\'').trim()
          } 
        }
      }
    }
    stage('Setup DNS') {
      steps {
        container('ansible'){
          sh &quot;&quot;&quot;
            ansible-playbook ansible/dns.yml -e target_ip=${env.TARGET_IP}
          &quot;&quot;&quot;
        }

      }
    }
    stage('Setup cert-manager') {
      steps {
        container('kubectl'){
          sh '''# Install cert-manager
              kubectl create namespace cert-manager
              kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true
              kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.8.0/cert-manager.yaml

              sleep 30s

              # Add cert-manager issuers
              kubectl apply -f cert-manager/staging-issuer.yaml
              kubectl apply -f cert-manager/production-issuer.yaml
            '''
        }
      }
    }
  }
}
</code></pre>

<p>This is not a particularly elegant solution at this point, but for an initial attempt it should be sufficient.</p>

<p>In the next parts of this series we will be taking a look at how to extend this to actually deploy and maintain CloudBees Core or CloudBees Jenkins Distribution.</p>
]]></content>
        </item>
        
        <item>
            <title>Self-Updating Jenkins: GitOps for Jenkins Configuration</title>
            <link>https://cb-technologists.github.io/posts/cjd-casc/</link>
            <pubDate>Wed, 03 Jul 2019 17:00:00 -0400</pubDate>
            
            <guid>https://cb-technologists.github.io/posts/cjd-casc/</guid>
            <description>In this blog post, we&amp;rsquo;ll walk through creating a self-updating instance of the CloudBees Jenkins Distribution, with all configuration stored as code in a GitHub repository.
We&amp;rsquo;ll deploy the CJD master as a StatefulSet in a Kubernetes cluster, configure the master using the Jenkins Configuration as Code plugin, and set up a TLS certificate through cert-manager. Finally, we&amp;rsquo;ll seed a Pipeline job that updates the master upon commit to the Git repository that contains the configuration - enabling GitOps for Jenkins itself.</description>
            <content type="html"><![CDATA[

<p>In this blog post, we&rsquo;ll walk through creating a self-updating instance of the <a href="https://www.cloudbees.com/products/cloudbees-jenkins-distribution">CloudBees Jenkins Distribution</a>, with all configuration stored as code in a GitHub repository.</p>

<p>We&rsquo;ll deploy the CJD master as a <code>StatefulSet</code> in a Kubernetes cluster, configure the master using the <a href="https://github.com/jenkinsci/configuration-as-code-plugin">Jenkins Configuration as Code plugin</a>, and set up a TLS certificate through <a href="https://github.com/jetstack/cert-manager">cert-manager</a>. Finally, we&rsquo;ll seed a Pipeline job that updates the master upon commit to the <a href="https://github.com/cb-technologists/cjd-casc">Git repository</a> that contains the configuration - enabling GitOps for Jenkins itself.</p>

<h2 id="deploying-cloudbees-jenkins-distribution-in-kubernetes">Deploying CloudBees Jenkins Distribution in Kubernetes</h2>

<p>First, we&rsquo;ll need to deploy a Jenkins instance into a Kubernetes cluster. In this case, we&rsquo;ll use <a href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a> to deploy a containerized version of CJD. To provision a cluster, we&rsquo;ll follow the Google Cloud documentation <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster">here</a>. (<strong>Note:</strong> this blog post assumes prior installation of and basic familiarity with using <code>kubectl</code> to interact with a Kubernetes cluster.)</p>

<p>Once the cluster has been provisioned and <code>kubectl</code> has been configured, we&rsquo;ll create a dedicated <code>namespace</code> for our CJD resources and update our <code>kubectl config</code> to use it by default:</p>

<pre><code class="language-bash">kubectl create namespace cjd
kubectl config set-context $(kubectl config current-context) --namespace cjd
</code></pre>

<p>We&rsquo;ll also need to ensure an ingress controller is deployed within the cluster. For this post, we&rsquo;ll assume the use of the <a href="https://kubernetes.github.io/ingress-nginx/">NGINX ingress controller</a>. Following the <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md">Installation Guide</a>, we&rsquo;ll manually deploy using a few <code>kubectl</code> commands:</p>

<pre><code class="language-bash"># grant cluster-admin to user
kubectl create clusterrolebinding cluster-admin-binding \ --clusterrole cluster-admin \ --user $(gcloud config get-value account)
# deploy nginx ingress controller resources
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud-generic.yaml
</code></pre>

<p>Next, let&rsquo;s look at the manifest file that will deploy the necessary resources for CJD using the <a href="https://github.com/cb-technologists/cjd-casc/blob/master/cjd.yaml">cjd.yaml</a> manifest file.</p>

<p>First, we create a <code>ServiceAccount</code>, a <code>Role</code> with the necessary permissions to manage agents and perform the required update actions, and a <code>RoleBinding</code> to connect the two.</p>

<pre><code class="language-yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  name: cjd

---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: cjd
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;pods&quot;,&quot;configmaps&quot;,&quot;services&quot;,&quot;serviceaccounts&quot;]
  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]
- apiGroups: [&quot;&quot;]
  resources: [&quot;pods/exec&quot;]
  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]
- apiGroups: [&quot;&quot;]
  resources: [&quot;pods/log&quot;]
  verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;]
- apiGroups: [&quot;&quot;]
  resources: [&quot;secrets&quot;]
  verbs: [&quot;get&quot;]
- apiGroups: [&quot;apps&quot;]
  resources: [&quot;statefulsets&quot;]
  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]
- apiGroups: [&quot;rbac.authorization.k8s.io&quot;]
  resources: [&quot;roles&quot;,&quot;rolebindings&quot;]
  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]
- apiGroups: [&quot;extensions&quot;]
  resources: [&quot;ingresses&quot;]
  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: cjd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cjd
subjects:
- kind: ServiceAccount
  name: cjd
</code></pre>

<p>Next, we create a <code>Service</code> that exposes ports for access to the CJD web interface and for master-agent communication:</p>

<pre><code class="language-yaml">---
apiVersion: v1
kind: Service
metadata:
  name: cjd
spec:
  selector:
    app: cjd
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  - name: agent
    port: 50000
    protocol: TCP
    targetPort: 50000
  type: ClusterIP
</code></pre>

<p>Next, we set up an <code>Ingress</code> to allow access to our CJD instance from outside of the cluster. We&rsquo;ll examine this in more detail in a later section where we walk through the setup of <code>cert-manager</code>.</p>

<pre><code class="language-yaml">---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: cjd
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    certmanager.k8s.io/issuer: &quot;letsencrypt-prod&quot; # add after cert-manager deploy
    certmanager.k8s.io/acme-challenge-type: http01 # add after cert-manager deploy
spec:
  tls: # cert-manager
  - hosts: # cert-manager
    - cjd.cloudbees.elgin.io # cert-manager
    secretName: cjd-tls # cert-manager
  rules:
  - host: cjd.cloudbees.elgin.io
    http:
      paths:
      - path: /
        backend:
          serviceName: cjd
          servicePort: 80
</code></pre>

<p>We&rsquo;ll also need to make sure that we create a DNS A Record through our hosting provider that maps our <code>host</code> URL to the <code>EXTERNAL-IP</code> of our ingress controller. We can get that IP after deploying our NGINX ingress controller by running:</p>

<pre><code class="language-bash">kubectl get svc -n ingress-nginx
</code></pre>

<p>Finally, we provision the <code>StatefulSet</code> that controls the CJD Pod and <code>PersistentVolumeClaim</code>. The container image we use here is a custom image inheriting from the <a href="https://hub.docker.com/r/cloudbees/cloudbees-jenkins-distribution/">official CJD Docker image</a>. We&rsquo;ll examine the <code>Dockerfile</code> for this image in the next section, when we detail the configuration.</p>

<p>Additionally, you&rsquo;ll notice the creation of a few <code>secretRef</code> environment variables, as well as the setting of the <code>CASC_JENKINS_CONFIG</code> environment variable and the mounting of a <code>jenkins-casc</code> <code>ConfigMap</code> - these again will be expanded upon in the configuration section.</p>

<pre><code class="language-yaml">---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cjd
spec:
  selector:
   matchLabels:
     app: cjd
  serviceName: &quot;cjd&quot;
  template:
    metadata:
      labels:
        app: cjd
    spec:
      containers:
      - name: cjd
        image: gcr.io/melgin/cjd-casc:d176f38b289d0437a2503c83af473f57b25a4d26
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
        - containerPort: 50000
        env:
        - name: CASC_JENKINS_CONFIG
          value: /var/jenkins_config/jenkins-casc.yaml
        envFrom:
          - secretRef:
              name: github
          - secretRef:
              name: url
          - secretRef:
              name: github-oauth
        volumeMounts:
        - name: jenkins-home
          mountPath: /var/jenkins_home/
        - name: jenkins-casc
          mountPath: /var/jenkins_config/
      securityContext:
        fsGroup: 1000
      serviceAccountName: cjd
      volumes:
      - name: jenkins-casc
        configMap:
          name: jenkins-casc
  volumeClaimTemplates:
  - metadata:
      name: jenkins-home
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      resources:
        requests:
          storage: 5Gi
</code></pre>

<h2 id="configuring-with-jenkins-configuration-as-code-plugin">Configuring with Jenkins Configuration-as-Code Plugin</h2>

<p>With the YAML for the required CJD Kubernetes resources laid out, we&rsquo;ll now go into the code handling the configuration of the master. While detailing the <code>StatefulSet</code> above, we mentioned that a custom Docker image is used for the CJD container. The <code>Dockerfile</code> for this image can be found below:</p>

<pre><code class="language-Dockerfile">FROM cloudbees/cloudbees-jenkins-distribution:2.164.3.2

LABEL maintainer &quot;melgin@cloudbees.com&quot;

ENV JAVA_OPTS=&quot;-Djenkins.install.runSetupWizard=false&quot;

USER root

RUN echo 2.0 &gt; /usr/share/jenkins/ref/jenkins.install.UpgradeWizard.state

ENV TZ=&quot;/usr/share/zoneinfo/America/New_York&quot;

ENV JENKINS_UC https://jenkins-updates.cloudbees.com
# add environment variable to point to configuration file
ENV CASC_JENKINS_CONFIG /usr/jenkins_config/jenkins-casc.yaml

# Install plugins
ADD https://raw.githubusercontent.com/jenkinsci/docker/master/install-plugins.sh /usr/local/bin/install-plugins.sh
RUN chmod 755 /usr/local/bin/install-plugins.sh
ADD https://raw.githubusercontent.com/jenkinsci/docker/master/jenkins-support /usr/local/bin/jenkins-support
RUN chmod 755 /usr/local/bin/jenkins-support
COPY plugins.txt /usr/share/jenkins/ref/plugins.txt
RUN bash /usr/local/bin/install-plugins.sh &lt; /usr/share/jenkins/ref/plugins.txt

USER jenkins
</code></pre>

<p>In this <code>Dockerfile</code>, we add custom configuration to the official CJD Docker image. We first set the <code>JENKINS_UC</code> environment variable to use the CloudBees update center, as well as the <code>CASC_JENKINS_CONFIG</code> variable to point to the location we&rsquo;ll mount our configuration file. Finally, we leverage the <a href="https://github.com/jenkinsci/docker#preinstalling-plugins">Jenkins Docker <code>install-plugins.sh</code> script</a> to install a list of plugins from our <code>plugins.txt</code> file. These plugins include:</p>

<pre><code class="language-txt">configuration-as-code:1.20
job-dsl:1.74
kubernetes:1.14.9
kubernetes-credentials:0.4.0
credentials:2.2.0
workflow-multibranch:2.20
github-branch-source:2.4.5
workflow-aggregator:2.5
blueocean:1.10.2
github-oauth:0.32
</code></pre>

<p>This will handle the initial installation of the plugins we need, including resolving any dependencies.</p>

<p>Next, we&rsquo;ll need to use the Configuration as Code plugin to handle the configuration of the master itself. To do so, we&rsquo;ll mount the configuration YAML as a <code>ConfigMap</code> that our CJD <code>StatefulSet</code> will use. Here&rsquo;s what our <code>jenkinsCasc.yaml</code> file looks like:</p>

<pre><code class="language-yaml">---
apiVersion: v1
kind: ConfigMap
metadata:
  name: jenkins-casc
data:
  jenkins-casc.yaml: |
    jenkins:
      agentProtocols:
      - &quot;Diagnostic-Ping&quot;
      - &quot;JNLP4-connect&quot;
      - &quot;Ping&quot;
      crumbIssuer:
        standard:
          excludeClientIPFromCrumb: false
      securityRealm:
        github:
          githubWebUri: &quot;https://github.com&quot;
          githubApiUri: &quot;https://api.github.com&quot;
          clientID: &quot;${CLIENT_ID}&quot;
          clientSecret: &quot;${CLIENT_SECRET}&quot;
          oauthScopes: &quot;read:org,user:email&quot;
      systemMessage: &quot;CJD in Kubernetes configured as code!&quot;
      clouds:
      - kubernetes:
          name: kubernetes
          jenkinsUrl: http://cjd
          containerCapStr: 100
      authorizationStrategy:
        loggedInUsersCanDoAnything:
          allowAnonymousRead: false
    credentials:
      system:
        domainCredentials:
          - credentials:
            - usernamePassword:
                scope: GLOBAL
                id: &quot;github&quot;
                description: &quot;GitHub API token&quot;
                username: ${username}
                password: ${token}
    jobs:
    - script: &gt;
        multibranchPipelineJob('cjd-casc') {
          branchSources {
            github {
              scanCredentialsId('github')
              repoOwner('cb-technologists')
              repository('cjd-casc')
            }
          }
          orphanedItemStrategy {
            discardOldItems {
              numToKeep(5)
            }
          }
        }
    security:
      remotingCLI:
        enabled: false
    unclassified:
      location:
        adminAddress: &quot;address not configured yet &lt;nobody@nowhere&gt;&quot;
        url: &quot;https://cjd.cloudbees.elgin.io/&quot;
</code></pre>

<p>This config file sets up a handful of basic Jenkins settings like allowed agent protocols, security settings, and an example system message.</p>

<p>Three config items in particular are worth additional exploration. First, the security realm is set to use a GitHub organization for authentication (see <a href="https://wiki.jenkins.io/display/JENKINS/GitHub+OAuth+Plugin">the Jenkins GitHub OAuth Plugin page</a> for details on setting up a GitHub OAuth application). To avoid hardcoding our Client ID and Client Secret in our GitHub repository, we take advantage of Kubernetes <code>Secrets</code>.</p>

<p>Recall from our <code>StatefulSet</code> above that we load a few environment variables from <code>Secrets</code>. These include our GitHub OAuth application ID &amp; secret, as well as the username and API token used by our Pipeline job to communicate with our repository.</p>

<p>To create these, we use the following <code>kubectl</code> commands (replacing the placeholder variables with the actual credentials):</p>

<pre><code class="language-bash">kubectl create secret generic github-oauth --from-literal=CLIENT_ID=${CLIENT_ID} --from-literal=CLIENT_SECRET=${CLIENT_SECRET}

kubectl create secret generic github --from-literal=username=${USERNAME} --from-literal=token=${TOKEN}
</code></pre>

<p>The second config item to note is the creation of a simple Kubernetes cloud that our master will use for provisioning pod template agents using the <a href="https://github.com/jenkinsci/kubernetes-plugin">Jenkins Kubernetes plugin</a>.</p>

<p>The third and final detail to call out is the <code>jobs</code> section, which uses the <a href="https://github.com/jenkinsci/job-dsl-plugin">Job DSL plugin</a> to seed a Multibranch Pipeline job. The Jenkinsfile for this Pipeline is stored in the same GitHub repository as the rest of our config files. We&rsquo;ll detail the contents of this Pipeline script in a later section.</p>

<p>To apply this configuration, we apply the <code>ConfigMap</code> manifest file to our cluster:</p>

<pre><code class="language-bash">kubectl apply -f jenkinsCasc.yaml
</code></pre>

<p>With our <code>ConfigMap</code> and related <code>Secrets</code> created, we can now apply the manifest file from the previous section to deploy the remainder of the CJD resources:</p>

<pre><code class="language-bash">kubectl apply -f cjd.yaml
</code></pre>

<h2 id="securing-with-cert-manager">Securing with cert-manager</h2>

<p>At this point, our CJD instance is not accessible through HTTPS. To remedy this and enhance the security of our environment, we&rsquo;ll be using <a href="https://docs.cert-manager.io/en/latest/"><code>cert-manager</code></a>, a Kubernetes tool used to automate the management of certificates within a cluster. In this case, we&rsquo;ll use it to manage our TLS certificate issuance from <a href="https://letsencrypt.org/">Let&rsquo;s Encrypt</a>.</p>

<p>Our setup process for <code>cert-manager</code> loosely follows their <a href="https://github.com/jetstack/cert-manager/blob/master/docs/tutorials/acme/quick-start/index.rst">Quick-Start guide</a>. Because we&rsquo;ve already configured an ingress controller with a corresponding DNS entry along with deploying the CJD resources, we can <a href="https://github.com/jetstack/cert-manager/blob/master/docs/tutorials/acme/quick-start/index.rst#step-0---install-helm-client">ensure Helm</a> <a href="https://github.com/jetstack/cert-manager/blob/master/docs/tutorials/acme/quick-start/index.rst#step-1---installer-tiller">&amp; Tiller</a> are installed on the cluster, then skip to the <a href="https://github.com/jetstack/cert-manager/blob/master/docs/tutorials/acme/quick-start/index.rst#step-5---deploy-cert-manager">step of actually deploying <code>cert-manager</code></a>.</p>

<p>Once <code>cert-manager</code> has been deployed in its new <code>namespace</code>, we&rsquo;ll next need to deploy the <code>Issuer</code> to our <code>cjd</code> <code>namespace</code>.</p>

<blockquote>
<p><strong>Note</strong>: on initial setup of <code>cert-manager</code>, it&rsquo;s probably prudent to heed the Quick-Start&rsquo;s recommendation to create a staging <code>Issuer</code> first to minimize the risk of being rate limited by Let&rsquo;s Encrypt. For brevity, we&rsquo;ll only walk through the production <code>Issuer</code> creation here.</p>
</blockquote>

<p>Using the provided <a href="https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/docs/tutorials/acme/quick-start/example/production-issuer.yaml">example <code>Issuer</code> manifest file</a>, we&rsquo;ll swap in our actual email address before creating the resource in our <code>cjd</code> <code>namespace</code>:</p>

<pre><code class="language-yaml">apiVersion: certmanager.k8s.io/v1alpha1
kind: Issuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    # The ACME server URL
    server: https://acme-v02.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: melgin@cloudbees.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-prod
    # Enable the HTTP-01 challenge provider
    http01: {}
</code></pre>

<pre><code class="language-bash">kubectl apply -f production-issuer.yaml
</code></pre>

<p>Once created, this <code>Issuer</code> relies on annotations on our <code>Ingress</code> to manage the TLS certificate creation. Recall that we briefly discussed the <code>Ingress</code> manifest in a previous section:</p>

<pre><code class="language-yaml">---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: cjd
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    certmanager.k8s.io/issuer: &quot;letsencrypt-prod&quot; # add after cert-manager deploy
    certmanager.k8s.io/acme-challenge-type: http01 # add after cert-manager deploy
spec:
  tls: # cert-manager
  - hosts: # cert-manager
    - cjd.cloudbees.elgin.io # cert-manager
    secretName: cjd-tls # cert-manager
  rules:
  - host: cjd.cloudbees.elgin.io
    http:
      paths:
      - path: /
        backend:
          serviceName: cjd
          servicePort: 80
</code></pre>

<p>The lines with comments referencing <code>cert-manager</code> are required for the TLS certificate to be successfully issued. These include specifying the <code>Issuer</code>, the challenge type, as well as the hostname and <code>secretName</code>.</p>

<p>You can confirm that the certificate has been successfully issued by running <code>kubectl get certificate</code> and verifying that <code>READY</code> is <code>True</code> for our <code>cjd-tls</code> certificate. Once this process has been completed, CJD should now be accessible via HTTPS.</p>

<h2 id="writing-the-update-pipeline-job">Writing the update Pipeline job</h2>

<p>With CJD now running in our cluster and accessible via HTTPS, we&rsquo;ll next take a look at the Pipeline script that will handle the update of the master. At a high-level, we need our Pipeline to accomplish two major tasks:</p>

<ol>
<li>build and push our Docker image whenever a change is pushed to the GitHub repository, and</li>
<li>update our Kubernetes resources with the newly built Docker image and any additional changes.</li>
</ol>

<p>We represent these two procedures as stages within our Pipeline script.</p>

<p>For the first stage, we will use <a href="https://github.com/GoogleContainerTools/kaniko">kaniko</a> to build and push our Docker image to <a href="https://cloud.google.com/container-registry/">Google Container Registry</a>. Because we&rsquo;ll be using different agents for each stage, we&rsquo;ll start the Pipeline with <code>agent none</code>. Within the first stage, we define our agent using YAML, which specifies the <a href="https://gcr.io/kaniko-project/executor:debug">Google-provided kaniko image</a> as the container we will use.</p>

<p>To use kaniko, we&rsquo;ll first need to <a href="https://github.com/GoogleContainerTools/kaniko#kubernetes-secret">follow this kaniko documentation</a> to create a Google Cloud service account with appropriate permissions and download the related JSON key. Assuming we&rsquo;ve renamed the key <code>kaniko-secret.json</code>, we can <a href="http://docs.heptio.com/content/private-registries/pr-gcr.html">follow this procedure from Heptio</a> to create another Kubernetes <code>Secret</code> to allow for authentication to Google Container Registry (again replacing the placeholder email with the real service account email address):</p>

<pre><code class="language-bash">kubectl create secret docker-registry gcr-secret \
    --docker-server=https://gcr.io \
    --docker-username=_json_key \
    --docker-email=${SERVICE_ACCOUNT@PROJECT.iam.gserviceaccount.com} \
    --docker-password=&quot;$(cat kaniko-secret.json)&quot;
</code></pre>

<p>Within the <code>step</code> block, we are accomplishing two main things:
1. In the default <code>jnlp</code> container, we store the specific Git commit ID that triggered the build as an environment variable
2. In the <code>kaniko</code> container, we build and push our latest Docker image, tagging it with the commit ID we just stored.</p>

<pre><code class="language-groovy">pipeline {
  agent none
  stages {
    stage('Build and push with kaniko') {
      agent {
        kubernetes {
          label &quot;kaniko-${UUID.randomUUID().toString()}&quot;
          yaml &quot;&quot;&quot;
kind: Pod
metadata:
  name: kaniko
spec:
  serviceAccountName: cjd
  containers:
  - name: kaniko
    image: gcr.io/kaniko-project/executor:debug-v0.10.0
    imagePullPolicy: Always
    command:
    - /busybox/cat
    tty: true
    volumeMounts:
      - name: jenkins-docker-cfg
        mountPath: /kaniko/.docker
  volumes:
  - name: jenkins-docker-cfg
    projected:
      sources:
      - secret:
          name: gcr-secret
          items:
            - key: .dockerconfigjson
              path: config.json
&quot;&quot;&quot;
        }
      }
      environment {
        PATH = &quot;/busybox:/kaniko:$PATH&quot;
      }
      steps {
        container('jnlp') {
          script {
              env.COMMIT_ID = sh(returnStdout: true, script: 'git rev-parse HEAD').trim()
          }
        }
        container(name: 'kaniko', shell: '/busybox/sh') {
          sh &quot;&quot;&quot;#!/busybox/sh
                /kaniko/executor --context `pwd` --destination gcr.io/melgin/cjd-casc:${env.COMMIT_ID} --cache=true
          &quot;&quot;&quot;
        }
      }
    }
</code></pre>

<p>In the subsequent stage, we now apply changes to our CJD configuration to the resources running in our Kubernetes cluster.</p>

<p>First, we use a <code>when</code> directive to ensure we only run this stage when the Pipeline is running off of the <em>master</em> branch. We then use the <a href="https://gcr.io/cloud-builders/kubectl">Google-provided kubectl image</a> for our stage agent pod template. Within this container, we apply changes to our <code>jenkins-casc</code> <code>ConfigMap</code>, the resources specified in <code>cjd.yaml</code>, and finally set the image for our CJD <code>StatefulSet</code> to the latest one we&rsquo;ve just pushed to Google Container Registry:</p>

<pre><code class="language-groovy">    stage('Update CJD') {
      when {
        beforeAgent true
        branch 'master'
      }
      agent {
        kubernetes {
          label &quot;kubectl-${UUID.randomUUID().toString()}&quot;
          yaml &quot;&quot;&quot;
kind: Pod
metadata:
  name: kubectl
spec:
  serviceAccountName: cjd
  containers:
  - name: kubectl
    image: gcr.io/cloud-builders/kubectl@sha256:50de93675e6a9e121aad953658b537d01464cba0e4a3c648dbfc89241bb2085e
    imagePullPolicy: Always
    command:
    - cat
    tty: true
&quot;&quot;&quot;
        }
      }
      steps {
        container('kubectl') {
          sh &quot;&quot;&quot;
            kubectl apply -f jenkinsCasc.yaml
            kubectl apply -f cjd.yaml
            kubectl set image statefulset cjd cjd=gcr.io/melgin/cjd-casc:${env.COMMIT_ID}
          &quot;&quot;&quot;
        }
      }
    }
  }
}
</code></pre>

<p>To ensure the <code>cjd-casc</code> Pipeline job is triggered automatically upon each commit or pull request, we need to ensure a webhook is setup within the GitHub repository following <a href="https://support.cloudbees.com/hc/en-us/articles/224543927-GitHub-Integration-Webhooks">this process</a>.</p>

<p>With this in place, we now have all of our Jenkins configuration stored as code in our GitHub repository, including the process for updating the configuration. Whenever a change is pushed to the repository, those changes will automatically be applied to our Jenkins master.</p>

<p><img src="/img/cjd-casc/cjd-casc-pipeline.png" alt="successful run of cjd-casc Pipeline" /></p>

<h2 id="further-enhancements">Further enhancements</h2>

<p>This approach moves us much closer to the practice of GitOps for our Jenkins configuration. However, there are certainly areas for enhancement going forward. A few immediate examples that come to mind include:</p>

<ul>
<li>Non-master branch Pipeline runs could deploy the CJD resources &amp; config to a staging <code>namespace</code>. This would allow for the vetting of changes in a non-production environment before merging to master - a workflow critical for use in any scenario supporting mission-critical workloads.</li>
<li>Some level of smoke testing should be introduced for either/both of the non-prod/prod <code>namespaces</code> as a third Pipeline stage. This could range from a simple <code>curl</code> command to check the Jenkins system message in order to verify Jenkins is up and running, all the way to more complex cases that verify the latest configuration has been appropriately applied.</li>
<li><code>post</code> blocks could be introduced for notification to the appropriate Slack channel, email list, etc., that a Jenkins update has commenced/succeeded/failed.</li>
<li>Right now, the Docker image is rebuilt on every Pipeline run - even if no changes have been committed to the <code>Dockerfile</code> or related files. While caching is in place, it would be even more efficient to check for changes to those specific files, then selectively skip or run the <code>Build and push with kaniko</code> stage (though this does somewhat complicate the tagging of the Docker image each time a commit triggers a build).</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>CloudBees&#39; Cross Team Collaboration for Asynchronous DevSecOps</title>
            <link>https://cb-technologists.github.io/posts/cloudbees-cross-team-and-dev-sec-ops/</link>
            <pubDate>Mon, 10 Jun 2019 07:50:46 -0400</pubDate>
            
            <guid>https://cb-technologists.github.io/posts/cloudbees-cross-team-and-dev-sec-ops/</guid>
            <description>What is Cross Team Collaboration? CloudBees&amp;rsquo; Cross Team Collaboration provides the ability to publish an event from a Jenkins job that triggers any other Jenkins job on the same master or different masters that are listening for that event. It is basically a light-weight PubSub for CloudBees Core Masters connected to CloudBees Operations Center. Jenkins has had the ability to trigger other jobs for quite a while now (and with CloudBees this is even easy to do across Masters), but it always required that the upstream job be aware of the downstream job(s) to be triggered.</description>
            <content type="html"><![CDATA[

<h2 id="what-is-cross-team-collaboration">What is Cross Team Collaboration?</h2>

<p>CloudBees&rsquo; Cross Team Collaboration provides the ability to publish an event from a Jenkins job that triggers any other Jenkins job on the same master or different masters that are listening for that event. It is basically a light-weight <a href="https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern"><strong>PubSub</strong></a> for CloudBees Core Masters connected to <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/operating/#managing-operation-center">CloudBees Operations Center</a>. Jenkins has had the ability to <a href="https://jenkins.io/doc/pipeline/steps/pipeline-build-step/">trigger other jobs</a> for quite a while now (and <a href="https://support.cloudbees.com/hc/en-us/articles/226408088-Trigger-jobs-across-masters">with CloudBees this is even easy to do across Masters</a>), but it always required that the upstream job be aware of the downstream job(s) to be triggered. The Cross Team Collaboration feature provides a loosely coupled link between upstream and downstream Jenkins jobs - so that any job that is interested in a certain event, for whatever reason, can subscribe to that event and get triggered whenever that event is published.</p>

<p>Here are a few good CloudBees&rsquo; blog posts and CloudBees&rsquo; documentation on CloudBees&rsquo; Cross Team Collaboration:</p>

<ul>
<li><a href="https://www.cloudbees.com/blog/cross-team-collaboration-part-1">Cross Team Collaboration (Part 1)</a></li>
<li><a href="https://www.cloudbees.com/blog/cross-team-collaboration-part-2">Cross Team Collaboration (Part 2)</a></li>
<li><a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/cross-team-collaboration/">Cross Team Collaboration documentation</a></li>
</ul>

<h2 id="devsecops">DevSecOps</h2>

<p><a href="https://tech.gsa.gov/guides/understanding_differences_agile_devsecops/">DevSecOps</a> - the idea of shifting security left in your Continuous Delivery pipelines - is becoming a vital component of successful CD. DevSecOps is all about speeding up software delivery, while maintaining, or even improving, the level of security for delivered application code. However, even though you should be shifting automated security left - you still don&rsquo;t want it to impede developers trying to deliver software more quickly. CloudBees&rsquo; Cross Team Collaboration feature is a perfect capability for automating security while at the same time getting out of the way of developers - improving the security and quality of your software delivery while minimizing the impact on delivery speed.</p>

<h2 id="use-case-asynchronously-scan-container-images-for-vulnerabilities-and-compliance">Use Case: Asynchronously Scan Container Images for Vulnerabilities and Compliance</h2>

<p>As containers become a more and more ubiquitous method for delivering your applications, ensuring that your container images don&rsquo;t have security vulnerabilities and/or organization specific security compliance issues is an important aspect of CD for containerized application delivery. However, scanning containers images isn&rsquo;t the fastest process in the world and you don&rsquo;t want to unnecessarily slow down developers trying to get stuff done. You also may not want to depend on individual development teams to configure and manage important securitys steps in their delivery pipelines.</p>

<p>Cross Team Collaboration enables you to publish an event from a <a href="https://jenkins.io/doc/book/pipeline/shared-libraries/">Pipeline Shared Library</a> for <a href="https://github.com/cloudbees-days/pipeline-library/blob/master/vars/kanikoBuildPush.groovy">securely building container images</a> and then asynchronously triggering <em>not-so-quick</em> security related jobs listening for events, making it very easy to provide security as part of the CD pipelines for an entire organization.</p>

<p>So, this container scan job can be run on different Jenkins Masters (or as we at CloudBees refer to them: <a href="https://www.cloudbees.com/blog/team-masters-continuous-delivery">Team Masters</a>) and are able to run automatically thanks to the <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/cross-team-collaboration/#cross-team-config">Cross Team Collaboration queue</a> managed by the <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/operating/#managing-operation-center">Operations Center</a> in CloudBees Core. Pipelines for building containers and checking vulnerabilities are then decoupled, but they run any time you build a container in an upstream job (e.g. every time engineering teams build containers, the vulnerabilities and compliance will be checked, but running this container scan doesn&rsquo;t require building the container again).</p>

<h3 id="cross-team-collaboration-events">Cross Team Collaboration Events</h3>

<p>There are basically <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/cross-team-collaboration/#cross-team-event-types">two types of Cross Team Collaboration events</a>:</p>

<p><strong>Simple Event:</strong></p>

<pre><code class="language-groovy">publishEvent simpleEvent(&quot;${dockerReg}/helloworld-nodejs:${repoName}-${BUILD_NUMBER}&quot;)
</code></pre>

<p><strong>JSON Event:</strong></p>

<pre><code class="language-groovy">publishEvent event:jsonEvent(&quot;{'eventType':'containerImagePush', 'image':'${dockerReg}/helloworld-nodejs:${repoName}-${BUILD_NUMBER}'}&quot;), verbose: true
</code></pre>

<p>For this example we will be using the more verbose JSON event. The problem with the <strong>Simple Event</strong> approach is that the triggered job would have to subscribe to a single <code>string</code> value and in this case a specific container <code>image</code>. But what we really want is to run an Anchore scan for all container images being pushed to our DEV container registry. The <strong>JSON Event</strong> approach allows us to subscribe to a more generic event, <code>containerImagePush</code>, while passing the exact container image being pushed as an additional JSON value for the key <code>image</code>.  But to use this approach the triggered job(s) must retrieve the value of the <code>image</code> key from the event payload.</p>

<h3 id="capturing-the-cross-team-collaboration-event-payload">Capturing the Cross Team Collaboration Event Payload</h3>

<p>Now let&rsquo;s compare using groovy code vs a <code>curl</code> call against the <a href="https://wiki.jenkins.io/display/JENKINS/Remote+access+API">Jenkins REST API</a> to get the JSON event payload:</p>

<ul>
<li>You could get the event JSON with the following: <code>currentBuild.getBuildCauses()[0].event.toString()</code>. But that will run on the Jenkins Master, not the Jenkins agent and will impact performance when you are scanning hundreds or even thousands of container images.</li>
<li>A better approach is to use the <code>sh</code> step with a <code>curl</code> call against the Jenkins REST API with a <a href="https://jenkins.io/blog/2018/07/02/new-api-token-system/">Jenkins API token</a> to get the JSON representation of the current build, and then piping the JSON response to <a href="https://stedolan.github.io/jq/"><strong>jq</strong></a> to get the value for the <code>image</code> key from the event payload in a Jenkins Pipeline triggered by the <code>EventTriggerCause</code>: <code>curl -u 'beedemo-admin':$TOKEN --silent ${BUILD_URL}/api/json| jq '.actions[0].causes[0].event.image'</code>. The advantages of this approach are:

<ul>
<li>The <code>sh</code> step will run on the agent, not the Jenkins Master, allowing you to scale across as many agents as needed for your container scans with very little impact on the performance of the Jenkins Master.</li>
<li>Using lightweight shell scripts provide easier testing and more portability of your CD pipelines to other platforms.</li>
</ul></li>
</ul>

<p>| NOTE: <code>BUILD_URL</code> is one of many <a href="https://jenkins.io/doc/book/pipeline/getting-started/#global-variable-reference">Pipeline global variables</a> available to all Jenkins Pipeline jobs.</p>

<h3 id="anchore-inline-scan">Anchore Inline Scan</h3>

<p>Earlier this year, <a href="https://anchore.com/">Anchore</a> provided some new tools and scripts to make it easier to execute Anchore scans without constantly running an Anchore Engine. The <a href="https://anchore.com/inline-scanning-with-anchore-engine/">Anchore <strong>inline scan</strong></a> provides the same analysis/vulnerability/policy evaluation and reporting as a statically managed Anchore engine and is used in this example to highlight how easy and fast you can add container security scanning to your own CD pipelines. However, a better long-term approach would be to stand-up your own centralized, managed and stable Anchore engine to use across all of you dev teams. The advantages of a static, always running Anchore Engine include:</p>

<ul>
<li><strong>Faster scans:</strong> since you don&rsquo;t have to wait for the Anchore engine to start-up for each job.</li>
<li><strong>Reduced infrastructure costs:</strong> if you only do a few scans a day then this is less of an advantage as you will have a constant infrastructure cost for the static Anchore engine. But if you are doing 100s of scan per day then you will definitely realize savings with this approach.</li>
<li><strong>More secure:</strong> as we will see in the <strong>inline scan</strong> example below, the Anchore <code>inline_scan</code> script requires access to a Docker daemon. And in this example we are using the <a href="https://github.com/jenkinsci/kubernetes-plugin">Jenkins Kubernetes plugin</a> to provide dynamic and ephemeral agent pods for the Anchore inline scan job. A quick and dirty approach - that has a number of security implications - for providing a K8s pod agent access to the Docker daemon is to mount the Docker socket as a <code>volume</code> on the pod.</li>
</ul>

<p>But again, we will use the newer Anchore <strong>inline scan</strong> in this example to highlight how fast you can add container scans to your own Jenkins Pipelines.</p>

<p><em>Anchore inline scan Pod</em> - <code>dockerClientPod.yml</code></p>

<pre><code class="language-yaml">apiVersion: v1
kind: Pod
spec:
  containers:
  - name: docker-client
    image: gcr.io/technologists/docker-client:0.0.3
    command: ['cat']
    tty: true
    volumeMounts:
    - name: dockersock
      mountPath: /var/run/docker.sock
  volumes:
  - name: dockersock
    hostPath:
      path: /var/run/docker.sock
</code></pre>

<p>Even though there is an <a href="https://plugins.jenkins.io/anchore-container-scanner">Anchore plugin for Jenkins</a>, there is no reason to install another plugin when you can accomplish the same thing with a straightforward <code>sh</code> step. As mentioned in my <a href="./jenkins-plugins-good-bad-ugly/">last post here on the Technologists site</a> - using fewer Jenkins plugins is a <strong>good</strong> thing.</p>

<pre><code class="language-groovy">container('docker-client'){
  sh &quot;curl -s https://ci-tools.anchore.io/inline_scan-v0.3.3 \
  | bash -s -- -f -b ./.anchore_policy.json -p ${containerImage}&quot;
}
</code></pre>

<p>Again, the only thing required to run the scan above is a Docker daemon. So you could just as easily run that command on your laptop running Docker as on a Jenkins agent that has access to a Docker daemon.</p>

<h3 id="putting-it-all-together">Putting It All Together</h3>

<p><em>CloudBees&rsquo; Pipeline Template Catalog, Pipeline Shared Library, and Cross Team Collaboration</em></p>

<p>By combining the new <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/pipeline/#_setting_up_a_pipeline_template_catalog">CloudBees&rsquo; Pipeline Template Catalogs</a> with a Pipeline Shared Library and CloudBees&rsquo; Cross Team Collaboration we are able to provide robust DevSecOps application delivery Pipelines that are very easy for development teams to adopt quickly.</p>

<p>First we have the Pipeline Shared Library for building our container images with <a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a>:</p>

<p><em>pipeline shared library</em> - <code>kanikoBuildPush.groovy</code></p>

<pre><code class="language-groovy">def call(String imageName, String imageTag = env.BUILD_NUMBER, String gcpProject = &quot;core-workshop&quot;, String target = &quot;.&quot;, String dockerFile=&quot;Dockerfile&quot;, Closure body) {
  def dockerReg = &quot;gcr.io/${gcpProject}&quot;
  imageName = &quot;helloworld-nodejs&quot;
  def label = &quot;kaniko-${UUID.randomUUID().toString()}&quot;
  def podYaml = libraryResource 'podtemplates/dockerBuildPush.yml'
  podTemplate(name: 'kaniko', label: label, yaml: podYaml, inheritFrom: 'default-jnlp', nodeSelector: 'type=agent') {
    node(label) {
      body()
      imageNameTag()
      gitShortCommit()
      def repoName = env.IMAGE_REPO.toLowerCase()
      container(name: 'kaniko', shell: '/busybox/sh') {
        withEnv(['PATH+EXTRA=/busybox:/kaniko']) {
          sh &quot;&quot;&quot;#!/busybox/sh
            /kaniko/executor -f ${pwd()}/${dockerFile} -c ${pwd()} --build-arg context=${repoName} --build-arg buildNumber=${BUILD_NUMBER} --build-arg shortCommit=${env.SHORT_COMMIT} --build-arg commitAuthor=${env.COMMIT_AUTHOR} -d ${dockerReg}/helloworld-nodejs:${repoName}-${BUILD_NUMBER}
          &quot;&quot;&quot;
        }
      }
      publishEvent event:jsonEvent(&quot;{'eventType':'containerImagePush', 'image':'${dockerReg}/helloworld-nodejs:${repoName}-${BUILD_NUMBER}'}&quot;), verbose: true
    }
  }
}
</code></pre>

<p>Note the <code>publishEvent</code> step at the end - after the container image has been successfully built and pushed to our <strong>dev</strong> container registry it will <strong>publish</strong> the <code>containerImagePush</code> event.</p>

<p><em>The JSON output for the <code>publishEvent</code> step - note the <code>image</code> key value is the container image just built and pushed by Kaniko:</em></p>

<pre><code class="language-json">{
  &quot;eventType&quot;: &quot;containerImagePush&quot;,
  &quot;image&quot;: &quot;gcr.io/core-workshop/helloworld-nodejs:beeops-cb-days-7&quot;,
  &quot;source&quot;:     {
      &quot;type&quot;: &quot;JenkinsTeamBuild&quot;,
      &quot;buildInfo&quot;:         {
          &quot;build&quot;: 7,
          &quot;job&quot;: &quot;template-jobs/beedemo-admin-helloworld-nodejs/master&quot;,
          &quot;jenkinsUrl&quot;: &quot;https://********/teams-sec/&quot;,
          &quot;instanceId&quot;: &quot;d37a81cc1906b6fe684f253a8a07834c&quot;,
          &quot;team&quot;: &quot;sec&quot;
      }
  }
}
</code></pre>

<p>Next, the <code>kanikoBuildPush</code> shared library is consumed by a <a href="https://github.com/cloudbees-days/pipeline-template-catalog">Pipeline Template Catalog</a> template. In this case a <a href="https://github.com/cloudbees-days/pipeline-template-catalog/tree/master/templates/nodejs-app">template for Node.js applications</a>:</p>

<p><a href="https://github.com/cloudbees-days/pipeline-template-catalog/blob/master/templates/nodejs-app/Jenkinsfile"><em>Pipeline Template</em></a> - <strong>Build and Push Image</strong> <code>stage</code></p>

<pre><code class="language-groovy">    stage('Build and Push Image') {
      when {
        beforeAgent true
        branch 'master'
      }
      steps {  
        echo &quot;${repoOwner}&quot;
        kanikoBuildPush(env.IMAGE_NAME, env.IMAGE_TAG, &quot;${gcpProject}&quot;) {
          checkout scm
        }
      }
      post {
        success {
          slackSend message: &quot;${JOB_NAME} pipeline job is awaiting approval at: ${RUN_DISPLAY_URL}&quot;
        }
      }
    }
</code></pre>

<p>Again, if the <code>kanikoBuildPush</code> library step is successful it will publish a <code>containerImagePush</code> event.</p>

<p>Finally, we set-up a job on our <strong>Security</strong> Jenkins Master to listen for the <code>containerImagePush</code> event:</p>

<p><a href="https://github.com/cloudbees-days/anchore-scan/blob/master/Jenkinsfile"><strong>anchore-scan</strong> <code>Jenkinsfile</code></a></p>

<pre><code class="language-groovy">def containerImage
pipeline {
  agent none

  triggers {
      eventTrigger jmespathQuery(&quot;eventType=='containerImagePush'&quot;)
  }
  
  stages {
    stage('Anchore Scan') {
      agent {
        kubernetes {
          label 'docker-client'
          yamlFile 'dockerClientPod.yml'
        }
      }
      when { 
        triggeredBy 'EventTriggerCause' 
        beforeAgent true
      }
      environment {
        TOKEN = credentials('beedemo-admin-api-key')
      }
      steps {
        script {
          containerImage = sh(script: &quot;&quot;&quot;
             curl -u 'beedemo-admin':$TOKEN --silent ${BUILD_URL}/api/json| jq '.actions[0].causes[0].event.image'
          &quot;&quot;&quot;, returnStdout: true)
        }
        echo containerImage
        container('docker-client'){
          sh &quot;curl -s https://ci-tools.anchore.io/inline_scan-v0.3.3 | bash -s -- -f -b ./.anchore_policy.json -p ${containerImage}&quot;
        }
      }
    }
  }
}
</code></pre>

<p>Note the <code>eventTrigger</code> step uses <code>jmespathQuery</code> to listen for the <code>containerImagePush</code> <code>eventType</code>. Also note the <code>triggeredBy</code> condition <code>EventTriggerCause</code> in the <a href="https://jenkins.io/doc/book/pipeline/syntax/#when"><code>when</code> directive</a> - this will result in the <code>Anchore Scan</code> stage only running (and the provisioning of a K8s pod based agent used for the scan) if this job is triggered by a Cross Team Collaboration event.</p>

<p>If the newly built container image doesn&rsquo;t pass all of the policies specified in the <a href="https://github.com/cloudbees-days/anchore-scan/blob/master/.anchore_policy.json"><code>.anchore_policy.json</code></a> file then the job will fail.</p>

<p>Here is an example Anchore report for a failed <code>anchore-scan</code> job:</p>

<pre><code class="language-console">Image Digest: sha256:e03d86b75d38d1d18035b58e9e43088c9d0d5dd6e49f2c507d949937174f3465
Full Tag: anchore-engine:5000/helloworld-nodejs:beeops-cb-days-5
Image ID: 0b22d7798cd24465252335d602059fea88128244b623bc4af20926eeec8f9b4c
Status: fail
Last Eval: 2019-06-07T12:56:03Z
Policy ID: custom-anchore-policy-nodejs
Final Action: stop
Final Action Reason: policy_evaluation

Gate              Trigger               Detail                                                                                     Status        
dockerfile        effective_user        User root found as effective user, which is explicity not allowed list                     stop          
dockerfile        instruction           Dockerfile directive 'HEALTHCHECK' not found, matching condition 'not_exists' check        warn          

Image Digest: sha256:e03d86b75d38d1d18035b58e9e43088c9d0d5dd6e49f2c507d949937174f3465
Full Tag: anchore-engine:5000/helloworld-nodejs:beeops-cb-days-5
Status: fail
Last Eval: 2019-06-07T12:56:04Z
Policy ID: custom-anchore-policy-nodejs
</code></pre>

<p>As you can see from the above output the scan failed because of the <code>effective_user</code> trigger - <a href="https://github.com/nodejs/docker-node/blob/master/10/alpine/Dockerfile">the official <code>node</code> container image we are using from DockerHub runs as <code>root</code></a> and <a href="https://snyk.io/blog/10-docker-image-security-best-practices/">this is a very bad security practice</a> as it allows <strong>container breakouts</strong> where the container user is able to escape the container namespace and interact with other processes on the host.</p>

<h3 id="some-improvements">Some Improvements</h3>

<ul>
<li>One improvement would be to run this without mounting the Docker socket in the <a href="https://github.com/cloudbees-days/anchore-scan/blob/declarative/dockerClientPod.yml"><code>docker-client</code> container</a>. The Anchore inline-scan script runs a number of Docker commands that requires a Docker daemon - but this is not good security. Using a static Anchore engine would allow us to do container scans without mounting the Docker socket.</li>
<li>Another improvement would be to extend the <code>anchore-scan</code> job to push the container image to a <strong>Prod</strong> container registry on success and notify interested dev teams that their image is now available for production deployments.</li>
</ul>

<h3 id="casc-for-cross-team-collaboration-configuration-for-your-cloudbees-core-v2-masters">CasC for Cross Team Collaboration Configuration for your CloudBees Core v2 Masters</h3>

<p>In order for all of this to work you have to turn on Cross Team Collaboration for all of your Core v2 Masters that you want to publish and subscribe to events. I am a big proponent of CasC for everything so here is an <a href="https://wiki.jenkins.io/display/JENKINS/Post-initialization+script"><code>init.groovy.d</code></a> script to set-up CasC to automatically enable Cross Team Collaboration notifications for your CloudBees Core v2 Masters on start-up:</p>

<p><a href="https://github.com/kypseli/cb-core-mm-workshop/blob/master/quickstart/init_61_notification_api.groovy"><em>cb-core-mm-workshop/quickstart/init_61_notification_api.groovy</em></a>:</p>

<pre><code class="language-groovy">import jenkins.model.Jenkins
import hudson.ExtensionList

import com.cloudbees.jenkins.plugins.notification.api.NotificationConfiguration
import com.cloudbees.jenkins.plugins.notification.spi.Router
import com.cloudbees.opscenter.plugins.notification.OperationsCenterRouter

jenkins = Jenkins.getInstance()

NotificationConfiguration config = ExtensionList.lookupSingleton(NotificationConfiguration.class);
Router r = new OperationsCenterRouter();
        config.setRouter(r);
        config.setEnabled(true);
        config.onLoaded();
</code></pre>

<p>I&rsquo;m also a big fan of the Jenkins Config-as-Code plugin. However, currently, the CloudBees&rsquo; plugins for Cross Team Collaboration do not yet support <a href="https://github.com/jenkinsci/configuration-as-code-plugin">JCasC</a> (but support for JCasC is coming soon).</p>

<h2 id="add-devsecops-to-your-cd-with-cloudbees-now">Add DevSecOps to Your CD with CloudBees Now</h2>

<p>So there&rsquo;s really no excuse NOT to add asynchronous container security scans to your container image CD pipelines with CloudBees Core v2, our Cross Team Collaboration feature and the Anchore <strong>inline scan</strong> - when it is as easy as this!</p>
]]></content>
        </item>
        
        <item>
            <title>Jenkins Plugins: The Good, the Bad and the Ugly</title>
            <link>https://cb-technologists.github.io/posts/jenkins-plugins-good-bad-ugly/</link>
            <pubDate>Thu, 30 May 2019 05:50:46 -0400</pubDate>
            
            <guid>https://cb-technologists.github.io/posts/jenkins-plugins-good-bad-ugly/</guid>
            <description>There are over 1600 Jenkins plugins and that is both a blessing and a curse. Of those 1600 plugins only a small percentage are well maintained and tested, and even fewer (140 of 1600+) are part of the CloudBees Assurance Program (CAP) as verified and/or compatible plugins - well tested to interoperate with the rest of the CAP plugins (and their dependencies) and with a specific LTS version of Jenkins.</description>
            <content type="html"><![CDATA[

<p>There are over <a href="http://updates.jenkins.io/pluginCount.txt">1600 Jenkins plugins</a> and that is both a blessing and a curse. Of those 1600 plugins only a small percentage are well maintained and tested, and even fewer (140 of 1600+) are part of the <a href="https://go.cloudbees.com/docs/cloudbees-documentation/assurance-program/">CloudBees Assurance Program (CAP)</a> as verified and/or compatible plugins - well tested to interoperate with the rest of the CAP plugins (and their dependencies) and with a specific LTS version of Jenkins. Problems can arise when you use plugins that aren&rsquo;t part of CAP, or a plugin that isn&rsquo;t well maintained or tested to work with all of the other plugins you are using and the specific version of Jenkins that you are using. But the extensibility offered by plugins has helped make Jenkins the most popular CI tool on the planet.</p>

<p>I typically like to end posts on a good note, so I will start with <em>The Ugly</em> and end with <em>The Good</em> - and then offer some opinionated ideas/best practices on Jenkins plugin management and usage.</p>

<h1 id="the-ugly">The Ugly</h1>

<p>There are almost always a number of Jenkins plugins that have security vulnerabilities. Over 55 plugins were listed as part of the <a href="https://jenkins.io/security/advisory/2019-04-03/">2019-04-03 Jenkins Security Advisory</a>. Even worse is when you find out that a plugin that you are using has a security vulnerability and you also find out that the plugin is not maintained anymore. You could search the 1600+ Jenkins plugins to see if there is another plugin that is maintained and that does what you need, or you could become a plugin maintainer - not exactly what you intended to sign up for when you first started using Jenkins. Are you developing your own applications or are you looking to become a Jenkins plugin developer?</p>

<p>Another <em>ugly</em> issue arises when you have numerous Jenkins masters in your organization. These Jenkins instances are often snowflakes comprised of many different plug-ins. So managing more than one Jenkins master with disparate sets of plugins can become very ugly, very quickly. CloudBees can certainly help you with this through CAP and something we call <a href="https://go.cloudbees.com/docs/cloudbees-documentation/admin-cje/cje-ux/#_when_to_use_a_team_master_when_to_use_a_managed_master">Team Masters - easily provisioned and managed team specific Jenkins masters</a> with an opinionated set of very stable and tested plugins. However, there is nothing stopping individual Jenkins master admins from manually installing a plugin and sometimes ending up with an unusable Jenkins master.</p>

<h1 id="the-bad">The Bad</h1>

<p>Installing a lot of plugins can result in maintenance hell and sometimes your Jenkins master doesn&rsquo;t even restart successfully after upgrading a plugin.</p>


    <img src="/img/jenkins-plugins-good-bad-ugly/jenkins_devil.png"  alt="Jenkins Devil"  class="left"  />



<p>And although the Jenkins Devil makes for a very cool sticker, it isn&rsquo;t something you ever want to see on <strong>your</strong> Jenkins Master, especially after restarting Jenkins for a plugin update. Backing out a plugin update that causes Jenkins to crash is not a fun thing to deal with and will slow down your software delivery.</p>

<p>Dependency hell is another <em>bad</em> thing that Jenkins admins have to deal with all the time. Sometimes upgrading just one plugin results in the need to update dozens others, and many Jenkins admins do this directly on their production Jenkins master. Blue Ocean, while a noble attempt at a new UI for Jenkins Pipelines, requires dozens of dependencies, many of which you probably have no use for - for example the Blue Ocean plugin suite requires both the <em>Bitbucket Pipeline for Blue Ocean</em> and the <em>GitHub Pipeline for Blue Ocean</em> plugins even if you don&rsquo;t use either Bitbucket or GitHub for source control.</p>

<p>Too many plugins that do the same thing - how do you choose? Search the Jenkins plugin site for <em>Docker</em> and you get 26 results. If I want Docker based agents should I use the <strong>Docker plugin</strong> or <strong>Yet Another Docker plugin</strong>? With 1600+ plugins, sometimes it can be hard to choose the right one.</p>

<h1 id="the-good">The Good</h1>

<p>The extensibility and integrations provided by Jenkins plugins are amazing. I don&rsquo;t believe that there is any other CI platform that integrates with as many source control tools/platforms as Jenkins. Without Jenkins&rsquo; extensive plugin ecosystem it would not be the CI automation tool of choice that it has become. Jenkins is by far the most flexible CI platform available, bar none, and the Jenkins plugin ecosystem is a big reason why.</p>

<p>There are a lot of very <em>good</em>, and even necessary, plugins. Like plugins for credentials and for source control - Jenkins has awesome integration with GitHub and Bitbucket for example. And the Jenkins Pipeline plugin suite (although another example of dependency hell) provides a <a href="https://jenkins.io/doc/book/pipeline/syntax/#declarative-pipeline">Declarative approach to building you CI/CD pipelines</a> that can be <a href="https://jenkins.io/doc/book/pipeline/jenkinsfile/">easily managed as-code in source control</a>. And finally, the <a href="https://jenkins.io/projects/jcasc/">JCasC plugin</a> makes it easier than ever to manager your Jenkins master configuration as-code in source control.</p>

<p>So there are some very <em>good</em> reasons to use <strong>some</strong> plugins.</p>

<h1 id="so-what-to-do">So What to Do</h1>

<p>CloudBees can certainly help. All of the CloudBees distributions, including the <a href="https://www.cloudbees.com/products/cloudbees-jenkins-distribution">free CloudBees Jenkins Distribution</a>, include CAP with Beekeeper. I have managed a few demo/workshop environments for the CloudBees Solution Architecture team for the last 4 years and update those environments almost every month. I have yet to have an update that has resulted in the Jenkins Devil - ok maybe one.</p>

<p>There are a few other things you can do right <strong>now</strong> , whether you use a CloudBees Distro or not, to make using Jenkins Plugins easier to manage and less impactful to your production Jenkins master - allowing you to focus on CD for the applications you are delivering instead of spending too much time managing Jenkins.</p>

<h2 id="use-jenkins-pipeline">Use Jenkins Pipeline</h2>

<p>Although Jenkins Pipeline does require a <a href="https://plugins.jenkins.io/workflow-aggregator">number of plugins and plugin dependencies</a> its advantages far outweigh the disadvantages of using Jenkins without Pipeline jobs. Using Jenkins Pipelines with a Jenkinsfile in source control and <a href="https://jenkins.io/doc/book/pipeline/shared-libraries/">Pipeline Shared Libraries</a> can greatly reduce the number of additional plugins you need to install and manage. For example if you need to send a Slack message, just run a simple <code>curl</code> command in a lightweight container instead of installing the <a href="https://github.com/jenkinsci/slack-plugin/issues">Jenkins Slack plugin</a>:</p>

<pre><code class="language-bash">curl -X POST -H 'Content-type: application/json' --data '{&quot;text&quot;:&quot;The build is broken :(&quot;}' YOUR_WEBHOOK_URL
</code></pre>

<p>This is actually considered a best practice for Jenkins Pipelines as any <code>step</code> that is run from a plugin will actually run on the Jenkins master, not on the agent (other than the <code>sh</code>, <code>bat</code> and <code>pwsh</code> steps). This will result in worse performance for your Jenkins master and may even bring your Jenkins master down - once again slowing down your application delivery.</p>

<p>Another big plus with replacing Jenkins Pipeline plugin based steps with lightweight shell scripts is that it provides easier testing and more portability of your CD pipelines to other platforms. For example, Jenkins X Pipelines with Tekton runs every pipeline step as a command in a container - adopting that approach with Jenkins Pipelines now will make it much easier to migrate to better emerging solutions in the future.</p>

<h2 id="use-fewer-plugins">Use Fewer Plugins</h2>

<p>Using fewer plugs will reduce the amount of pain you will incur from many of the <em>ugly</em> and <em>bad</em> issues mentioned above. Migrating as many Jenkins Pipeline <code>steps</code> from plugins to <code>sh</code> steps running in containers not only reduces the <em>bad</em> and <em>ugly</em> above, it also makes it easier to test and reduce dependencies on the less than stellar plugin maintainers (like me), and provides better portability to other emerging CD technologies - like <a href="https://kurtmadel.com/posts/native-kubernetes-continuous-delivery/jenkins-x-goes-native/#re-tooling-with-tekton">Jenkins X Pipelines with Tekton</a>.</p>

<p>Do you really need the Docker plugin and the Yet Another Docker plugin? Or the Chuck Noris plugin? The fewer plugins that you install, the fewer plugins you have to manage and the less chance that they will have security issues or even worse, bring your Jenkins master down - Jenkins Devil and all.</p>

<h2 id="test">Test</h2>

<p>Always test any new plugin or plugin update before you put it into your production Jenkins master(s). Running Jenkins as a container can certainly make this easier - and is what I suggest - but there is no reason why you can&rsquo;t use Jenkins to automate this kind of testing regardless of how you deploy Jenkins. Just spin up a Jenkins master with a few <em>fake</em> jobs that use the plugins in a similar way to how you use them in your <em>real</em> jobs. All of this can be automated with Jenkins itself.</p>

<p>The <a href="https://github.com/jenkins-x/jenkins-x-serverless-filerunner">Jenkins X ephemeral masters</a> basically went with this approach - extensive testing whenever a new <a href="https://github.com/jenkins-x/jenkins-x-serverless-filerunner/blob/master/pom.xml#L32">plugin was added to the the CasC Master container image</a>.</p>

<h2 id="manage-plugins-with-casc">Manage Plugins with CasC</h2>

<p>Never use the Jenkins UI to install plugins. Maintain your plugins as code in source control, where every new plugin and plugin upgrade can be tracked as commits. The easiest and best way to do this, in my opinion, is to use a customized Docker image that includes the plugins you <strong>absolutely need</strong> - in addition to other configuration via JCasC (and if necessary, <a href="https://wiki.jenkins.io/display/JENKINS/Post-initialization+script"><code>init</code> scripts</a>). If you have read any of my other posts you will know that I am a big fan of containers - and have always run Jenkins with containers since I started at CloudBees back in 2015. The Jenkins GitHub Org <em>docker</em> project <a href="https://github.com/jenkinsci/docker/blob/master/install-plugins.sh">provides a script</a> for <a href="https://github.com/jenkinsci/docker#preinstalling-plugins">preinstalling plugins</a> from a simple <code>plugins.txt</code> file so your Jenkins master container image has all the plugins you need on startup. This makes it easier to test plugin changes and all of your plugin changes are captured as code commits - and a tool like Git (GitHub, BitBucket, even GitLab) is much better at tracking/auditing/controlling such changes than Jenkins was ever meant to be. Here is a simple <code>plugins.txt</code> file and <code>Dockerfile</code> to get you started:</p>

<p><em>plugins.txt</em></p>

<pre><code class="language-txt">configuration-as-code:1.19
credentials:2.2.0
</code></pre>

<p>Yes, only two plugins. The reason why we only need these two plugins is because the <a href="https://www.cloudbees.com/blog/cloudbees-jenkins-distribution-adds-stability-and-security-your-jenkins-environment">CloudBees Jenkins Distribution</a> already contains a curated set of plugins for Jenkins Pipeline, Blue Ocean, source control management and everything else we need - all well tested for us already.</p>

<p>This version of the Credentials plugin is an exception, because the recent version of the plugin with JCasC support has not been integrated into CAP yet (coming soon!).</p>

<p><em>Extending the CloudBees Jenkins Distribution container image with plugins and JCasC</em></p>

<pre><code class="language-Dockerfile">FROM cloudbees/cloudbees-jenkins-distribution:2.164.3.2

# optional, but you might want to let everyone know who is responsible for their Jenkins ;)
LABEL maintainer &quot;kmadel@cloudbees.com&quot;

#set java opts variable to skip setup wizard; plugins will be installed via license activated script
ENV JAVA_OPTS=&quot;-Djenkins.install.runSetupWizard=false&quot;
#skip setup wizard; per https://github.com/jenkinsci/docker/tree/master#preinstalling-plugins
RUN echo 2.0 &gt; /usr/share/jenkins/ref/jenkins.install.UpgradeWizard.state

# diable cli
ENV JVM_OPTS -Djenkins.CLI.disabled=true -server
# set your timezone
ENV TZ=&quot;/usr/share/zoneinfo/America/New_York&quot;

#config-as-code plugin configuration
COPY config-as-code.yml /usr/share/jenkins/config-as-code.yml
ENV CASC_JENKINS_CONFIG /usr/share/jenkins/config-as-code.yml

# use CloudBees' update center to ensure you don't allow any really bad plugins
ENV JENKINS_UC http://jenkins-updates.cloudbees.com

#install suggested and additional plugins
COPY plugins.txt /usr/share/jenkins/ref/plugins.txt
COPY jenkins-support /usr/local/bin/jenkins-support
COPY install-plugins.sh /usr/local/bin/install-plugins.sh
RUN bash /usr/local/bin/install-plugins.sh &lt; /usr/share/jenkins/ref/plugins.txt
</code></pre>

<h1 id="use-plugins-you-need-and-no-more">Use Plugins You Need and No More</h1>

<p>So, don&rsquo;t avoid Jenkins plugins - they are an important part of what makes Jenkins great and add critical features to the way you will use Jenkins - but be smart about the plugins you use and keep your application delivery your primary focus - not your CI tool.</p>
]]></content>
        </item>
        
        <item>
            <title>Extending Jenkins X for Traditional Deployments with CloudBees Flow</title>
            <link>https://cb-technologists.github.io/posts/jenkins-x-flow-integration/</link>
            <pubDate>Wed, 29 May 2019 12:47:46 -0400</pubDate>
            
            <guid>https://cb-technologists.github.io/posts/jenkins-x-flow-integration/</guid>
            <description>Jenkins X is quickly becoming the de facto standard for high performing teams wanting to do CI/CD in a highly scalable and fault tolerant environment. For those who haven’t gotten the opportunity to try out Jenkins X, it allows teams to run CI/CD workloads natively in a Kubernetes environment while taking advantage of modern operating patterns like GitOps and serverless architectures. For teams wanting to modernize their continuous integration and continuous deployment capabilities, Jenkins X is the go to solution.</description>
            <content type="html"><![CDATA[

<p><a href="https://jenkins-x.io">Jenkins X</a> is quickly becoming the de facto standard for high performing teams wanting to do CI/CD in a highly scalable and fault tolerant environment. For those who haven’t gotten the opportunity to try out Jenkins X, it allows teams to run CI/CD workloads natively in a Kubernetes environment while taking advantage of modern operating patterns like GitOps and serverless architectures. For teams wanting to modernize their continuous integration and continuous deployment capabilities, Jenkins X is the go to solution.</p>

<p>In today’s heterogenous technology environment, most organizations tend to have a mix of modern cloud native architectures as well as more traditional workloads which get deployed either on-prem or within the cloud. In the latter case, a combination of Jenkins X (performing CI steps) and CloudBees Flow (handling the deployment) can add a huge amount of flexibility and power to a Continuous Delivery process.  The combination of Jenkins X and CloudBees Flow also brings improved visibility and tracability across the application landscape.</p>

<p>Jenkins X can be easily extended to accommodate any type of workload required - it can be a full end to end CI/CD tool for building, deploying, and running applications all within a Kubernetes cluster, or it can handle CI while offloading other release and deployment tasks to another solution.  In this blog post we’re going to cover how Jenkins X can be extended to offload release/deployment tasks to <a href="https://www.cloudbees.com/cloudbees-acquires-electric-cloud">CloudBees Flow</a>.  We will accomplish this by extending the maven Jenkins X build pack in order to call the CloudBees Flow REST API as part of the Jenkins X pipeline execution.</p>

<h1 id="extending-jenkins-x">Extending Jenkins X</h1>

<p>For the purposes of this blog, we’re going to be focusing on the Jenkins X serverless pipeline execution engine with Tekton (See <a href="https://jenkins-x.io/architecture/jenkins-x-pipelines/">https://jenkins-x.io/architecture/jenkins-x-pipelines/</a>). There are two main ways to customize a Jenkins X pipeline in order to integrate with CloudBees Flow.  The first and simplest would be to modify the jenkins-x.yml (more information on Jenkins X pipelines: <a href="https://jenkins-x.io/architecture/jenkins-x-pipelines/#differences-to-jenkins-pipelines">https://jenkins-x.io/architecture/jenkins-x-pipelines/#differences-to-jenkins-pipelines</a> and the jenkins-x.yml file) pipeline file in the source code repo for the project we’re going to build.  The other way is to extend the <a href="https://jenkins-x.io/architecture/build-packs/">Jenkins X build packs</a> and modify the build pack for the language/build tool you want to use.  Both will work, but by forking the build packs you can get reuse across multiple projects using the build pack you extend. In this example, we’ll walk through how to extend the Jenkins X build packs.</p>

<h2 id="creating-our-cluster-and-installing-jenkins-x">Creating our Cluster and Installing Jenkins X</h2>

<p>To start, we’ll fork the Jenkins X Kubernetes build packs into our own repository: <a href="https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes">https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes</a>.  Later we&rsquo;ll be extending the maven build pack to support a REST API call into CloudBees Flow.</p>

<p>Now it’s time to create a Kubernetes cluster on GKE using Jenkins X and <a href="https://github.com/tektoncd/pipeline">Tekton</a>.  In this case, we&rsquo;re starting by creating a cluster from scratch, but Jenkins X can also be installed into an existing Kubernetes cluster if you already have one available by using the <code>jx install</code> command:</p>

<pre><code class="language-bash">jx create cluster gke --tekton --no-tiller
</code></pre>

<p>Fill out the options.  For example:</p>

<pre><code class="language-shell">$  jx create cluster gke
Your browser has been opened to visit:

    https://accounts.google.com/o/oauth2/auth?redirect_uri=....


? Google Cloud Project: jhendrick-ckcd
Updated property [core/project].
Lets ensure we have container and compute enabled on your project
No apis need to be enable as they are already enabled: container compute
No cluster name provided so using a generated one: crownprong
? What type of cluster would you like to create Zonal
? Google Cloud Zone: us-west1-a
? Google Cloud Machine Type: n1-standard-4
? Minimum number of Nodes (per zone) 3
? Maximum number of Nodes 5
? Would you like use preemptible VMs? No
? Would you like to access Google Cloud Storage / Google Container Registry? No
Creating cluster...
Initialising cluster ...
? Select Jenkins installation type: Serverless Jenkins X Pipelines with Tekton
Setting the dev namespace to: jx
Namespace jx created 
</code></pre>

<p>Create an ingress controller if one doesn’t exist and setup the domain or use the default *.nip.io address if you don’t have one.  Go through the prompts and then configure your GitHub credentials.  Create an API token using the URL provided if you don’t have one:</p>

<pre><code class="language-shell">If you don't have a wildcard DNS setup then setup a DNS (A) record and point it at: 35.197.85.1 then use the DNS domain in the next input...
? Domain 35.197.85.1.nip.io
nginx ingress controller installed and configured
? Would you like to enable Long Term Storage? A bucket for provider gke will be created No
Lets set up a Git user name and API token to be able to perform CI/CD

Creating a local Git user for GitHub server
? GitHub user name: jhendrick
To be able to create a repository on GitHub we need an API Token
Please click this URL https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo

Then COPY the token and enter in into the form below:

? API Token: ****************************************
Select the CI/CD pipelines Git server and user
? Do you wish to use GitHub as the pipelines Git server: Yes
Creating a pipelines Git user for GitHub server
To be able to create a repository on GitHub we need an API Token
Please click this URL https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo

Then COPY the token and enter in into the form below:

? API Token: ****************************************
Setting the pipelines Git server https://github.com and user name jhendrick.
Saving the Git authentication configuration
</code></pre>

<p>In the setup we’re going to choose the Kubernetes workloads option and later modify the kubernetes workload build packs to include the CloudBees Flow specific steps:</p>

<pre><code class="language-shell">? Pick default workload build pack: [Use arrows to move, space to select, type to filter]
&gt; Kubernetes Workloads: Automated CI+CD with GitOps Promotion
Library Workloads: CI+Release but no CD
</code></pre>

<h2 id="editing-the-build-packs">Editing the Build Packs</h2>

<p>You can use your favorite IDE but in this case, we&rsquo;ll modify the Jenkins X build packs in VS Code with the YAML Language extension installed (<a href="https://jenkins-x.io/architecture/jenkins-x-pipelines/#editing-in-vs-code">https://jenkins-x.io/architecture/jenkins-x-pipelines/#editing-in-vs-code</a>) for validation as recommended by the Jenkins X team.</p>

<p>This example is going to focus on a sample Spring Boot application using Maven.  To start we&rsquo;ll modified the maven build pack in our forked build pack repo (<a href="https://github.com/jhendrickCB/jenkins-x-kubernetes/blob/master/packs/maven/pipeline.yaml):">https://github.com/jhendrickCB/jenkins-x-kubernetes/blob/master/packs/maven/pipeline.yaml):</a></p>

<pre><code class="language-yaml">extends:
 import: classic
 file: maven/pipeline.yaml
pipelines:
 release:
   build:
     steps:
     - sh: jx step post build --image $DOCKER_REGISTRY/$ORG/$APP_NAME:\$(cat VERSION)
       name: post-build
   promote:
     steps:
     - sh: jx step changelog --version v\$(cat ../../VERSION)
       name: changelog
     - comment: call CloudBees Flow to run a release
       sh: &gt;
         curl -X POST --header &quot;Authorization: Basic $(jx step credential -s flow-token -k token)&quot; --header &quot;Content-Type: application/json&quot; --header &quot;Accept: application/json&quot; -d &quot;{}&quot; &quot;https://ps9.ecloud-kdemo.com/rest/v1.0/pipelines?pipelineName=my_pipeline&amp;projectName=my_project&quot; --insecure
       name: cloudbees-flow-release
</code></pre>

<p>Compare the original build pack for maven found here: <a href="https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs/maven">https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs/maven</a> vs. our forked build pack.  We’ve removed all the references to skaffold, watch, and helm since we’re no longer having Jenkin’s X handle the deployment to our Kubernetes cluster.  We’ve also updated the pipeline file to make an API call into our CloudBees Flow server using <a href="https://curl.haxx.se/">cURL</a>:</p>

<pre><code class="language-bash">curl -X POST --header &quot;Authorization: Basic $(jx step credential -s flow-token -k token)&quot; --header &quot;Content-Type: application/json&quot; --header &quot;Accept: application/json&quot; -d &quot;{}&quot; &quot;https://ps9.ecloud-kdemo.com/rest/v1.0/pipelines?pipelineName=my_pipeline&amp;projectName=my_project&quot; --insecure
</code></pre>

<p>The above API call into CloudBees Flow tells Flow to run a pipeline called <code>my_pipeline</code> within a project called <code>my_project</code>.</p>

<p>You’ll also notice that we’re using a Jenkins X feature (<code>jx step credential</code>) to get our secret, <code>flow-token</code>, which we created previously so that we can authenticate to the Flow Rest API. Note that there are many other possible ways to call into the CloudBees Flow API’s besides cURL such as the command line tool <a href="http://docs.electric-cloud.com/eflow_doc/9_0/API/HTML/FlowAPI_Guide_9_0.htm#EFlow_api/usingAPI.htm?Highlight=ectool">ectool</a> as well as <a href="http://docs.electric-cloud.com/eflow_doc/9_0/API/HTML/FlowAPI_Guide_9_0.htm#EFlow_api/usingAPI.htm%3FTocPath%3DUsing%2520the%25C2%25A0ElectricFlow%2520Perl%2520API%7C_____0">perl</a> or <a href="http://docs.electric-cloud.com/eflow_doc/9_0/API/HTML/FlowAPI_Guide_9_0.htm#EFlow_api/UsingGroovy.htm%3FTocPath%3D_____11">groovy libraries</a>.  Also note that for a production environment we would want to setup the proper certificates rather than using the <code>--insecure parameter</code>.</p>

<p>Next, we need to tell Jenkins X to use our new build pack:</p>

<pre><code class="language-shell">$ jx edit buildpack -u https://github.com/jhendrickCB/jenkins-x-kubernetes -r master -b

Setting the team build pack to  repo: https://github.com/jhendrickCB/jenkins-x-kubernetes ref: master
</code></pre>

<p>Since we have to authenticate when calling the <a href="http://docs.electric-cloud.com/eflow_doc/9_0/API/HTML/FlowAPI_Guide_9_0.htm">Flow REST API</a>, we’ll create a Kubernetes secret to store our username/password basic authentication token:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
 name: flow-token
type: Opaque
data:
 token: &lt;Basic Auth Token&gt;
</code></pre>

<p>Note: In this case, the <code>&lt;Basic Auth Token&gt;</code> will take the form of <code>username:password</code> base64 encoded.  Take note that we’ll actually need to base64 encode our username:password token twice as it will get base64 decoded automatically when we access it later.</p>

<p>To apply the secret in our Kubernetes cluster, we can save our secret to a file called <code>flow-token-secret.yaml</code> and run the command:</p>

<pre><code class="language-bash">kubectl apply -f flow-token-secret.yaml
</code></pre>

<h1 id="creating-a-sample-spring-boot-project">Creating a Sample Spring Boot Project</h1>

<p>To test out our new build pack, we’ll use Jenkins X’s capability to create a quick start project for a Spring Boot microservice:</p>

<pre><code class="language-bash">jx create spring -d web -d actuator
</code></pre>

<p>Follow the prompts to create the Spring Boot project and setup the repository on your GitHub account:</p>

<pre><code class="language-shell">$ jx create spring -d web -d actuator
Using Git provider GitHub at https://github.com
? Do you wish to use jhendrick as the Git user name? Yes


About to create repository  on server https://github.com with user jhendrick
? Which organisation do you want to use? jhendrickCB
? Enter the new repository name:  jx-spring-flowdemo


Creating repository jhendrickCB/jx-spring-flowdemo
? Language: java
? Group: com.example
Created Spring Boot project at /Users/jhendrick/Cloudbees/jx-spring-flowdemo
The directory /Users/jhendrick/Cloudbees/jx-spring-flowdemo is not yet using git
? Would you like to initialise git now? Yes
? Commit message:  Initial import

Git repository created
selected pack: /Users/jhendrick/.jx/draft/packs/github.com/jhendrickCB/jenkins-x-kubernetes/packs/maven

replacing placeholders in directory /Users/jhendrick/Cloudbees/cloudbees-days/kops-cluster/jx-spring-flowdemo
app name: jx-spring-flowdemo, git server: github.com, org: jhendrickcb, Docker registry org: jhendrickcb
skipping directory &quot;/Users/jhendrick/Cloudbees/jx-spring-flowdemo/.git&quot;
skipping ignored file &quot;/Users/jhendrick/Cloudbees/jx-spring-flowdemo/HELP.md&quot;
Pushed Git repository to https://github.com/jhendrickCB/jx-spring-flowdemo

Creating GitHub webhook for jhendrickCB/jx-spring-flowdemo for url http://hook.jx.35.197.85.1.nip.io/hook

Watch pipeline activity via:    jx get activity -f jx-spring-flowdemo -w
Browse the pipeline log via:    jx get build logs jhendrickCB/jx-spring-flowdemo/master
Open the Jenkins console via    jx console
You can list the pipelines via: jx get pipelines
When the pipeline is complete:  jx get applications

For more help on available commands see: https://jenkins-x.io/developing/browsing/

Note that your first pipeline may take a few minutes to start while the necessary images get downloaded!
</code></pre>

<p>Once created, the project should build and run automatically.  If everything worked, we should see our Spring Boot project built with Maven, artifacts uploaded automatically to our Nexus repository and then our CloudBees Flow pipeline executed within our CloudBees Flow server.</p>

<p>If for some reason, we made a mistake, the pipeline can be re-run by using:</p>

<pre><code class="language-bash">jx start pipeline
</code></pre>

<p>To debug, build logs can be checked with:</p>

<pre><code class="language-bash">jx get build logs 
</code></pre>

<p>Or more specifically with our project name:</p>

<pre><code class="language-bash">jx get build logs jhendrickCB/jx-spring-flowdemo/master
</code></pre>

<p>We can get build activity with:</p>

<pre><code class="language-bash">jx get activity -w
</code></pre>

<p>Or more specifically:</p>

<pre><code class="language-bash">jx get activity -f jx-spring-flowdemo -w
</code></pre>

<h1 id="in-conclusion">In Conclusion</h1>

<p>In the above example we were able to use Jenkins X to build our application as well as store the built artifacts, and then utilize CloudBees flow to handle execution of our release pipeline.  This allows us to take advantage of the scalability and efficiency of Jenkins X while leveraging the power and control of CloudBees Flow for managing the release.</p>

<p>For organizations who want to take advantage of modern CI/CD on Jenkins X but are not yet &ldquo;all in&rdquo; on Kubernetes and still deploying traditional applications, this provides a very solid approach to achieving Continuous Delivery.</p>
]]></content>
        </item>
        
        <item>
            <title>Introducing the Technologists, A CloudBees Solution Architecture Team</title>
            <link>https://cb-technologists.github.io/posts/introducing-technologists/</link>
            <pubDate>Thu, 23 May 2019 19:10:46 -0400</pubDate>
            
            <guid>https://cb-technologists.github.io/posts/introducing-technologists/</guid>
            <description>The Technologists is a new team of CloudBees Solution Architects. Technologists have a passion for emerging technologies, continuously learning and teaching through thought leadership, providing technical direction within CloudBees and the broader tech community, and driving the best technical solutions for customers.
 Technical integrity is of the utmost importance for a Technologist - always providing the RIGHT solution. We are Technologists focused on providing best practices, solutions, and adoption paths to organizations navigating software delivery transformations with leading edge technologies.</description>
            <content type="html"><![CDATA[<p>The Technologists is a new team of CloudBees Solution Architects. Technologists have a passion for emerging technologies, continuously learning and teaching through thought leadership, providing technical direction within CloudBees and the broader tech community, and driving the best technical solutions for customers.</p>

<ul>
<li>Technical integrity is of the utmost importance for a Technologist - always providing the RIGHT solution.</li>
<li>We are Technologists focused on providing best practices, solutions, and adoption paths to organizations navigating software delivery transformations with leading edge technologies.</li>
<li>Technologists are Thought Leaders internally at CloudBees and in the DevOps and wider Tech community - writing blog posts, speaking at conferences and meetups, contributing to open source projects on technologies and technical practices related to CloudBees&rsquo; products and to the DevOps space in general.</li>
</ul>

<p>From Cloud Native CD to microservice and even nanoservice architecture to service meshes and API gateways, Technologists are early adopters of the best of the best emerging technologies related to software delivery. Look to this website for interesting posts on a number of technical subjects related to CloudBees&rsquo; products and DevOps in general, like:</p>

<ul>
<li>Native Kubernetes Continuous Delivery</li>
<li>Jenkins Plugins: The Good, the Bad and the Ugly</li>
<li>The State of DevOps Analytics</li>
</ul>

<p>Stay tuned for more&hellip;</p>
]]></content>
        </item>
        
    </channel>
</rss>
