<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on CloudBees Technologists</title>
        <link>https://technologists.dev/posts/</link>
        <description>Recent content in Posts on CloudBees Technologists</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 20 Oct 2019 09:05:15 -0400</lastBuildDate>
        <atom:link href="https://technologists.dev/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Securely Using Cloud Services from Jenkins Kubernetes Agents</title>
            <link>https://technologists.dev/posts/best-practices-for-cloudbees-core-jenkins-on-kubernetes/securely-using-cloud-services-from-jenkins-kubernetes-agents/</link>
            <pubDate>Sun, 20 Oct 2019 09:05:15 -0400</pubDate>
            
            <guid>https://technologists.dev/posts/best-practices-for-cloudbees-core-jenkins-on-kubernetes/securely-using-cloud-services-from-jenkins-kubernetes-agents/</guid>
            <description>In the second part of this series on best practices for Jenkins (and CloudBees Core) on Kubernetes we will continue to look at security. In this post we will look at how to reduce security risk of using cloud services from Jenkins Kubernetes agents, similar to how the previous post in this series showed how Kubernetes Pod Security Policies can be used with Jenkins Kubernetes agents to limit the security risk of Jenkins agent containers.</description>
            <content type="html"><![CDATA[

<p>In the second part of this series on <a href="/series/best-practices-for-cloudbees-core-jenkins-on-kubernetes/">best practices for Jenkins (and CloudBees Core) on Kubernetes</a> we will continue to look at security. In this post we will look at how to reduce security risk of using cloud services from Jenkins Kubernetes agents, similar to how <a href="/posts/best-practices-for-cloudbees-core-jenkins-on-kubernetes/core-psp/">the previous post in this series</a> showed how Kubernetes Pod Security Policies can be used with Jenkins Kubernetes agents to limit the security risk of Jenkins agent containers.</p>

<h2 id="the-problem">The Problem</h2>

<p>I have already established in several other posts <a href="https://kurtmadel.com/posts/native-kubernetes-continuous-delivery/native-k8s-cd/">why Kubernetes is an excellent platform for CD</a>. However, accessing cloud services from Kubernetes CD jobs that require Identity Access Management (IAM) permissions for AWS, Azure and Google Cloud Platform (GCP), a typical step in many CD pipelines, presents a number of security challenges and usually falls short of <a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege">the principle of least privilege</a>. Existing approaches for accessing cloud services requiring IAM permissions from Kubernetes based Jenkins agents have numerous security implications and complexities. Furthermore, the most commonly proposed solutions do not meet many organizations&rsquo; security requirements. In addition to the security implications, some of these approaches aren&rsquo;t native cloud provider solutions and have to be self managed, and also have performance and reliability issues.</p>

<p>There is a better way for providing <strong>least privilege</strong> IAM permissions to Jenkins Kubernetes agents - at least for the Amazon Elastic Kubernetes Service (EKS) and Google Kubernetes Engine (GKE). But before we look at a <em>better way</em>, I am going to review the security implications for some of the other most commonly used solutions.</p>

<h2 id="the-old-way">The Old Way</h2>

<h4 id="create-cloud-iam-credentials-and-store-them-as-jenkins-credentials-or-kubernetes-secrets">Create Cloud IAM credentials and store them as Jenkins Credentials or Kubernetes Secrets</h4>

<p>First, lets walk through the steps for setting up a GCP IAM Service Account Key (<a href="https://cloud.google.com/kubernetes-engine/docs/tutorials/authenticating-to-cloud-platform">a previously recommended approach for GKE</a>) to use from a Jenkins Pipeline running on a <a href="https://github.com/jenkinsci/kubernetes-plugin#pipeline-support">Kubernetes agent</a>. For this example we are going to deploy a container image (from a public Google Container Registry (GCR)) to <a href="https://cloud.google.com/run/">Google Cloud Run</a>. The IAM Service Account will be created in the same GCP project where CloudBees Core (Jenkins) is running on the Google Kubernetes Engine (GKE).</p>

<ol>
<li>Create an IAM Service Account with the <strong>Cloud Run Admin</strong> permissions to use the gcloud SDK to execute the <code>gcloud run deploy</code> command.</li>
<li>Export a JSON key file for that IAM Service Account.</li>
<li>Create a Kubernetes <strong>Secret</strong> with the contents of that key file in the same Kubernetes Namespace where the Jenkins Kubernetes agent will run.</li>
<li>Configure the Jenkins Kubernetes Pod template for the agent to mount the Kubernetes <strong>Secret</strong> for the IAM Service Account key file.</li>
<li>Inside the Jenkins Pipeline, use the gcloud SDK to authenticate using the key file from the Kubernetes Secret.</li>
<li>Execute the gcloud command for deploying to Cloud Run.</li>
</ol>

<p><em>The Jenkins Kubernetes Pod template:</em></p>

<pre><code class="language-yaml">kind: Pod
metadata:
  name: cloud-run
spec:
  - name: gcloud
    image: google/cloud-sdk:252.0.0-slim
    command:
    - cat
    tty: true
    volumeMounts:
      - name: gcp-credential
        mountPath: /home/
    env:
      - name: GOOGLE_CLOUD_KEYFILE_JSON
        value: &quot;/home/gcp-service.json&quot;
  volumes:
    - name: gcp-credential
      secret:
        secretName: gcp-credential
</code></pre>

<p><em>The Jenkins Pipeline:</em></p>

<pre><code class="language-groovy">pipeline {
  agent {
    kubernetes {
      label 'cloud-run'
      yamlFile 'pod.yml'
    }
  }
  stages {
    stage('Cloud Run Deploy') {
      steps {
        container('gcloud'){
          sh '''
            gcloud auth activate-service-account --key-file=$GOOGLE_CLOUD_KEYFILE_JSON
            gcloud beta run deploy bee-cd --image gcr.io/core-workshop/bee-cd:65 --allow-unauthenticated --platform managed --region us-east1
            echo &quot;$(cat $GOOGLE_CLOUD_KEYFILE_JSON)&quot; //Don't do this!
          '''
        }
      }
    }
  }
}
</code></pre>

<p>Some of the issues with this approach include:</p>

<ol>
<li>These type of credentials are long-lived.

<ul>
<li>GCP IAM Service Account keys are valid for 10 years</li>
<li>AWS credentials (Access Key ID and Secret Access Key) - require custom rotation policies, but that is typically set to 90 days</li>
</ul></li>
<li>These types of credentials are valid no matter where they are used, whether it is from a Jenkins agent running in EKS or GKE, or from the shell of a personal computer.</li>
<li>Storing these credentials as Jenkins Credentials or Kubernetes Secrets is inherently insecure.

<ul>
<li>It is relatively straightforward to print out Jenkins Credentials or Kubernetes Secrets to Jenkins build logs in plain text.</li>
<li>Unless extra security precautions are taken, Kubernetes Secrets are typically stored as base64 encoded strings accessible by all Pods that run in that Namespace.</li>
</ul></li>
<li>Many organizations wonâ€™t allow the use of these type of credentials in Jenkins, and for good reason.</li>
<li>The management overhead of inventory and rotation makes this a less than ideal method for authenticating.</li>
</ol>

<h3 id="associate-an-iam-object-with-a-cloud-instance-and-or-instance-group-node-pool">Associate an IAM object with a cloud instance and/or instance group (node pool)</h3>

<ol>
<li>All Kubernetes Pods created on the node/node pool with an instance profile will have access to the same set of cloud IAM permissions - regardless of the Kubernetes Namespace these Pods run in.</li>
<li>The principle of least privilege makes this method of authenticating less than ideal.</li>
</ol>

<h3 id="third-party-solutions">Third-party solutions</h3>

<p>For EKS: <a href="https://github.com/jtblin/kube2iam">kube2iam</a> and <a href="https://github.com/uswitch/kiam">kiam</a></p>

<ol>
<li>It isn&rsquo;t a cloud provider solution so good luck with support.</li>
<li>You have to install and manage it.</li>
<li>Performance issues. Kiam was specifically <a href="https://www.bluematador.com/blog/iam-access-in-kubernetes-kube2iam-vs-kiam">created because of critical security and performance issues with kube2iam</a>.</li>
<li>Require running a Kubernetes services that has the ability to provide all permissions you need across all Jenkins Pipeline jobs.</li>
</ol>

<p>For GKE: <a href="https://github.com/imduffy15/k8s-gke-service-account-assigner">k8s-gke-service-account-assigner</a></p>

<ol>
<li>Basically a rewrite of kube2iam for GKE with all the same issues listed above.</li>
</ol>

<h2 id="a-better-way">A Better Way</h2>

<p>Kubernetes v1.11 introduced <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection">Service Account Token Volume Projection</a> and that feature became beta in v1.12. This allows projecting a temporary Kubernetes Service Account Token into a Pod and allows specifying the audience and validity duration. Furthermore, this projected Service Account token becomes invalid once the pod is deleted.</p>

<p>AWS and GCP both created new offerings around this Kubernetes feature for their respective managed Kubernetes platforms. AWS created <a href="https://docs.aws.amazon.com/en_pv/eks/latest/userguide/iam-roles-for-service-accounts-technical-overview.html">IAM roles for service accounts</a> for EKS and Google Cloud created <a href="https://cloud.google.com/blog/products/containers-kubernetes/introducing-workload-identity-better-authentication-for-your-gke-applications">Workload Identity</a> for GKE. Both offerings have a similar architecture that allow binding native cloud IAM permissions (via AWS IAM Roles and GCP IAM Service Accounts) to a specific Kubernetes Service Account in a specific Namespace.</p>

<p>These <strong>bound</strong> permissions have several advantages over the approaches mentioned above:</p>

<ol>
<li>No file or passwords to store anywhere - no Jenkins Credentials, no Kubernetes Secrets.</li>
<li>Kubernetes Pods in a given Namespace and created with a specific Kubernetes ServiceAccount only have the cloud IAM permissions you want them to have and come closer than any other solution to achieving the principle of least privilege.</li>
<li>They are token based and the generated tokens can only be used from the Kubernetes Namespace and Service Account they are bound.</li>
<li>The tokens are short-lived and are destroyed when the Pod using it is destroyed.</li>
<li>The tokens are never actually exposed to the Jenkins Pipeline as they are integrated with the cloud provider SDKs for automatic authentication and authorization. No extra authentication step is necessary in your Jenkins Pipeline.</li>
</ol>

<h3 id="using-with-oss-jenkins-and-cloudbees-core-enterprise-jenkins">Using with OSS Jenkins and CloudBees Core (Enterprise Jenkins)</h3>

<p>These IAM to Kubernetes Service Account binding solutions can be used with OSS Jenkins, but they are even more effective when combined with CloudBees Core on Kubernetes. That is because <a href="https://docs.cloudbees.com/docs/cloudbees-core/latest/cloud-admin-guide/managing-masters">CloudBees Core provides dynamic provisioning and easy management of many team specific Jenkins Masters that we refer to as Managed Masters</a> and <a href="https://docs.cloudbees.com/docs/cloudbees-core/latest/cloud-admin-guide/managing">each Managed Master can be easily provisioned into its own Kubernetes Namespace</a>. This allows you to utilize a default standard Kubernetes Cloud across all Masters that should have the same Kubernetes and IAM permissions, but also to easily manage Jenkins Master specific Kubernetes Cloud configurations for individual teams that need additional IAM permissions that you don&rsquo;t want all teams to have.</p>

<blockquote>
<p>NOTE: If you do want to have multiple teams on one Jenkins Master you can create multiple Jenkins Kubernetes Cloud configurations - each in its own Kubernetes Namespace - and then leverage the Kubernetes plugin capability to <a href="https://github.com/jenkinsci/kubernetes-plugin#restricting-what-jobs-can-use-your-configured-cloud">restrict pipeline support to authorized folders</a>. You will then have to apply proper RBAC configuration so that only the users you want have access to configure folders to use one or more protected Jenkins Kubernetes clouds, and you will have to create the jobs that need to use the more permissive Kubernetes cloud configuration in those folders.</p>
</blockquote>

<p>The default configuration for the <a href="https://github.com/jenkinsci/kubernetes-plugin">Jenkins Kubernetes Cloud plugin</a> uses the same Namespace and Kubernetes Service Account as the Jenkins Master it is configured for - when the Jenkins Master is also running on Kubernetes. This is typically the case with Core on Kubernetes and also for OSS Jenkins. To fully leverage binding IAM permissions to Kubernetes Service Accounts we must revisit how we set-up and use the Jenkins Kubernetes Clouds for agents.</p>

<h4 id="create-a-unique-kubernetes-namespace-and-serviceaccount-for-each-managed-master-and-kubernetes-cloud">Create a unique Kubernetes Namespace and ServiceAccount for each Managed Master and Kubernetes Cloud</h4>

<p>Placing each of your Jenkins Masters into their own Kubernetes Namespace provides and extra layer of security that isn&rsquo;t just limited to binding IAM credentials - it also protects Kubernetes Secrets from other Jenkins Masters and this will provide a more secure integration for managing Jenkins credentials with JCasC and Kubernetes Secrets. Creating a unique Kubernetes Cloud per Jenkins Master can be <a href="https://github.com/kypseli/workshop-mm-jcasc/blob/ops/jcasc.yml">managed more easily with JCasC</a>, as we will manage the Kubernetes cloud configuration and the Jenkins credential used to connect to Kubernetes for the Master specific cloud. The Jenkins credential used for the Kubernetes Cloud configuration will be the Kubernetes Service Account token stored in Jenkins as a Secret Text credential - stored at the Master level (not CloudBees Core Operations Center), so only that Jenkins Master has access to it. The Kubernetes Service Account token should be managed as a Kubernetes Secret in the same Namespace as the Managed Master is created so it can be dynamically injected into the JCasC configuration for that Master. No other Core Managed Masters will have access to this Kubernetes Secret.</p>

<blockquote>
<p>NOTE: This approach depends on managing what Kubernetes Namespaces are used by Managed Masters. You must not allow untrusted users to have access to configure Managed Masters on your Core Operations Center, otherwise they could use a Namespace they shouldn&rsquo;t have access to - that is why <a href="https://github.com/kypseli/demo-mm-jcasc/blob/cloud-run/groovy/createManagedMaster.groovy">I prefer to do it as code</a>.</p>
</blockquote>

<h4 id="example-using-workload-identity-with-a-core-managed-master-to-deploy-a-container-image-to-google-cloud-run">Example: Using Workload Identity with a Core Managed Master to Deploy a Container Image to Google Cloud Run</h4>

<ol>
<li><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#enable_workload_identity_on_an_existing_cluster">Enable Workload Identity for your GKE cluster</a> - this only has to be done once per cluster. You also have to enable each pre-existing node pool where you want to use Workload Identity.</li>
<li>Create an IAM Service Account with the <strong>Cloud Run Admin</strong> permissions that will allow us to use the <a href="https://cloud.google.com/sdk/">gcloud SDK</a> to execute the <code>gcloud run deploy</code> command.</li>

<li><p>Create a new Kubernetes Namespace and Service Account that will be unique to the Core Managed Master. Note the <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server"><code>automountServiceAccountToken</code></a> is set to <code>false</code> - this will require that you configure the Jenkins Kubernetes cloud with a <strong>credential</strong> for the same Kubernetes Service Account that is used to create the Managed Master as it will no longer have the Service Account token automatically mounted to its Pod.</p>

<pre><code class="language-yaml">  apiVersion: v1
  kind: Namespace
  metadata:
    name: cloud-run
  ---
  apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: cloud-run-sa
    namespace: cloud-run
  automountServiceAccountToken: false
</code></pre>

<ol>
<li>Bind the GCP IAM Service Account (<code>core-cloud-run@core-workshop.iam.gserviceaccount.com</code> in the example below) to the GKE Namespace and Service Account (<code>cloud-run</code> and <code>cloud-run-sa</code> in the example below):</li>
</ol>

<pre><code class="language-shell">  gcloud iam service-accounts add-iam-policy-binding \
    --role roles/iam.workloadIdentityUser \
    --member &quot;serviceAccount:core-workshop.svc.id.goog[cloud-run/cloud-run-sa]&quot; \
    core-cloud-run@core-workshop.iam.gserviceaccount.com
</code></pre></li>

<li><p><a href="https://docs.cloudbees.com/docs/cloudbees-core/latest/cloud-admin-guide/managing">Create a Managed Master in the Master specific Kubernetes Namespace</a> - here is <a href="https://github.com/kypseli/demo-mm-jcasc/tree/cloud-run">an example of automating this with code</a>. Here is an example of a Managed Master Kubernetes yaml configuration that specifies a unique Kubernetes Service Account - note the <code>serviceAccount</code> value matches the Kubernetes Service Account we created above:</p>

<pre><code class="language-yaml">  ---
  kind: StatefulSet
  spec:
    template:
      metadata:
        annotations:
            cluster-autoscaler.kubernetes.io/safe-to-evict: &quot;false&quot;
      spec:
        containers:
        - name: jenkins
          env:
            # With the help of SECRETS environment variable
            # we point Jenkins Configuration as Code plugin the location of the secrets
            - name: SECRETS
              value: /var/jenkins_home/mm-secrets
            - name: CASC_JENKINS_CONFIG
              value: https://raw.githubusercontent.com/kypseli/demo-mm-jcasc/cloud-run/jcasc.yml
          volumeMounts:
          - name: mm-secrets
            mountPath: /var/jenkins_home/mm-secrets
            readOnly: true
        volumes:
        - name: mm-secrets
          secret:
            secretName: mm-secrets
        nodeSelector:
          type: master
        serviceAccount: cloud-run-sa
        serviceAccountName: cloud-run-sa
        securityContext:
          runAsUser: 1000
          fsGroup: 1000 
</code></pre>

<ol>
<li>Create <a href="https://github.com/kypseli/demo-mm-jcasc/blob/cloud-run/jcasc.yml#L8">a Master specific Jenkins Kubernetes cloud configuration</a> that uses the Master specific Namespace and Kubernetes Service Account. To ensure that we are using the <code>cloud-run-sa</code> Kubernetes Service Account for all Pod based agents we <a href="https://github.com/kypseli/demo-mm-jcasc/blob/cloud-run/jcasc.yml#L36">explicitly set this on a Kubernetes cloud Pod template</a> that is <a href="https://github.com/kypseli/demo-mm-jcasc/blob/cloud-run/jcasc.yml#L12">configured as the <code>defaultsProviderTemplate</code></a>.</li>
<li>Update the <code>cloud-run-sa</code> Kubernetes Service Account with the <code>iam.gke.io/gcp-service-account</code> annotation:</li>
</ol>

<pre><code class="language-shell">  kubectl annotate serviceaccount \
    --namespace cloud-run \
    cloud-run-sa \
    iam.gke.io/gcp-service-account=core-cloud-run@core-workshop.iam.gserviceaccount.com
</code></pre></li>

<li><p>Create a yaml file based Pod template for the GCP gcloud SDK (<a href="https://github.com/kypseli/core-cloud-run-example/blob/master/pod.yml">source on GitHub</a>):</p>

<pre><code class="language-yaml">  kind: Pod
  metadata:
    name: cloud-run-pod
    namespace: cloud-run
  spec:
    serviceAccountName: cloud-run-sa
    containers:
    - name: gcp-sdk
      image: google/cloud-sdk:252.0.0-slim
      command:
      - cat
      tty: true
</code></pre>

<blockquote>
<p>NOTE: If you look at the example on GitHub you will notice that it also specifies the <code>runAsUser</code> and mounts an <code>emptyDir</code> volume at the path <code>/.config/gcloud/logs</code> - this is because we have enabled Pod Security Policies on the GKE cluster we are using for this example and we don&rsquo;t allow Pods to run as the <code>root</code> (0) user. You can read more about <a href="/posts/best-practices-for-cloudbees-core-jenkins-on-kubernetes/core-psp/">using Pod Security Policies with CloudBees Core Jenkins in the previous post of this series</a>.</p>
</blockquote>

<ol>
<li>Create a Jenkins Declarative Pipeline that uses the above Pod template and executes the gcloud Cloud Run deploy command:</li>
</ol>

<pre><code class="language-groovy">  pipeline {
    agent {
      kubernetes {
        label 'cloud-run'
        yamlFile 'pod.yml'
      }
    }
    stages {
      stage('Cloud Run Deploy') {
        steps {
          container('gcp-sdk'){
            sh 'gcloud beta run deploy bee-cd --image gcr.io/core-workshop/bee-cd:65 --allow-unauthenticated --platform managed --region us-east1'
          }
        }
      }
    }
  }
</code></pre></li>
</ol>

<blockquote>
<p>NOTE: There is no authentication step needed as the glcoud SDK automatically authenticates with the token provided by  Workload Identity. The AWS SDK also doesn&rsquo;t require an explicit assume role step or authentication step.</p>
</blockquote>

<p>One security limitation of the Jenkins Kubernetes plugin is that you can define Pod templates as standalone yaml files to include specifying whatever Kubernetes Namespace and Service Account that you want (this is also a very nice feature for managing your agent Pod template configuration as code). But if we were to create a Pod template specifying the <code>cloud-run</code> Namespace and <code>cloud-run-sa</code> Service Account it will only work from a Master with a Jenkins Kubernetes cloud that is configured with the Namespace and Service Account token that has permissions to create Pods in the <code>cloud-run</code> Namespace. Running it from unauthorized Jenkins Kubernetes clouds will result in the following error:</p>

<pre><code class="language-shell">io.fabric8.kubernetes.client.KubernetesClientException: 
Failure executing: POST at: https://10.11.240.1/api/v1/namespaces/cloud-run/pods. 
Message: pods is forbidden: User &quot;system:serviceaccount:core-demo:jenkins&quot; cannot create resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;cloud-run&quot;.
</code></pre>

<h2 id="a-better-way-but-not-perfect">A Better Way, But Not Perfect</h2>

<p>I know this seems like a lot of steps, but once you have IAM permissions bound to a Kubernetes Service Account set up you can easily update the GCP IAM Service Account with additional permissions, and all the Pods launched with the Master specific Namespace/Service Account will instantly have access to those permissions. And for all of the different solutions for providing cloud IAM permissions to Jenkins Kubernetes based agents - these new IAM binding solutions from AWS and GCP come closest to achieving the principle of least privilege.</p>
]]></content>
        </item>
        
        <item>
            <title>Using Kubernetes Pod Security Policies with CloudBees Core - Jenkins</title>
            <link>https://technologists.dev/posts/best-practices-for-cloudbees-core-jenkins-on-kubernetes/core-psp/</link>
            <pubDate>Wed, 04 Sep 2019 05:05:15 -0400</pubDate>
            
            <guid>https://technologists.dev/posts/best-practices-for-cloudbees-core-jenkins-on-kubernetes/core-psp/</guid>
            <description>What are Pod Security Policies? Although Kubernetes Pod Security Policies are still a beta feature of Kubernetes they are an important security feature that should not be overlooked. Pod Security Policies (PSPs) are built-in Kubernetes resources that allow you to enforce security related properties of every container in your cluster. If a container in a pod does not meet the criteria for an applicable PSP then it will not be scheduled to run.</description>
            <content type="html"><![CDATA[

<h2 id="what-are-pod-security-policies">What are Pod Security Policies?</h2>

<p>Although <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Kubernetes Pod Security Policies</a> are still a <strong>beta</strong> feature of Kubernetes they are an important security feature that should not be overlooked. Pod Security Policies (PSPs) are built-in Kubernetes resources that allow you to enforce security related properties of every container in your cluster. If a container in a pod does not meet the criteria for an applicable PSP then it will not be scheduled to run.</p>

<h2 id="best-practices-for-cloudbees-core-v2-on-kubernetes">Best Practices for CloudBees Core v2 on Kubernetes</h2>

<p>There are <a href="https://rancher.com/blog/2019/2019-01-17-101-more-kubernetes-security-best-practices/">numerous articles</a> <a href="https://www.twistlock.com/2019/06/06/5-kubernetes-security-best-practices/">on security best practices</a> for Kubernetes (to include <a href="https://www.cncf.io/blog/2019/01/14/9-kubernetes-security-best-practices-everyone-must-follow/">this one published on the CNCF blog site</a>). Many of these articles include similar best practices and most, if not all, apply to running Core v2 on Kubernetes. Some of these best practices are inherent in CloudBees&rsquo; documented install of Core v2 on Kubernetes, while others are documented best practices and are recommended next steps after your initial Core v2 installation.</p>

<p>Before we take a look at the best practices that aren&rsquo;t necessarily covered by the CloudBees reference architectures and best practice documentation, I will provide a quick overview of what is already available with an OOTB Core v2 install and highlight some CloudBees documentation that speaks to other best practices for running Core v2 on Kubernetes more securely.</p>

<h3 id="enable-role-based-access-control-rbac">Enable Role-Based Access Control (RBAC)</h3>

<p>Although you can certainly install Core v2 on Kubernetes without RBAC enabled - the CloudBees install for Core v2 comes with RBAC pre-configured. Running Kubernetes with RBAC enabled is typically the default (it is for all the major cloud providers) and is always a recommended security setting.</p>

<h3 id="use-namespaces-to-establish-security-boundaries-separate-sensitive-workloads">Use Namespaces to Establish Security Boundaries &amp; Separate Sensitive Workloads</h3>

<p>CloudBees recommends that you create a <code>namespace</code> specifically for Core v2 as part of the install. CloudBees also recommends establishing boundaries between your CloudBees Jenkins masters and agent workloads by <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-install-guide/gke-install/#_distinct_node_pools">setting up distinct node pools</a> using <a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">taints and tolerations</a> and <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">assigning pods to specific node pools with node selectors</a>.</p>

<h3 id="create-and-define-cluster-network-policies">Create and Define Cluster Network Policies</h3>

<p>Although CloudBees doesn&rsquo;t provide specific Kubernetes Network Policies, CloudBees does recommend using them and <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-reference-architecture/ra-for-eks/#_setting_up_a_private_and_encrypted_network">provides documentation for setting up a private and encrypted network for AWS EKS</a>.</p>

<h3 id="run-a-cluster-wide-pod-security-policy">Run a Cluster-wide Pod Security Policy</h3>

<p>At the time of this post, this is one component that is not documented as part of the CloudBees installation guides for Core v2 on Kubernetes and will be the focus of the rest of this post.</p>

<h2 id="why-should-you-use-pod-security-policies-posts-build-continaer-images"><a href="/posts/build-continaer-images/">Why should you use Pod Security Policies?</a></h2>

<p>From the Kubernetes documentation on Pod Security Policies (PSPs): &ldquo;Pod security policy control is implemented as an optional (<strong>but recommended</strong>) <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy">admission controller</a>.&rdquo; If you read any number of posts on security best practices for Kubernetes, pretty much all of them will mentions PSPs.</p>

<p>A CD platform, like CloudBees Core v2 on Kubernetes, is typically a multi-tenant service where security is of the utmost importance. In addition to multi-tenancy, when running CD workloads on a platform like Kubernetes there are typically other workloads deployed and if any workload does not have proper security configured it can impact all of the workloads running on the cluster.</p>

<p>The combination of PSPs with Kubernetes RBAC, namespaces and workload specific node pools allows for the granular security you need to ensure there are adequate safeguards in place to greatly reduce the risk of unintentional (and intentional) actions that breaks your cluster. PSPs provide additional safeguards along with targeted node pools, namespaces and service accounts. This allows for the flexibility needed by CI/CD users while providing adequate guard rails so they don&rsquo;t negatively impact CD workloads or other important Kubernetes workloads by doing something stupid - accidental or otherwise.</p>

<h2 id="using-pod-security-policies-with-cloudbees-core-v2">Using Pod Security Policies with CloudBees Core v2</h2>

<p>As mentioned above, Pod Security Polices are an optional Kubernetes feature (and still beta) so they are not enabled by default on most Kubernetes distributions - to include GCP GKE, and Azure AKS. PSPs can be created and applied to a <code>ClusterRole</code> or a <code>Role</code> resource definition without enabling the PodSecurityPolicy admission controller. This is very important, because <strong>once you enable the PodSecurityPolicy admission controller any <code>pod</code> that does not have a PSP applied to it will not get scheduled</strong>.</p>

<blockquote>
<p>NOTE: PSPs are enabled by default on AWS EKS 1.13 and above, but with a very permissive PSP that is the same as running EKS without PSPs.</p>
</blockquote>

<p>We will define two PSPs for our Core v2 cluster:</p>

<ul>
<li>A very restrictive PSP used for all CloudBees components, additional Kubernetes services being leveraged with Core v2 and the <em>majority</em> of dynamic ephemeral Kubernetes based agents used by our Core v2 cluster:</li>
</ul>

<pre><code class="language-yaml">apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: cb-restricted
  annotations:
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'
    apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'
    seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'
    apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
spec:
  # prevents container from manipulating the network stack, accessing devices on the host and prevents ability to run DinD
  privileged: false
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  runAsUser:
    rule: 'MustRunAs'
    ranges:
      # Don't allow containers to run as ROOT
      - min: 1
        max: 65535
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  # Allow core volume types. But more specifically, don't allow mounting host volumes to include the Docker socket - '/var/run/docker.sock'
  volumes:
  - 'emptyDir'
  - 'secret'
  - 'downwardAPI'
  - 'configMap'
  # persistentVolumes are required for CJOC and Managed Master StatefulSets
  - 'persistentVolumeClaim'
  - 'projected'
  hostPID: false
  hostIPC: false
  hostNetwork: false
  # Ensures that no child process of a container can gain more privileges than its parent
  allowPrivilegeEscalation: false
</code></pre>

<p>Once the primary Core v2 PSP (<code>cb-restricted</code> in this case) has been created you must update the <code>Roles</code> to use it. CloudBees defines two Kubernetes <code>Roles</code> for the Core v2 install on Kubernetes, <code>cjoc-master-management</code> bound to the <code>cjoc</code> <code>ServiceAccount</code> for <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-reference-architecture/ra-for-gke/#_master_provisioning">provisioning Managed/Team Masters <code>StatefulSets</code> from CJOC</a>, and <code>cjoc-agents</code> bound to the <code>jenkins</code> <code>ServiceAccount</code> for scheduling dynamic ephemeral agent pods from Managed/Team Masters. The following Kubernetes configuration snippets show how this is configured:</p>

<pre><code class="language-yaml">---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: cjoc-master-management
rules:
- apiGroups: ['extensions']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - cb-restricted
...
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: cjoc-agents
rules:
- apiGroups: ['extensions']
  resources: ['podsecuritypolicies']
  verbs:     ['use']
  resourceNames:
  - cb-restricted
...
</code></pre>

<ul>
<li><p>The second PSP will be almost identical except for <code>RunAsUser</code> will be set to <code>RunAsAny</code> to allow running as <code>root</code> - this is specifically to run Kaniko containers (<a href="https://kurtmadel.com/posts/native-kubernetes-continuous-delivery/building-container-images-with-kubernetes/">read more about building containers as securely as possible with Kaniko</a>), but there may be some other uses cases that require containers to run as <code>root</code>:</p>

<pre><code>runAsUser:
rule: 'RunAsAny'
</code></pre></li>
</ul>

<h3 id="bind-restrictive-psp-role-for-ingress-nginx">Bind Restrictive PSP Role for Ingress Nginx</h3>

<p>CloudBees recommends the <a href="https://github.com/kubernetes/ingress-nginx">ingress-nginx</a> <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a> controller to manage external access to Core v2. The NGINX Ingress Controller is a top-level Kubernetes project and <a href="https://kubernetes.github.io/ingress-nginx/examples/psp/">provides an example</a> for using Pod Security Policies with the ingress-nginx <code>Deployment</code>. Basically, all you have to do is run the following command before installing the NGINX Ingress controller:</p>

<pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/docs/examples/psp/psp.yaml
</code></pre>

<p>The above command will create the following PSP, <code>Role</code> and <code>RoleBinding</code> with the primary differences from the <code>cb-restricted</code> PSP being the addition of the <code>NET_BIND_SERVICE</code> as an <code>allowedCapabilities</code> and allowing <code>hostPorts</code> of <strong>80</strong> to 65535:</p>

<pre><code class="language-yaml">apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  annotations:
    # Assumes apparmor available
    apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'
    apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default'
    seccomp.security.alpha.kubernetes.io/defaultProfileName:  'docker/default'
  name: ingress-nginx
spec:
  allowedCapabilities:
  - NET_BIND_SERVICE
  allowPrivilegeEscalation: true
  fsGroup:
    rule: 'MustRunAs'
    ranges:
    - min: 1
      max: 65535
  hostIPC: false
  hostNetwork: false
  hostPID: false
  hostPorts:
  - min: 80
    max: 65535
  privileged: false
  readOnlyRootFilesystem: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
    ranges:
    - min: 33
      max: 65535
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
    # Forbid adding the root group.
    - min: 1
      max: 65535
  volumes:
  - 'configMap'
  - 'downwardAPI'
  - 'emptyDir'
  - 'projected'
  - 'secret'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: ingress-nginx-psp
  namespace: ingress-nginx
rules:
- apiGroups:
  - policy
  resourceNames:
  - ingress-nginx
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: ingress-nginx-psp
  namespace: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: ingress-nginx-psp
subjects:
- kind: ServiceAccount
  name: default
- kind: ServiceAccount
  name: nginx-ingress-serviceaccount
</code></pre>

<blockquote>
<p>NOTE: You can also run that command after you have already installed the NGINX Ingress controller but the PSP will only be applied after restarting or recreating the ingress-nginx <code>Deployment</code>.</p>
</blockquote>

<h3 id="pod-security-policies-for-other-services">Pod Security Policies for Other Services</h3>

<p>The cluster used as an example for this post relies on the <a href="https://github.com/jetstack/cert-manager"><strong>cert-manager</strong></a> Kubernetes add-on for automatically provisioning and managing TLS certificates for the Core v2 install on GKE. If cert-manager or other services are installed before you enable PSPs on your cluster then the <code>pods</code> associated with them will not run if they are restarted if the associated <code>Roles</code>/<code>ClusterRoles</code> don&rsquo;t have PSPs applied to them. <strong>cert-manager</strong> is deployed to its own namespace so an easy way to ensure that all <code>ServiceAccounts</code> associated with the <strong>cert-manager</strong> service have a PSP applied is to create a <code>ClusterRole</code> with the PSP  and then bind that <code>ClusterRole</code> to all <code>ServiceAccounts</code> in the applicable <code>namespace</code>:</p>

<p><em><code>ClusterRole</code> with the cb-restricted PSP applied</em></p>

<pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: psp-restricted-clusterrole
rules:
- apiGroups:
  - extensions
  resources:
  - podsecuritypolicies
  resourceNames:
  - cb-restricted
  verbs:
  - use
</code></pre>

<p><em><code>RoleBindings</code> for cert-manager <code>ServiceAccounts</code></em></p>

<pre><code class="language-yaml">apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cert-manager-psp-restricted
  namespace: cert-manager
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: psp-restricted-clusterrole
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:serviceaccounts
</code></pre>

<blockquote>
<p>NOTE: You can use this command <code>kubectl get role,clusterrole --all-namespaces</code> to check your cluster for any other <code>Roles</code> or <code>ClusterRoles</code> that need to have a PSP applied to them. Remember, any <code>pod</code> that is running under a <code>ServiceAccount</code> that doesn&rsquo;t have a PSP will be shut down as soon as you enable the Pod Security Policy Admission Controller. For GKE you don&rsquo;t need to apply PSPs to any <code>Roles</code> in the <code>kube-system</code> <code>namespace</code> or any <strong>gce</strong> or <strong>system</strong> <code>ClusterRoles</code> as GKE will automatically apply the necessary PSPs.</p>
</blockquote>

<h2 id="enable-the-pod-security-policy-admission-controller">Enable the Pod Security Policy Admission Controller</h2>

<p>Now that PSPs are applied to all the necessary <code>Roles</code> and <code>ClusterRoles</code> you can enable the Pod Security Policy Admission Controller for your GKE cluster:</p>

<pre><code class="language-shell">gcloud beta container clusters update [CLUSTER_NAME] --zone [CLUSTER_ZONE] --enable-pod-security-policy
</code></pre>

<p>Next, you should ensure that all <code>pods</code> are still running:</p>

<pre><code class="language-shell">kubectl get pods --all-namespaces
</code></pre>

<p>If a <code>pod</code> that you expect to be running is not, you need to find the <code>Role</code>/<code>ClusterRole</code> that is used for the <code>pod</code>/<code>deployment</code>/<code>service</code> and apply a PSP to it.</p>

<p>Default Pod Security Policies created when enabling the <code>pod-security-policy</code> feature on a GKE cluster:</p>

<pre><code class="language-shell">NAME                           PRIV    CAPS   SELINUX    RUNASUSER   FSGROUP    SUPGROUP   READONLYROOTFS   VOLUMES
gce.event-exporter             false          RunAsAny   RunAsAny    RunAsAny   RunAsAny   false            hostPath,secret
gce.fluentd-gcp                false          RunAsAny   RunAsAny    RunAsAny   RunAsAny   false            configMap,hostPath,secret
gce.persistent-volume-binder   false          RunAsAny   RunAsAny    RunAsAny   RunAsAny   false            nfs,secret
gce.privileged                 true    *      RunAsAny   RunAsAny    RunAsAny   RunAsAny   false            *
gce.unprivileged-addon         false          RunAsAny   RunAsAny    RunAsAny   RunAsAny   false            emptyDir,configMap,secret
</code></pre>

<blockquote>
<p>NOTE: The default Pod Security Policies created automatically cannot be modified - Google will automatically change them back to those above.</p>
</blockquote>

<p><a href="https://docs.aws.amazon.com/eks/latest/userguide/pod-security-policy.html">AWS EKS</a> and <a href="https://docs.microsoft.com/en-us/azure/aks/use-pod-security-policies">Azure AKS - Preview</a> also support Pod Security Policies.</p>

<h2 id="oh-no-my-jenkins-kubernetes-agents-won-t-start">Oh no, My Jenkins Kubernetes Agents Won&rsquo;t Start!</h2>

<p>The <a href="https://github.com/jenkinsci/kubernetes-plugin">Jenkins Kubernetes plugin</a> (for ephemeral K8s agents) defaults to using a K8s <a href="https://kubernetes.io/docs/concepts/storage/volumes/#emptydir"><code>emptyDir</code> volume</a> type for the Jenkins agent workspace. This causes issues when using a restrictive PSP such at the <strong>cb-restricted</strong> PSP above. Kubernetes defaults to mounting <code>emptyDir</code> volumes as <code>root:root</code> with permissions set to <code>750</code> - as <a href="https://github.com/kubernetes/kubernetes/issues/2630">detailed by this GitHub issue</a> opened way back in 2014. When using a PSP, with Jenkins K8s agent pods, that doesn&rsquo;t allow containers to run as <code>root</code> the containers will not be able to access the default K8s plugin workspace directory. One approach for dealing with this is to set the K8s <code>securityContext</code> for <code>containers</code> in the <code>pod</code> spec. You can do this in the K8s plugin UI via the <strong>Raw yaml for the Pod</strong> field:</p>

<p><img src="/posts/cloudbees-core-on-kubernetes-best-practices/raw-yaml-for-the-pod.png" alt="Raw yaml for the Pod" /></p>

<p>This can also be set in the raw yaml of a <code>pod</code> spec that you <a href="https://github.com/cloudbees-days/pipeline-template-catalog/blob/master/templates/nodejs-app/Jenkinsfile#L2">load into your Jenkins job from a file</a>:</p>

<p><em><code>pod</code> spec with the <code>securityContext</code></em></p>

<pre><code class="language-yaml">kind: Pod
metadata:
  name: nodejs-app
spec:
  containers:
  - name: nodejs
    image: node:10.10.0-alpine
    command:
    - cat
    tty: true
  - name: testcafe
    image: gcr.io/technologists/testcafe:0.0.2
    command:
    - cat
    tty: true
  securityContext:
    runAsUser: 1000
</code></pre>
]]></content>
        </item>
        
        <item>
            <title>Securely Building Container Images on Kubernetes</title>
            <link>https://technologists.dev/posts/build-continaer-images/</link>
            <pubDate>Sat, 03 Aug 2019 10:05:15 -0400</pubDate>
            
            <guid>https://technologists.dev/posts/build-continaer-images/</guid>
            <description>Originally published on kurtmadel.com
Back in 2013, before Kubernetes was a thing, Docker was making Linux containers (LXC) much more accessible and use of Docker based containers took off (and Docker quickly dropped LXC as the default execution engine for their own container runtime). At the same time continuous integration (CI) was rapidly maturing as a best practice and a necessity for efficient software delivery. The use of Docker containers with CI was quickly adopted as the best way to manage CI tools - compilers, testing tools, security scans, etc.</description>
            <content type="html"><![CDATA[

<p><em>Originally published on <a href="https://kurtmadel.com/posts/native-kubernetes-continuous-delivery/building-container-images-with-kubernetes/">kurtmadel.com</a></em></p>

<p>Back in 2013, <a href="https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/">before Kubernetes was a thing</a>, Docker was making Linux containers (LXC) much more accessible and use of Docker based containers took off (and <a href="https://blog.docker.com/2014/03/docker-0-9-introducing-execution-drivers-and-libcontainer/">Docker quickly dropped LXC as the default execution engine for their own container runtime</a>). At the same time continuous integration (CI) was rapidly maturing as a best practice and a necessity for efficient software delivery. The use of Docker containers with CI was quickly adopted as the best way to manage CI tools - compilers, testing tools, security scans, etc. But it was new and there weren&rsquo;t a lot of best practices defined - it was more like &lsquo;go figure it out&rsquo;. And early on one very important aspect of using containers for CI/CD was using containers to build container images and pushing those images to container registries - but again, this was all very new, and a lot of people didn&rsquo;t really know what they were doing and there wasn&rsquo;t a <em>Building Container Images for Dummies</em>.</p>

<p>Fast forward a couple of years to September of 2015 when JÃ©rÃ´me Petazzoni published an article entitled <a href="https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">&ldquo;Using Docker-in-Docker for your CI or testing environment? Think twice.&rdquo;</a> The article basically describes how using <a href="https://github.com/jpetazzo/dind">Docker-in-Docker (DinD)</a> is bad choice for a CI/CD workload for a number of different reasons - it is definitely an article still worth a read. He promoted the concept of mounting the Docker socket of the host machine running the Docker daemon and using the host Docker daemon to execute Docker commands in your CI/CD jobs. Mounting the <code>/var/run/docker.sock</code> file as a volume in a Docker container allows you to accomplish similar functionality to DinD, but without some of the drawbacks of DinD -  to include layer caching and requiring running DinD containers with <a href="https://blog.docker.com/2013/09/docker-can-now-run-within-docker/"><code>--privileged</code> mode</a> enabled. In Part 5 of <a href="/series/native-kubernetes-continuous-delivery/">this series on Native Kubernetes Continuous Delivery</a> we will explore why it is no longer a best practice to use either of these two approaches for building and pushing container images as part of your Native Kubernetes Continuous Delivery pipelines. We will look at this from two different perspectives: security and performance. Finally, we will take a look at an alternative approach with security and performance in mind.</p>

<h2 id="container-images-vs-docker-images">Container Images vs Docker Images</h2>

<p>Before we dive into securely building and pushing container images on Kubernetes I wanted to share some thoughts on container terminology. I typically refer to an image that you run as a container in a Kubernetes Pod as a <strong>Container Image</strong> instead of a <strong>Docker Image</strong>. Back in 2015, Docker was kind enough to <a href="https://blog.docker.com/2017/07/oci-release-of-v1-0-runtime-and-image-format-specifications/">donate the Docker image format</a> to the then newly established <a href="https://www.opencontainers.org/">Open Container Initiative (OCI)</a> - in addition to the container <a href="https://github.com/opencontainers/image-spec/blob/master/spec.md">Image Specification</a>, the OCI also maintains an open <a href="https://github.com/opencontainers/runtime-spec/blob/master/spec.md">Runtime Specification</a> for container execution. That makes Docker <em>no-longer-required</em> for running containers and pulling container images - and, as you will see later in this post, even building and pushing container images.</p>

<h2 id="what-s-wrong-with-docker-in-docker-dind">What&rsquo;s Wrong with Docker-in-Docker (DinD)</h2>

<p>Again, to understand the drawbacks of using DinD to build and push container images I recommend that you read <a href="https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">JÃ©rÃ´me Petazzoni&rsquo;s article</a>. But here is a quick summary:</p>

<ul>
<li>Security - The DinD container must run with <code>--privileged</code> flag enabled, resulting in undesirable attack vectors against the underlying Docker host.</li>
<li>Performance - Layer caching is not shared across builds when using ephemeral DinD containers. All layers of all images must be pulled every time a new DinD container is used, resulting in much slower container image builds.</li>
</ul>

<h2 id="what-s-wrong-with-mounting-the-docker-socket">What&rsquo;s Wrong with Mounting the Docker Socket</h2>

<p>So we have established that using DinD for CI/CD, especially for building and pushing container images, is a bad idea. But for Kubernetes CD you should also think twice if you are mounting the Docker socket for CI/CD - to include building and pushing container images.</p>

<h3 id="performance">Performance</h3>

<p>Letâ€™s put security aside for a moment - it turns out that mounting the Docker socket has significant issues in a Kubernetes based CD environment. A Kubernetes cluster is made up of one or more worker nodes, and it is on these worker nodes where Kubernetes schedules and runs <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pods</a>. When you mount the Docker socket to a Pod you are mounting the <code>/var/run/docker.sock</code> file into every container that makes up your Pod. When those containers run Docker commands against that socket they are actually being executed directly by the Docker daemon running as <strong>root</strong> on the worker node where the Pod was scheduled. The Kubernetes scheduler has no way to track that these other containers are running - they arenâ€™t managed by Kubernetes, rather they are managed by the Docker daemon running on the node where the Pod gets scheduled. This may result in serious Kubernetes scheduling issues, especially on busy CD clusters. And one of the main reasons to use Kubernetes in the first place is because of its robust orchestration and scheduling capabilities of containers, so why would you want to circumvent that for your CI/CD?</p>

<p>Another performance issue is container image layer caching. If you depend on the built-in caching provided by the Docker daemon then you may end up on different K8s nodes for different builds of the same container image or other container images that share layers - thus negating the caching provided by the Docker daemon on a specific K8s worker node.</p>

<p>Additionally, any layers that are stored on the host and any logs generated by the containers running via the Docker socket will not be automatically cleaned up for you.</p>

<h3 id="security">Security</h3>

<p>In simplistic terms, increased security goes hand in hand with reducing the attack surface or attack vectors. It is no different with CD on Kubernetes. If the Docker socket is exposed to a CD job then a curious/malicious developer can modify the job to run Docker commands as build steps, potentially becoming <code>root</code> on the node where that job lands. If they gain access to the underlying host as <code>root</code>, there are reasonably straightforward methods to escalate privileges and gain access to the entire Kubernetes cluster. Many cluster operators don&rsquo;t have the proper monitoring in place to properly detect this kind of activity and separate it from legitimate CD job runs, so the exploitation is likely to go unnoticed for a while.</p>

<p><strong>DinD</strong> has always <a href="https://blog.docker.com/2013/09/docker-can-now-run-within-docker/">required that the <code>--privileged</code> flag be enabled for the container running DinD</a> - and this has always been considered insecure. But mounting the Docker socket has never been any more secure, and has relied on the use of dedicated Docker daemon instances to isolate CI/CD workloads from other container workloads - like production applications for example.</p>

<p>While this is high on the <strong>bad</strong> scale for a single-purpose cluster just doing CI/CD, it is extremely high on the <strong>bad</strong> scale for a K8s cluster with multiple workloads that should have isolation - the isolation you should expect from containers running on K8s. For example, your <strong>Production</strong> containers may be just a namespace away from the namespace where your CD job is running.</p>

<p>If you go down this path, the net result is <em>anyone who can modify a CD job has a way to become root for the entire cluster</em>.</p>

<h2 id="block-the-use-of-dind-and-the-docker-socket-for-k8s-cd-pods">Block the Use of DinD and the Docker Socket for K8s CD Pods</h2>

<p>For some, the drawbacks of mounting the Docker socket for CD on Kubernetes are significant enough that they <a href="https://applatix.com/case-docker-docker-kubernetes-part-2/">recommended going back to DinD</a>. But we have already dismissed both <strong>DinD</strong> and mounting the Docker socket as acceptable approaches for CD on Kubernetes. Before we look at an alternative we will explore K8s features that allow you to block the use of DinD and the mounting of the Docker socket for all containers running on your K8s cluster (or part of your cluster).</p>

<p>By using Kubernetes for dynamic ephemeral CD executors you can run each of your CD steps directly in containers built specifically and singularly for the purpose of executing that step or step(s), all within a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod/">Pod</a> managed by Kubernetes. In addition to orchestrating the scheduling and running of these CD Pods - K8s also allows managing other aspects of these Pods&rsquo; lifecycles to include security sensitive aspects of the pod specification that enable fine-grained authorization of pod creation and updates. A native K8s solution for managing pod security is the <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podsecuritypolicy">Pod Security Policy Admission Controller</a>. A Pod Security Policy (PSP) may be configured in such a way that a container running in a Pod using a correctly configured policy won&rsquo;t be scheduled if it is configured to mount the Docker socket and won&rsquo;t be allowed to run as a <code>--privileged</code> container - disabling DinD.</p>

<h3 id="use-pod-security-policies-to-block-dind-and-mounting-the-docker-socket">Use Pod Security Policies to Block DinD and Mounting the Docker Socket</h3>

<p>The Pod Security Policy Admission Controller is a <a href="https://kubernetes.io/blog/2018/07/18/11-ways-not-to-get-hacked/#6-use-linux-security-features-and-podsecuritypolicies">critical feature for enhancing the security of your K8s cluster</a>. The Pod Security Policy Admission Controller allows you to specify Pod Security Policies that limit what containers are allowed to do - if a container in a Pod is configured to do something that is not allowed by the Pod Security Policy then K8s will not schedule the Pod.</p>

<blockquote>
<p><a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/#what-is-a-pod-security-policy">From the Kubernetes official docs</a>: <em>A Pod Security Policy is a cluster-level resource that controls security sensitive aspects of the pod specification. The PodSecurityPolicy objects define a set of conditions that a pod must run with in order to be accepted into the system, as well as defaults for the related fields</em></p>
</blockquote>

<p>Let&rsquo;s look at some specific settings that will greatly reduce the <em>attack</em> surface thus mitigating risk:</p>

<ul>
<li><code>privileged</code>: set to <code>false</code> will disallow the use of DinD.</li>
<li><code>runAsUser</code>: set this to <code>MustRunAsNonRoot</code> so containers can&rsquo;t run as the <code>ROOT</code> user.</li>
</ul>

<blockquote>
<p>NOTE: You will need to allow <code>USER root</code> to actually do anything meaningful with Kaniko to build and push container image, so you will most likely need to set <code>runAsUser</code> to <code>RunAsAny</code>. The goal with a Kaniko PSP is to reduce other available attack vectors.</p>
</blockquote>

<ul>
<li><code>allowPrivilegeEscalation</code>: disable privilege escalation so that no child process of a container can gain more privileges than its parent.</li>
<li><code>volumes</code>: Don&rsquo;t allow mounting host directories/files as volumes by specifying <a href="https://kubernetes.io/docs/concepts/storage/volumes/">specific volume types</a> and not allowing the <code>hostPath</code> volume for any CD containers. This will disable the ability to mount the Docker socket.</li>
<li>PSP <code>annotations</code>: confine all Pod containers to the <code>runtime/default</code> <strong>seccomp</strong> profile via the <code>seccomp.security.alpha.kubernetes.io/defaultProfileName</code> annotation and don&rsquo;t set the <code>seccomp.security.alpha.kubernetes.io/allowedProfileNames</code> so the default cannot be changed.</li>
</ul>

<p>Here is an example of a restrictive <strong>PSP</strong> that won&rsquo;t allow <strong>mounting the Docker socket</strong> and won&rsquo;t allow <strong>DinD</strong>:</p>

<pre><code class="language-yaml">apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: cd-restricted
  annotations:
    seccomp.security.alpha.kubernetes.io/defaultProfileName:  'runtime/default'
    apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'
spec:
  privileged: false
  # Required to prevent escalations to root.
  allowPrivilegeEscalation: false
  # This is redundant with non-root + disallow privilege escalation,
  # but we can provide it for defense in depth.
  requiredDropCapabilities:
    - ALL
  # Allow core volume types. But more specifically, don't allow mounting host volumes to include the Docker socket - '/var/run/docker.sock'
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    # Assume that persistentVolumes set up by the cluster admin are safe to use.
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    # Don't allow containers to run as ROOT
    rule: 'MustRunAsNonRoot'
  seLinux:
    # This policy assumes the nodes are using AppArmor rather than SELinux.
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      # Forbid adding the root group.
      - min: 1
        max: 65535
  readOnlyRootFilesystem: false
</code></pre>

<blockquote>
<p><strong>NOTE</strong>: At the time this post was published <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/pod-security-policies">GKE has beta support for Pod Security Polices</a>, <a href="https://docs.microsoft.com/en-us/azure/aks/use-pod-security-policies">AKS introduced preview support for Pod Security Policies in April</a> and <a href="https://docs.aws.amazon.com/eks/latest/userguide/pod-security-policy.html">EKS added Pod Security Polices as a default feature with their support for K8s 1.13 (also new 1.12 EKS clusters will include support for Pod Security Policies)</a>.</p>
</blockquote>

<h3 id="don-t-run-docker">Don&rsquo;t Run Docker</h3>

<p>Another compelling way to not allow DinD or mounting the Docker Socket is to not use Docker as the container runtime for your K8s cluster.
<a href="https://containerd.io/"><strong>containerd</strong></a> is an implementation of the OCI image runtime mentioned above  (<a href="https://blog.docker.com/2017/03/docker-donates-containerd-to-cncf/">also donated by Docker to the CNCF</a>) and is (at the time of this post) <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/using-containerd">supported by the Google Kubernetes Engine (GKE)</a> - <em>but no other major cloud providers</em>. By using the OCI run spec provided by <strong>containerd</strong>, you don&rsquo;t actually need Docker - and you will actually see better performance for your K8s containers with <strong>containerd</strong> because Docker actually uses <strong>containerd</strong> under the covers - so to speak - resulting in an extra daemon and unnecessary communication overhead. One interesting aspect of <strong>containerd</strong> is that it is only a runtime for containers - it does not support building container images. But a big plus of <strong>containerd</strong>, besides better performance, is that it makes it impossible to use <strong>DinD</strong> or mount the Docker socket - thus providing a more secure container runtime. But how do you build container images without Docker?</p>

<h2 id="building-container-images-without-docker">Building Container Images without Docker</h2>

<p>Or more specifically, building container images without the Docker <strong>daemon</strong> - no <strong>DinD</strong>, no Docker socket. But we still want to leverage K8s managed containers for CD, to include building and pushing container images. How do we do that?</p>

<h3 id="enter-kaniko">Enter Kaniko</h3>

<p>Kaniko is a tool that is capable of building and pushing container images without the Docker daemon, but it does have one major drawback from a security perspective: to build anything useful/easily you must run as the <code>root</code> <code>USER</code> in the Kaniko container.</p>

<blockquote>
<p><a href="https://github.com/GoogleContainerTools/kaniko#security"><strong>From Kaniko&rsquo;s docs</strong></a>: If you have a minimal base image (SCRATCH or similar) that doesn&rsquo;t require permissions to unpack, and your Dockerfile doesn&rsquo;t execute any commands as the root user, you can run Kaniko without root permissions. It should be noted that Docker runs as root by default, so you still require (in a sense) privileges to use Kaniko.</p>

<p>You may be able to achieve the same default seccomp profile that Docker uses in your Pod by setting <code>seccomp</code> profiles with annotations on a PodSecurityPolicy to create or update security policies on your cluster.</p>
</blockquote>

<p>As we already mentioned above, running as <code>root</code> is an attack vector that many consider to be an unacceptable security hole - but the use of Pod Security Policies will reduce the attack surface of the Kaniko container running as part of a K8s Pod and provides greater security than the Docker based approaches we have already dismissed.</p>

<h4 id="basic-configuration-for-kaniko-with-pod-security-policies">Basic Configuration for Kaniko with Pod Security Policies</h4>

<p>The following configuration assumes that you have a K8 cluster up and running, and that you have access to <code>kubectl</code> to add and modify K8s resources.</p>

<ol>
<li><strong>Container registry</strong>: I recommend having a staging or sandbox container registry that Kaniko pushes images to, and a production registry that Kaniko does not have access to push to. A separately managed and secured CD job should be used to promote container images from staging to production once required tests/scans/policies have been successfully run against that container image. But of course the Kaniko container image itself (and any other container images used by CD jobs) should always be pulled from the production container registry.</li>
<li><code>PodSecurityPolicy</code>: Once the PodSecurityPolicy admission controller is enabled you will need at least 2 PodSecurityPolicies (actually you will want to defined and apply all of your Pod Security Policies before enabling the admission controller, not doing so will prevent any pods from being created in the cluster):

<ol>
<li>The most restrictive policy possible for executing a build and push with Kaniko - the only change to the PSP above is to change <code>runAsUser</code> to <code>RunAsAny</code> to allow using the ROOT user in Dockerfiles to be built by Kaniko.</li>
<li>A <a href="https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/policy/privileged-psp.yaml">privileged policy</a> that is equivalent to not using the Pod Security admission controller for Pods that use it.</li>
</ol></li>
<li><code>Namespace</code>, <code>Service Account</code>, <code>Role</code>, <code>RoleBinding</code>: a PodSecurityPolicy is applied to a K8s <code>Role</code> that is then bound to a <code>ServiceAccount</code> via a <code>RoleBinding</code>. I recommend creating a <code>ServiceAccount</code> bound to a <code>Role</code> with a restrictive PodSecurityPolicy specifically for Kaniko/CD jobs.</li>
</ol>

<h4 id="kaniko-with-gcp-gke-and-gcr">Kaniko with GCP, GKE and GCR</h4>

<p>Kaniko is a Google sponsored project, so naturally there is good support for using Kaniko with a GKE cluster. I <a href="https://github.com/GoogleContainerTools/kaniko#running-kaniko-in-a-kubernetes-cluster">defer to the Kaniko instructions for GKE + GCR</a>.</p>

<p><a href="https://cloud.google.com/kubernetes-engine/docs/how-to/pod-security-policies">Using Pod Security Policies with GKE.</a></p>

<h4 id="kaniko-with-aws-eks-and-ecr">Kaniko with AWS, EKS and ECR</h4>

<p>The Kaniko instructions tell you to create a Kubernetes secret with your <code>~/.aws/credentials</code> to push container images to the ECR, but most organizations don&rsquo;t allow you to use AWS credentials this way. Here are <a href="https://github.com/GoogleContainerTools/kaniko#pushing-to-amazon-ecr">Kaniko&rsquo;s instructions for pushing to AWS ECR</a>.</p>

<p>Another approach for securely using Kaniko with EKS and ECR is to use worker node IAM roles with an instance group dedicated to Kaniko/CD.</p>

<p><a href="https://aws.amazon.com/blogs/opensource/using-pod-security-policies-amazon-eks-clusters/">Using Pod Security Policies with EKS.</a></p>

<h4 id="kaniko-with-azure-aks-and-acr">Kaniko with Azure, AKS and ACR</h4>

<p>At the time of this post, Kaniko does not have official support for the Azure Container Registry (ACR) - but that doesn&rsquo;t mean it isn&rsquo;t possible. There is an <a href="https://github.com/GoogleContainerTools/kaniko/issues/425">open issue</a> in the Kaniko GitHub repository that includes some tips on pushing container images to the ACR from Kaniko.</p>

<p><a href="https://docs.microsoft.com/en-us/azure/aks/use-pod-security-policies">Using Pod Security Policies with AKS.</a></p>

<h4 id="kaniko-the-easy-way">Kaniko the Easy Way</h4>

<p>Jenkins X allows you to <a href="https://jenkins-x.io/getting-started/create-cluster/#the-jx-create-cluster-gke-process">enable Kaniko as the default way to build and push container images</a> for all of your Jenkins X CD jobs and will be automatically configured to push to the default container registry of the cloud where you install Jenkins X and Kaniko caching is automatically set up for you - resulting in fast, secure container image builds that are pushed to your default Jenkins X container registry.</p>

<p><strong>Important:</strong> Jenkins X does not have OOTB support for Pod Security Policies as tracked by <a href="https://github.com/jenkins-x/jx/issues/1074">this GitHub issue</a>. In my next post we will take a look at using Pod Security Policies with Jenkins X - but not just for Kaniko, because once you enable Pod Security Policy every K8s <code>Role</code>/<code>ClusterRole</code> has to have a Pod Security Policy associated to it.</p>

<h4 id="drawbacks-for-kaniko">Drawbacks for Kaniko</h4>

<ul>
<li>Requires running the Kaniko container as <code>ROOT</code> to execute most container builds</li>
<li>Doesn&rsquo;t work with all <code>Dockerfiles</code> but keeps improving</li>
<li>Is slightly more complicated to setup than the good old <code>docker build</code></li>
</ul>

<h2 id="other-security-vectors-you-should-cover">Other Security Vectors You Should Cover</h2>

<p>Enforce use of specific container registries - for example donâ€™t allow pulling from or pushing to a public container registry like DockerHub. You should maintain your own container images and container registries - to include having at least two different container registries - one for CD container images (along with intermediate application container images) and another container registry for production approved application container images. I would actually take this a step further and have a third container registry that is a sandbox of sorts that allows CD specific container images and intermediate application container images that haven&rsquo;t been approved for more secure environments - like production - but are allowed to run in less secure test environments.</p>

<p>Scan your container images before you make them available for use. Here is <a href="https://cb-technologists.github.io/posts/cloudbees-cross-team-and-dev-sec-ops/">a blog post of using Anchore with a Jenkins Pipeline</a> - super simple to set-up so there really is no reason not to scan your container images and use the scan as a gate for promotion to more secure container registries.</p>

<h2 id="other-solutions">Other Solutions</h2>

<p>Other solutions that allow you to build and push container images using a Kubernetes Pod based container and that don&rsquo;t rely on the Docker daemon:</p>

<ul>
<li><a href="https://github.com/genuinetools/img">img</a>: <strong>img</strong> is only at version 0.5.7 (May 3, 2019 release), but this project is promising. <strong>img</strong> required <a href="https://github.com/genuinetools/img#upstream-patches">upstream patches</a> to different projects that should eventually make it the most secure (and fast) way to build and push container images from a K8s Pod container.</li>
<li><a href="https://github.com/GoogleContainerTools/jib">jib</a> - Another Google project, <strong>jib</strong> supports building and pushing container images without the Docker daemon.

<ul>
<li><strong>jib</strong> only supports building container images for Java applications with Maven and Gradle support.</li>
</ul></li>
</ul>

<p>If you aren&rsquo;t already, start building your container images with Kaniko on Kubernetes - with Pod Security Policies. And again, if you want to get Kaniko up and running quickly and easily then you should checkout Jenkins X. Jenkins X will automatically set up Kaniko, along with the necessary configuration to push container images to the Docker registry of your choice (GCR, ECR, Docker Hub, etc). You can go from a Dockerfile in GitHub to container image in your registry in minutes with Jenkins X.</p>
]]></content>
        </item>
        
        <item>
            <title>Safety First with Jenkins and Snyk</title>
            <link>https://technologists.dev/posts/jenkins-snyk/</link>
            <pubDate>Wed, 31 Jul 2019 07:00:00 +0000</pubDate>
            
            <guid>https://technologists.dev/posts/jenkins-snyk/</guid>
            <description>Implementing security scanning as a preventative measure in your CI pipeline. Overview Todayâ€™s developers are being empowered to expeditiously innovate, creating new software capabilities and building continuous customer value. At CloudBees, we commonly tell our customers:
 &amp;ldquo;Every business is a software business and is under pressure to innovate constantly. This increased velocity introduces new business risks. CloudBees is building the world&amp;rsquo;s first end-to-end automated software delivery system (SDM), enabling companies to balance governance and developer freedom.</description>
            <content type="html"><![CDATA[

<h2 id="implementing-security-scanning-as-a-preventative-measure-in-your-ci-pipeline">Implementing security scanning as a preventative measure in your CI pipeline.</h2>

<p><img src="/img/jenkins-snyk/jenkins-snyk.png" alt="Jenkins + Snyk = Love Security" /></p>

<h3 id="overview">Overview</h3>

<p>Todayâ€™s developers are being empowered to expeditiously innovate, creating new software capabilities and building continuous customer value. At CloudBees, we commonly tell our customers:</p>

<blockquote>
<p>&ldquo;Every business is a software business and is under pressure to innovate constantly. This increased velocity introduces new business risks. CloudBees is building the world&rsquo;s first end-to-end automated software delivery system (SDM), enabling companies to balance governance and developer freedom.&rdquo;</p>
</blockquote>

<p>Many of the products getting built today contain or rely on sensitive consumer data and tend to have wide-spread privacy implications should the consumer&rsquo;s data get leaked. Unfortunately, in today&rsquo;s world, we&rsquo;re seeing more and more companies fall victim to security vulnerabilities, and the exposure of sensitive data is also happening with a semi-regular cadence.</p>

<p>The Equifax Breach, which resulted in an up to $700 million settlement last week, could have likely been avoidable had a mechanism existed that made mitigating security issues reliable and straightforward for developers. In this case, a lapse in process and legacy code get attributed with the blame.</p>

<p>Recent supply chain and <a href="https://www.techrepublic.com/article/malicious-libraries-in-package-repositories-reveal-a-fundamental-security-flaw/">typo squatting</a> attacks against popular open-source libraries and repositories are also becoming a more significant attack vector. A simple typo or desire to stay current with the latest stable release can have a significant impact on business security and customer privacy.</p>

<h3 id="enter-snyk">Enter Snyk</h3>

<p><img src="/img/jenkins-snyk/snyk-logo.png" alt="snyk" /></p>

<p>For those of us that are not security experts, the ability to arm oneself with relevant, actionable data can be the difference between an all hands on deck security catastrophe and a bland Tuesday at the office. In 2018 an average of 45 new CVE entries were created daily for a total of 16555 entries. That&rsquo;s quite a bit of information to sift through, especially when you&rsquo;re a developer. Snyk provides multiple mechanisms that not only inform developers but can also provide valuable feedback before a vulnerability gets committed to a code repository.</p>

<p>Snyk is a SaaS offering that manages it&rsquo;s own vulnerability database and provides continuous monitoring of an application&rsquo;s dependencies and their respective docker containers. Snyk can also provide additional security metrics coupled with historical project reporting and has features for managing OSS license policies and compliance reporting as a paid feature. NOTE: They also offer on-prem licensing for enterprises.</p>

<p>The Snyk offering focuses on 5 specific actions when encountering a vulnerability. These actions include monitoring, prevention, finding, fixing, and alerting.</p>

<p>I chose to explore Snyk because it offers a free, open-source tier available to the public and also because it provides different ways of interacting with the service depending on your use case. Snyk has support for quite a few languages (Java, Ruby, Node, Python, Scala, Golang, .NET, and PHP) and integrates with other popular services like Github, Docker Hub, Slack, Jira and of course Jenkins.</p>

<h3 id="jenkins-and-cloudbees-ci-cd">Jenkins and CloudBees CI/CD</h3>

<p><img src="/img/jenkins-snyk/cloudbees-logo.png" alt="cloudbees" /></p>

<p>Jenkins is fairly ubiquitous when it comes to continuous integration and continuous delivery. Itâ€™s easily one of the most battle-tested, configurable, and programmable automation tools being used by developers and enterprises alike.</p>

<p>For those not as familiar, Welcome, Jenkins is an open-source project that offers smaller teams and individuals the ability to create CI/CD pipelines plus more. The CloudBees offering focuses on economies of scale, security, administration, and support for medium-sized businesses to large scale enterprises. Jenkins is a component of the CloudBees offering. As good stewards do, CloudBees donates about 80% of the code it writes back to the Jenkins open-source project.</p>

<p>Many of the customers I speak to have a desire for developer self-service capabilities when it comes to CI/CD. While enterprises want to enforce specific policies, especially around security, they also want to promote developer autonomy and creativity.</p>

<p>Security teams typically want to draw a line in the sand when it comes to critical and high severity security issues. Who could blame them? Using Jenkins and Snyk together, vulnerability scanning becomes a first-class citizen of the CI/CD pipeline. Developers and Security teams can invoke failure for a build based on a predefined policy, preventing security incidents early in the development cycle. Developers are also empowered with data and the <code>snyk wizard</code> capability to fix reported issues.</p>

<p>On the CloudBees side, we take this one step further. Using custom marker files, we allow CloudBees administrators to control immutable events that happen in the pre and post-build phase of the pipeline outside the scope of a Jenkinsfile. Custom marker files allow security operations to impose vulnerability scanning at the top-level versus a team or developer having to add it to a pipeline for each project. When compliance and auditing are essential to the customer, the custom marker file plays a pivotal role. If you&rsquo;re interested in learning more about custom marker files, you can read about them <a href="https://go.cloudbees.com/docs/plugins/workflow/#pipeline-custom-factories">here</a>.</p>

<h3 id="getting-started">Getting Started</h3>

<ul>
<li>Sign up for a free account at <a href="https://app.snyk.io/signup">https://app.snyk.io/signup</a>

<ul>
<li>Take note of your Snyk API token here: <a href="https://app.snyk.io/account">https://app.snyk.io/account</a></li>
</ul></li>
<li>You must be running an instance of Jenkins or CloudBees Core

<ul>
<li>You must have the ability to configure Jenkins and install plugins if you want to use Snykâ€™s Jenkins plugin.</li>
<li>You must have a docker daemon present on the agent including your language runtime if youâ€™re using the plugin</li>
</ul></li>
<li>You must have commit access to a git repository. (We use Github)</li>
<li>Prior Jenkins knowledge will be required. You should also be familiar with installing dependencies for your project and language.</li>
</ul>

<p>Snyk provides a Jenkins plugin for developers looking to get started quickly. This comes in the form of a freestyle build step, and a pipeline function for developers using Jenkinsfile.</p>

<p>Snyk also provides a container (snyk-cli) that provides users the CLI interface for container based builds. Containers are tagged for corresponding languages and runtimes (e.g. <code>snyk/snyk-cli:python-3</code>).</p>

<h4 id="1-install-the-snyk-plugin-optional">1. <strong>Install the Snyk plugin, optional</strong></h4>

<p>The first part of this tutorial highlights the functionality of the Snyk plugin. The second part will focus on using the snyk-cli with container technology (Docker, Kubernetes) and does not require a plugin install.</p>

<p><a href="https://plugins.jenkins.io/snyk-security-scanner">Snyk Jenkins Plugin Page</a></p>

<p><img src="/img/jenkins-snyk/install-plugin.png" alt="plugin install" /></p>

<p>If youâ€™re using a CloudBees plugin catalog you can update your catalog schema with the following plugin details:</p>

<pre><code class="language-JSON">&quot;includePlugins&quot; : {
  &quot;snyk-security-scanner&quot;: {
    &quot;version&quot; : &quot;2.10.0&quot;
  }
}
</code></pre>

<p>The plugin has one additional requirement. You must configure Snyk with a global configuration (Manage Jenkins -&gt; Global Tool Configuration)</p>

<p><img src="/img/jenkins-snyk/global-tool.png" alt="Global Tool Configuration" /></p>

<h4 id="2-add-your-snyk-token-to-jenkins">2. <strong>Add your Snyk token to Jenkins</strong></h4>

<p>Get your Snyk token from <a href="https://app.snyk.io/account">https://app.snyk.io/account</a></p>

<p>Add a global credential in Jenkins. Under the kind option make sure you choose Snyk API Token. (Credentials -&gt; Global -&gt; Add Credentials)</p>

<blockquote>
<p>If you do not intend on using the Snyk plugin you&rsquo;ll still be required to add a credential. Use the secret text credential type instead and add the Snyk token as the secret.</p>
</blockquote>

<h4 id="3-freestyle-jobs-with-plugin">3. <strong>Freestyle Jobs (with plugin)</strong></h4>

<p>Snyk requires two steps in order to scan successfully. Step one, installing dependencies. To do this use an Execute Shell step on a new or existing freestyle job.</p>

<p><img src="/img/jenkins-snyk/build-step-shell.png" alt="build step shell" /></p>

<p>In the command box youâ€™ll need to provide the command to install your dependencies. Weâ€™re using python and the Snyk supported requirements.txt file to pip install.</p>

<pre><code>$ pip install -r requirements.txt
</code></pre>

<p>Now we can add a second build step, Invoke Snyk Security task</p>

<p><img src="/img/jenkins-snyk/build-step-snyk.png" alt="build step snyk" /></p>

<p>After choosing Synk Security Task, several options will need to be configured.</p>

<p><img src="/img/jenkins-snyk/configure-build-step-snyk.png" alt="configure snyk" /></p>

<p><strong>When issues are found</strong></p>

<ul>
<li>Fail on Build if severity is high, medium or low or continue regardless of vulnerability severity (Weâ€™ve chosen high).</li>
</ul>

<p><strong>Monitor project on build</strong></p>

<ul>
<li>Continue monitoring project will scan the repo once a day and provide notifications outside of Jenkins.</li>
</ul>

<blockquote>
<p>This is a good feature if youâ€™re not always building your project or code that isnâ€™t changed often. Developers can continue to get access to vulnerability notifications regardless of mean lead time or deploy frequency.</p>
</blockquote>

<p><strong>Snyk details</strong></p>

<ul>
<li>Snyk API token</li>
<li>Target file (Weâ€™re using a requirements.txt file and assuming python)</li>
<li>Organization</li>
<li>Name of the project youâ€™re scanning (Weâ€™re using git repo name)</li>
</ul>

<p><strong>Advanced</strong></p>

<ul>
<li>Snyk Install (Added in the global configuration in Step 1)</li>
<li>Snyk CLI arguments

<ul>
<li><a href="https://snyk.io/blog/snyk-cli-cheat-sheet/">Snyk CLI cheat sheet</a>

<br /></li>
</ul></li>
</ul>

<p>Once the project is configured itâ€™s time to build. Trigger or start a build</p>

<p><strong>PASS:</strong></p>

<pre><code>Testing for known issues...
&gt; /home/jenkins/tools/io.snyk.jenkins.tools.SnykInstallation/snyk-latest/snyk-alpine test --json --severity-threshold=high --file=requirements.txt --org=cloudbees --project-name=project-python
Result: 0 known issues | No high severity vulnerabilities
</code></pre>

<p><strong>FAIL:</strong></p>

<pre><code>Testing for known issues...
&gt; /home/jenkins/tools/io.snyk.jenkins.tools.SnykInstallation/snyk-latest/snyk-alpine test --json --severity-threshold=high --file=requirements.txt --org=cloudbees --project-name=project-python
Result: 2 known issues | 2 high severity vulnerable dependency paths
</code></pre>

<p>Snyk will also provide a link to the security report for each build run:</p>

<p><img src="/img/jenkins-snyk/build-report.png" alt="snyk build report" /></p>

<p>Looking forward, you can add multiple build steps to freestyle jobs if you want to for example scan your Dockerfile and your Python dependencies in a single job.</p>

<p>At CloudBees we typically recommend users start with pipeline jobs if they have the opportunity. While freestyle jobs work well, pipelines jobs can provide several distinct advantages (e.g. parallel steps, shared libraries).</p>

<h4 id="4-pipeline-jobs-with-plugin">4. <strong>Pipeline Jobs (with plugin)</strong></h4>

<p>The Snyk plugin provides a pipeline function for scanning. Per the official documentation:</p>

<p>The snykSecurity function accepts the following parameters:</p>

<ul>
<li><strong>snykInstallation</strong> - Snyk installation configured in the Global Tool Configuration.</li>
<li><strong>snykTokenId</strong> - The ID for the API token from the Credentials plugin to be used to authenticate to Snyk.</li>
<li><strong>additionalArguments</strong> (optional, default none) - Refer to the Snyk CLI help page for information on additional arguments.</li>
<li><strong>failOnIssues</strong> (optional, default true) - This specifies if builds should be failed or continued based on issues found by Snyk.</li>
<li><strong>organisation</strong> (optional, default none) - The Snyk organisation in which this project should be tested and monitored.</li>
<li><strong>projectName</strong> (optional, default none) - A custom name for the Snyk project created for this Jenkins project on every build.</li>
<li><strong>severity</strong> (optional, default low)- Only report vulnerabilities of provided level or higher (low/medium/high).</li>
<li><strong>targetFile</strong> (optional, default none) - The path to the manifest file to be used by Snyk.</li>
</ul>

<p>An example Jenkinsfile using the Snyk Jenkins plugin:</p>

<pre><code class="language-groovy">pipeline {
  agent any
  stages {
    stage('snyk dependency scan') {
      tools {
        snyk 'snyk-latest'
      }	
      steps {
        snykSecurity(
          organisation: 'cloudbees',
          severity: 'high',
          snykInstallation: 'snyk-latest',
          snykTokenId: 'snyk',
          targetFile: 'requirements.txt',
          failOnIssues: 'true'
        )		
      }
    }
  }
}
</code></pre>

<h4 id="4-pipeline-jobs-without-plugins">4. <strong>Pipeline Jobs (without plugins)</strong></h4>

<blockquote>
<p>To get started without a plugin you need to have a Snyk token stored as a secret text credential outlined in <strong>Step 2</strong>.</p>
</blockquote>

<p>Using Snyk in your pipeline is also possible without the using Snyk plugin for Jenkins. In fact, I would say depending on the user or company, this is typically what a CloudBees Solutions Architect or Professional Services Consultant would recommend when setting up your Pipeline, Multi-Branch or Github Organization job. If you&rsquo;re curious or asked why, here are a couple of reasons:</p>

<ol>
<li>Not every user has admin privileges. A plugin install and global tool configuration require escalated privileges. This may not be an issue for some, in enterprise settings this isn&rsquo;t always a simple hurdle to overcome.</li>
<li>Closer to what a developer would do if they wanted to run Snyk manually or locally</li>
<li>Configuration as Code and GitOps. The benefits are numerous. Empowered developers, and reduced Jenkins administration overhead are just two.<br /></li>
</ol>

<p>Using the snyk-cli tool, developers can craft steps using the command line interface. For Jenkins and CloudBees users, this means you&rsquo;ll need to invoke <code>sh</code> within your stage and then pass the CLI parameters required to satisfy a successful snyk.</p>

<blockquote>
<p>If you want to install the snyk-cli locally and have a working NodeJS environment you can run the following command:</p>
</blockquote>

<pre><code class="language-bash">$ npm install -g snyk
</code></pre>

<p><strong>DOCKER:</strong></p>

<pre><code class="language-groovy">pipeline {
  agent none
  stages {
    stage('snyk dependency scan') {
      agent {
        docker {
          image 'snyk/snyk-cli:python-3'
        }
      }
      environment {
        SNYK_TOKEN = credentials('snyk-token')
      }	
      steps {
        sh &quot;&quot;&quot;
          pip install -r requirements.txt
          snyk auth ${SNYK_TOKEN}
          snyk test --json \
            --severity-threshold=high \
            --file=requirements.txt \
            --org=cloudbees \
            --project-name=project-python
        &quot;&quot;&quot;		
      }
    }
  }
}
</code></pre>

<p><strong>KUBERNETES</strong></p>

<p>In the example below we&rsquo;re using some of the additional docker functionality provided by Snyk. We&rsquo;ve also added the required docker integration to our pod.</p>

<blockquote>
<p>Docker in docker is <a href="http://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/">probably</a> not the way you want to build your production containers, scanning however should not be an issue. Projects such as <a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a> provide a path for developers that want to use Kubernetes for building docker containers with Dockerfile. See Matt Elgin&rsquo;s <a href="https://cb-technologists.github.io/posts/cjd-casc/">post</a> for some good examples on using Kaniko.</p>
</blockquote>

<p>The pipeline is scanning both the container and the python dependencies in parallel with the <code>failFast</code> option set to <code>true</code>. This enables the build to run quickly, if failure is detected in either stage, the build is short circuited and both stages are stopped.</p>

<pre><code class="language-groovy">pipeline {
  agent {
    kubernetes {
      label 'python-dev'
      yaml &quot;&quot;&quot;
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: snyk-python
    image: snyk/snyk-cli:python-3
    command:
    - /bin/cat
    tty: true
  - name: snyk-docker
    image: snyk/snyk-cli:docker
    command:
    - /bin/cat
    tty: tru
    env:
    - name: DOCKER_HOST
      value: tcp://localhost:2375
  - name: dind
    image: docker:18.09-dind
    securityContext:
      privileged: true
    volumeMounts:
      - name: dind-storage
        mountPath: /var/lib/docker
  volumes:
  - name: dind-storage
    emptyDir: {}
&quot;&quot;&quot;
    }
  }
  stages {
    stage('Snyk Scan') {
      failFast true
      environment {
        SNYK_TOKEN = credentials('snyk-token')
      }	
      parallel {
        stage('dependency scan') {
          steps {
            container('snyk-python') {
              sh &quot;&quot;&quot;
                pip install -r requirements.txt
                snyk auth ${SNYK_TOKEN}
                snyk test --json \
                  --file=requirements.txt \
                  --severity-threshold=high \
                  --org=cvega \
                  --project-name=project-python
              &quot;&quot;&quot;
            }
          }
        }
        stage('docker scan') {
          steps {
            container('snyk-docker') {
              sh &quot;&quot;&quot;
                docker build -t python-project .
                snyk auth ${SNYK_TOKEN}
                snyk test --json \
                  --docker python-project:latest \
                  --file=Dockerfile \
                  --severity-threshold=high \
                  --org=cvega \
                  --project-name=project-python
              &quot;&quot;&quot;
            }
          }
        }
      }
    }
  }
}
</code></pre>

<h4 id="5-snyk-reports-and-wizard">5.  <strong>Snyk Reports and Wizard</strong></h4>

<p>Like Jenkins and CloudBees, Snyk has quite a few valuable features. The Snyk portal provides data about  vulnerabilities in your project.</p>

<p><img src="/img/jenkins-snyk/snyk-web.png" alt="snyk portal" /></p>

<p>The CLI wizard feature of Snyk enables developers to take action and update dependencies. Using multi-branch jobs this makes testing updates to dependencies manageable.</p>

<p><img src="/img/jenkins-snyk/snyk-wizard.png" alt="snyk wizard" /></p>

<h4 id="6-summary">6. <strong>Summary</strong></h4>

<p>We&rsquo;ve presented several different ways to integrate Snyk and Jenkins using different types of technology and all of it fits together well. This is exactly the type of testing that could have prevented recent data breaches. This is also the type of development self-service that actually empowers developers and security teams to take the proper mitigation steps.</p>

<p><strong>Last but not Least</strong></p>

<p>Join CloudBees and Snyk at DevOps World.</p>

<p><a href="https://www.cloudbees.com/devops-world/"><img src="/img/jenkins-snyk/dwjw.png" alt="" title="DevOps World - Jenkins World" /></a></p>
]]></content>
        </item>
        
        <item>
            <title>Jenkins X Orchestration: More than Tekton on Steroids</title>
            <link>https://technologists.dev/posts/tekton-jx-pipelines/</link>
            <pubDate>Sun, 28 Jul 2019 23:44:10 +0100</pubDate>
            
            <guid>https://technologists.dev/posts/tekton-jx-pipelines/</guid>
            <description>We may know Jenkins X as a new pure CI/CD cloud native implementation different than Jenkins. It is based on the use of Kubernetes Custom Resource Definitions (CRD&amp;rsquo;s) to experience a seamless execution of CI/CD pipelines. This happens by leveraging the power of Kubernetes in terms of scalability, infrastructure abstraction and velocity.
The new main approach of Jenkins X is about a serverless experience, because there is no more traditional Jenkins engine running.</description>
            <content type="html"><![CDATA[

<p>We may know <a href="https://jenkins-x.io">Jenkins X</a> as a new pure CI/CD cloud native implementation different than <a href="https://jenkins.io">Jenkins</a>. It is based on the use of Kubernetes Custom Resource Definitions (CRD&rsquo;s) to experience a seamless execution of CI/CD pipelines. This happens by leveraging the power of Kubernetes in terms of scalability, infrastructure abstraction and velocity.</p>

<p>The new main approach of Jenkins X is about a serverless experience, because there is no more traditional Jenkins engine running. So, it relies on a CI/CD pipeline engine that can run on any standard Kubernetes deployment. This engine is the <a href="https://github.com/tektoncd/pipeline">Tekton CD project </a>, a former Google project that is also - like Jenkins X - part of the <a href="https://cd.foundation/">Continuous Delivery Foundation</a>.</p>

<p>In this post, we are showing the power of Tekton as a decoupled CI/CD pipeline engine execution. But more important, why an orchestration pipeline platform is practically required to design, configure and run your pipelines for the whole Software Delivery process. This orchestration platform is Jenkins X.</p>

<h2 id="the-tekton-base">The Tekton base</h2>

<p>Why then is Tekton a cool CI/CD engine?</p>

<p>First of all, Tekton is built on and for Kubernetes. This means that containers are the building blocks of any pipeline definition and execution. Kubernetes orchestrates the container&rsquo;s magic. One step, one container. But it&rsquo;s more than that:</p>

<ul>
<li>Everything is decoupled. So for example, a group of steps, or any pipeline resource can be shared and reused accross different pipeline executions.</li>
<li>Kubernetes is the platform, meaning that pipelines can be deployed and executed basically anywhere.</li>
<li>Sequential execution of <code>Tasks</code> defines a <code>Pipeline</code>. So creating a pipeline conceptually is as easy as defining the order of the tasks that we want to run and that may be already deployed in our Kubernetes cluster.</li>
<li>Any task can be run by instantiating it from a parametrized <code>TaskRun</code>. Because every previous task can be parametrized, reusing them is just a matter of calling the right task with a specific parameter. Again, decoupling and reusing.</li>
<li>Pipelines usually consume different resources like code, containers, files, etc. So Tekton uses <code>PipelineResources</code> as inputs and outputs between tasks to execute the pipeline workflow. That means that pipeline resources can be shared between pipelines in a descriptive way.</li>
<li>Every pipeline component is a CRD, so pipeline execution is a matter of containers orchestration, something that Kubernetes does really well and pretty fast. It is reliable, stable, scalable and performant.</li>
</ul>

<p>Let&rsquo;s try to understand Tekton pipelines decoupled architecture in the following diagram :
<img src="/img/tekton-jx-orchestration/TektonPipeline_Arch.png" alt="Tekton Pipeline Architecture" /></p>

<p>In terms of scalability, it&rsquo;s a nice way to isolate objects that can be reused easily, right?</p>

<h2 id="decoupling-ci-cd-is-good-but">Decoupling CI/CD is good, but&hellip;</h2>

<p>Tekton then is about the power of a decoupled CI/CD engine, which has many advantages. But no one said that defining decoupled CI/CD pipelines was an easy task. It can be complex from a conceptual point of view if you don&rsquo;t change your mindset. Traditionally, we&rsquo;ve been defining pipelines as different stages with their current steps to be executed. So, everything is only about how to orchestrate stages as sequential or parallel tasks, configuring parameters or conditions into the pipeline. Let&rsquo;s say that CI/CD has a monolithic mindset to configure and define pipelines.</p>

<p>If we start designing reusable components, then the resources, the pipeline flow, the execution parametrization and then feeding all components back and forth&hellip; This can be messy and error prone till having the complete decoupled pipeline map configured.</p>

<h2 id="an-orchestration-engine-to-manage-pipelines">An orchestration engine to manage pipelines</h2>

<p>We might think about how to orchestrate this. Let&rsquo;s then think about three decoupling best practices for pipelines in containerized ecosystems:</p>

<ul>
<li>The CI/CD execution  must be flexible, reusable and decoupled.</li>
<li>The pipeline orchestration must be manageable, understandable and easy to deploy.</li>
<li>Resources configuration for pipeline components needs to be standardized and easy to manage.</li>
</ul>

<p>CI/CD in not only about designing automation pipelines to deliver software. It is also about managing and providing all resources required for automation execution. In Kubernetes we will need to deal also with different objects like <code>Secrets</code>, <code>ServiceAccounts</code>, <code>PersistentVolumes</code>, <code>ConfigMaps</code>, <code>Ingress</code>, etc.</p>

<p>To focus on CI/CD pipelines, resources management should be an easy task.</p>

<h2 id="an-example-to-understand-tekton-and-jenkins-x-orchestration">An example to understand Tekton and Jenkins X orchestration</h2>

<p>We are using an example of building an application by defining and running CI/CD pipelines with standalone Tekton. Then, we are taking the same application to see how Jenkins X pipelines, using Tekton, generate similar objects but from a different focus. It will show how pipeline orchestration capabilities are applied to a decoupled CI/CD pipeline engine, no matter how I need to deal with platform or infrastructure resources.</p>

<p>To do that we are using a well known <a href="https://projects.spring.io/spring-petclinic/">Spring Petclinic example</a> Spring Boot application. And let&rsquo;s use a repo that is using traditional Jenkins pipeline to build the application, create a Docker container and deploy into Kubernetes cluster.</p>

<p>In my traditional Jenkins example I use two pipelines in fact that are automated using <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/cross-team-collaboration/">CloudBees Core Cross Team Collaboration</a> feature:</p>

<ul>
<li><a href="https://github.com/dcanadillas/petclinic-kaniko">Repo with a Jenkins pipeline to build</a> the app, the Docker container and push it into Docker Registry</li>
<li>A simple <a href="https://github.com/dcanadillas/petclinic-kaniko-deploy">Jenkins pipeline repo to deploy</a> the previous container published</li>
</ul>

<p>But in our case, we are putting those pipeline steps into one pipeline to do the build and deploy. First with a &ldquo;pure Tekton&rdquo; pipeline definition, and later using Jenkins X serverless pipelines.</p>

<h3 id="the-pure-tekton-way">The pure Tekton way</h3>

<p>So let&rsquo;s try to configure and execute the pipeline from a pure Tekton pipeline point of view. That means:</p>

<ul>
<li>Creating <a href="https://github.com/tektoncd/pipeline/blob/master/docs/resources.md"><code>PipelineResources</code></a> that are going to be used by <code>Tasks</code></li>
<li>Defining and creating <a href="https://github.com/tektoncd/pipeline/blob/master/docs/tasks.md"><code>Tasks</code></a> that contain the steps to be executed in the <code>Pipeline</code></li>
<li>Defining and creating the <a href="https://github.com/tektoncd/pipeline/blob/master/docs/pipelines.md"><code>Pipeline</code></a> that orchestrates the execution of <code>Tasks</code> with <code>Resources</code></li>
<li>Creating the <a href="https://github.com/tektoncd/pipeline/blob/master/docs/pipelineruns.md"><code>PipelineRun</code></a></li>
<li>Installing required Kubernetes resources, like <code>Secrets</code>, <code>ServiceAccounts</code> or permissions, in order to execute required steps within right containers (e.g. secrets used by Kaniko builder)</li>
</ul>

<p>I already created a <a href="https://github.com/dcanadillas/petclinic-tekton">GitHub Repo</a> with all YAML files needed (except secrets, which are explained in the <code>README</code> file). But let&rsquo;s go through them.</p>

<p>We can create a YAML file with all <code>Tasks</code> objects and the <code>Pipeline</code> definition. We can call this file <code>petclinic-pipeline.yaml</code>:</p>

<pre><code class="language-yaml">apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
  name: build-maven
spec:
  inputs:
    resources:
      - name: workspace
        type: git
    params:
      - name: workingDir
        description: Working directory parameter
        default: /workspace/workspace
  outputs:
    resources:
      - name: workspace
        type: git
  steps:
    - name: maven-build
      image: gcr.io/cloud-builders/mvn:3.5.0-jdk-8
      workingDir: ${inputs.params.workingDir}
      command: [&quot;mvn&quot;]
      args:
        - &quot;clean&quot;
        - &quot;install&quot;
    - name: ls-target
      image: ubuntu
      command:
        - &quot;ls&quot;
      args:
        - &quot;-la&quot;
        - &quot;/workspace/workspace/target&quot;
---
apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
  name: build-kaniko
spec:
  inputs:
    resources:
      - name: workspace
        type: git
        targetPath: petclinic
    params:
      - name: workingDir
        description: Working directory parameter
        default: /workspace/petclinic
      - name: DockerFilePath
        decription: Path to DockerFile
        default: /workspace/petclinic/Dockerfile
  outputs:
    resources:
      - name: dockerImage
        type: image
  steps:
    - name: kaniko-build
      image: gcr.io/kaniko-project/executor:latest
      command:
        - /kaniko/executor
      args:
        - &quot;--dockerfile=${inputs.params.DockerFilePath}&quot;
        - &quot;--context=${inputs.params.workingDir}&quot;
        - &quot;--destination=${outputs.resources.dockerImage.url}&quot;
      env:
        - name: GOOGLE_APPLICATION_CREDENTIALS
          value: /secret/kaniko-secret.json
      volumeMounts:
        - name: kaniko-secret
          mountPath: /secret
  volumes:
    - name: kaniko-secret
      secret:
        secretName: emea-sa-secret
---
apiVersion: tekton.dev/v1alpha1
kind: Task
metadata:
  name: deploy-kubectl
spec:
  inputs:
    resources:
      - name: workspacedeploy
        type: git
    params:
      - name: workingDir
        description: Working directory parameter
        default: /workspace/workspacedeploy
      - name: deployFile
        description: Deployment file for app
        default: test-deploy.yaml
        # Just a parameter to check the depployment name, because one step will force redeploy
      - name: deploymentName
        description: The K8s deployment object name
        default: petclinic
  steps:
    - name: kubectl-clean
      image: gcr.io/cloud-builders/kubectl:latest
      workingDir: ${inputs.params.workingDir}
      command: [&quot;/bin/bash&quot;]
      args:
        - -c
        - MYDEPLOY=$(kubectl get deployments -l app=petclinic -o name | awk -F'/' '{print $2}');
        - if [ &quot;$MYDEPLOY&quot; = &quot;${inputs.params.deploymentName}&quot; ];
        - then kubectl delete deployment $MYDEPLOY;
        - fi
    - name: kubectl-deploy
      image: gcr.io/cloud-builders/kubectl:latest
      workingDir: ${inputs.params.workingDir}
      command: [&quot;kubectl&quot;]
      args:
        - apply
        - -f 
        - ${inputs.params.deployFile}
---
apiVersion: tekton.dev/v1alpha1
kind: Pipeline
metadata:
  name: petclinic-pipeline
spec:
  resources:
    - name: source-repo
      type: git
    - name: docker-container
      type: image
    - name: deploy-repo
      type: git
  tasks:
    - name: petclinic-maven
      taskRef:
        name: build-maven
      resources:
        inputs:
          - name: workspace
            resource: source-repo
        outputs:
          - name: workspace
            resource: source-repo
    - name: petclinic-kaniko
      taskRef:
        name: build-kaniko
      resources:
        inputs:
          - name: workspace
            resource: source-repo
            from: 
              - petclinic-maven
        outputs:
          - name: dockerImage
            resource: docker-container
    - name: petclinic-deploy
      taskRef:
        name: deploy-kubectl
      runAfter:
        - petclinic-kaniko
      resources:
        inputs:
          - name: workspacedeploy
            resource: deploy-repo
      params:
        - name: deployFile
          value: test-deploy-secret.yaml
</code></pre>

<p>Then we can define also a file <code>pipeline-resources.yaml</code> with all required <code>PipelineResources</code> objects needed:</p>

<pre><code class="language-yaml">apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
  name: petclinic-git
spec:
  type: git
  params:
    - name: url
      value: https://github.com/dcanadillas/petclinic-kaniko.git
---
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
  name: petclinic-image
spec:
  type: image
  params:
    - name: url
      value: eu.gcr.io/emea-sa-demo/petclinic-kaniko:latest
---
apiVersion: tekton.dev/v1alpha1
kind: PipelineResource
metadata:
  name: petclinic-deploy
spec:
  type: git
  params:
    - name: url
      value: https://github.com/dcanadillas/petclinic-kaniko-deploy.git
</code></pre>

<p>As <code>PipelineResources</code> we are using the two GitHub repositories from the original repos as inputs (one for the application and the other for deployment definitions), and one container image as output to be built.</p>

<p>Next, let&rsquo;s define the <code>PipelineRun</code>, that executes and instantiate the pipeline, in a file <code>petclinic-run.yaml</code>:</p>

<pre><code class="language-yaml">apiVersion: tekton.dev/v1alpha1
kind: PipelineRun
metadata:
  name: petclinic-pipelinerun
spec:
  pipelineRef:
    name: petclinic-pipeline
  serviceAccount: 'default'
  serviceAccounts:
    - taskName: petclinic-deploy
      serviceAccount: tekton-deployment
  resources:
  - name: source-repo
    resourceRef:
      name: petclinic-git
  - name: docker-container
    resourceRef:
      name: petclinic-image
  - name: deploy-repo
    resourceRef:
      name: petclinic-deploy
</code></pre>

<p>In the <code>PipelineRun</code> object it&rsquo;s important to understand that it is intended in the way of defining the specific running instance of a deployed pipeline. It&rsquo;s the right place to assign running input/output parameter values, specify <code>serviceAccounts</code> to be used, or referencing the right <code>PipelineResources</code> to be used.</p>

<p>Once we have the YAML definitions, it&rsquo;s only a matter of Kubernetes deployment of the Tekton CRD objects. We can use then <code>kubectl</code> command line tool to deploy first the two YAML files with <code>PipelineResources</code>, <code>Tasks</code> and <code>Pipeline</code>:</p>

<pre><code class="language-bash">$ kubectl apply -f petclinic-resources.yaml,petclinic-pipeline.yaml
pipelineresource.tekton.dev/petclinic-git created
pipelineresource.tekton.dev/petclinic-image created
pipelineresource.tekton.dev/petclinic-deploy created
task.tekton.dev/build-maven created
task.tekton.dev/build-kaniko created
task.tekton.dev/deploy-kubectl created
pipeline.tekton.dev/petclinic-pipeline created
</code></pre>

<p>We can see then the objects already deployed, as <code>CRDs</code> objects.</p>

<pre><code class="language-bash">$ kubectl get tasks,pipelines,pipelineresources
NAME                             AGE
task.tekton.dev/build-kaniko     2s
task.tekton.dev/build-maven      2s
task.tekton.dev/deploy-kubectl   2s

NAME                                     AGE
pipeline.tekton.dev/petclinic-pipeline   2s

NAME                                           AGE
pipelineresource.tekton.dev/petclinic-deploy   2s
pipelineresource.tekton.dev/petclinic-git      2s
pipelineresource.tekton.dev/petclinic-image    2s
</code></pre>

<p>And we can check the pipeline to be executed:</p>

<pre><code class="language-bash">$ kubectl describe pipeline.tekton.dev/petclinic-pipeline
Name:         petclinic-pipeline
Namespace:    tekton-pipelines
Labels:       &lt;none&gt;
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {&quot;apiVersion&quot;:&quot;tekton.dev/v1alpha1&quot;,&quot;kind&quot;:&quot;Pipeline&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;petclinic-pipeline&quot;,&quot;namespace&quot;:&quot;tekton-pipelin...}}
API Version:  tekton.dev/v1alpha1
Kind:         Pipeline
Metadata:
  Creation Timestamp:  2019-07-13T10:19:12Z
  Generation:          1
  Resource Version:    22835394
  Self Link:           /apis/tekton.dev/v1alpha1/namespaces/tekton-pipelines/pipelines/petclinic-pipeline
  UID:                 a8d4e6ca-a557-11e9-b2c9-42010a8400ab
Spec:
  Resources:
    Name:  source-repo
    Type:  git
    Name:  docker-container
    Type:  image
    Name:  deploy-repo
    Type:  git
  Tasks:
    Name:  petclinic-maven
    Resources:
      Inputs:
        Name:      workspace
        Resource:  source-repo
      Outputs:
        Name:      workspace
        Resource:  source-repo
    Task Ref:
      Name:  build-maven
    Name:    petclinic-kaniko
    Resources:
      Inputs:
        From:
          petclinic-maven
        Name:      workspace
        Resource:  source-repo
      Outputs:
        Name:      dockerImage
        Resource:  docker-container
    Task Ref:
      Name:  build-kaniko
    Name:    petclinic-deploy
    Params:
      Name:   deployFile
      Value:  test-deploy-secret.yaml
    Resources:
      Inputs:
        Name:      workspacedeploy
        Resource:  deploy-repo
    Run After:
      petclinic-kaniko
    Task Ref:
      Name:  deploy-kubectl
Events:      &lt;none&gt;
</code></pre>

<p><code>Pipeline</code> object is going to execute in order the tasks already existing in K8s cluster: <code>build-maven</code>, <code>build-kaniko</code> and <code>deploy-kubectl</code>. And for that we are setting as inputs and outputs the different <code>PipelineResources</code>.</p>

<p>But we are using some resources expected in the cluster that are not created at pipeline definition, like <code>Secrets</code> for pushing into private Docker Registry (GCR in my case), <code>ServiceAccount</code>, <code>Roles</code> and <code>RoleBindings</code> to deploy in Kubernetes with the specific permissions. I am not focusing on doing this at this post, but you can read how to do it in my <a href="https://github.com/dcanadillas/petclinic-tekton/blob/master/README.md#configuration-requirements">original repo documentation</a>.</p>

<p>Now, running the pipeline is just about deploying the <code>PipelineRun</code> definition in our <code>pipeline-run.yaml</code> file:</p>

<pre><code class="language-bash">kubectl apply -f petclinic-run.yaml
</code></pre>

<p>A Tekton <code>CRD</code> is then created to run the pipeline.</p>

<pre><code>pipelinerun.tekton.dev/petclinic-pipelinerun created
</code></pre>

<p>Different things are going to happen in this case:</p>

<ul>
<li>The <code>PipelineRun</code> is going to create a <code>TaskRun</code> per <code>Task</code>. Than means that every stage of the pipeline is going to be executed within the <code>Task</code> definition already deployed, depending on the <code>Pipeline</code> flow created</li>
<li>Every <code>TaskRun</code> is going to be executed in a Kubernetes <code>Pod</code>, using the containers specified in the different <code>Steps</code> in <code>Tasks</code>. Remember about Tekton: One step. One container.</li>
<li><code>PipelineResources</code> are just &ldquo;consumed or produced&rdquo; by <code>Tasks</code> depending on the parameters used during <code>TaskRuns</code></li>
<li>Different <code>ServiceAccounts</code> are going to be used for <code>Tasks</code> depending on the definition of the <code>PipelineRun</code> (it makes sense that different roles are needed for different tasks)</li>
</ul>

<p>So, if we take a look about our execution after the complete run:</p>

<pre><code class="language-bash">$ kubectl get pods
NAME                                                      READY   STATUS      RESTARTS   AGE
petclinic-6f668b59b5-w8zrr                                1/1     Running     0          21s
petclinic-pipelinerun-petclinic-deploy-54kws-pod-c5a511   0/3     Completed   0          1m
petclinic-pipelinerun-petclinic-kaniko-s4nnw-pod-67721d   0/4     Completed   0          1m
petclinic-pipelinerun-petclinic-maven-9gnpf-pod-fa757f    0/5     Completed   0          4m
tekton-pipelines-controller-6b565f9859-knqsb              1/1     Running     0          8h
tekton-pipelines-webhook-7f47c995cd-db2rv                 1/1     Running     0          8h
</code></pre>

<p>And we can check that <code>TaskRuns</code> where created:</p>

<pre><code class="language-bash">$ kubectl get taskruns
NAME                                           SUCCEEDED   REASON      STARTTIME   COMPLETIONTIME
petclinic-pipelinerun-petclinic-deploy-54kws   True        Succeeded   2h          2h
petclinic-pipelinerun-petclinic-kaniko-s4nnw   True        Succeeded   2h          2h
petclinic-pipelinerun-petclinic-maven-9gnpf    True        Succeeded   2h          2h
</code></pre>

<p>Looking inside at one of them we can see the execution status and completion:</p>

<pre><code class="language-bash">$ kubectl describe taskrun/petclinic-pipelinerun-petclinic-kaniko-s4nnw

[...]

Status:
  Completion Time:  2019-07-15T18:13:25Z
  Conditions:
    Last Transition Time:  2019-07-15T18:13:25Z
    Message:               All Steps have completed executing
    Reason:                Succeeded
    Status:                True
    Type:                  Succeeded
  Pod Name:                petclinic-pipelinerun-petclinic-kaniko-s4nnw-pod-67721d
  Start Time:              2019-07-15T18:12:55Z
  Steps:
    Name:  kaniko-build
    Terminated:
      Container ID:  docker://f077cc23c848bb5a937201719543db01d8cfd2712dabfb577df4b02b8c32b704
      Exit Code:     0
      Finished At:   2019-07-15T18:13:24Z
      Reason:        Completed
      Started At:    2019-07-15T18:13:09Z
    Name:            image-digest-exporter-kaniko-build-ldh8q
    Terminated:
      Container ID:  docker://106f2819a96246761eed0486e2f66428dafb38cdff291c3004e9e75d9a245963
      Exit Code:     0
      Finished At:   2019-07-15T18:13:24Z
      Message:       []
      Reason:        Completed
      Started At:    2019-07-15T18:13:12Z
    Name:            create-dir-workspace-sgtdd
    Terminated:
      Container ID:  docker://f80a5a1da2898e78edcaf2110d0adb304131ff7f07a14b4e40ab8a03a9f69167
      Exit Code:     0
      Finished At:   2019-07-15T18:13:13Z
      Reason:        Completed
      Started At:    2019-07-15T18:13:06Z
    Name:            source-copy-workspace-8dmsp
    Terminated:
      Container ID:  docker://081e363b08f7fe1d8bd21efa36c670531a6cbc5debf09b2662d8b27340d9ad48
      Exit Code:     0
      Finished At:   2019-07-15T18:13:14Z
      Reason:        Completed
      Started At:    2019-07-15T18:13:06Z
Events:              &lt;none&gt;
</code></pre>

<p>Finally, the application should have been deployed. Let&rsquo;s check:</p>

<pre><code class="language-bash">$ kubectl get svc
NAME                          TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE
petclinic-service             LoadBalancer   10.31.240.225   35.240.80.216   9090:31261/TCP   12m
tekton-pipelines-controller   ClusterIP      10.31.254.32    &lt;none&gt;          9090/TCP         11d
tekton-pipelines-webhook      ClusterIP      10.31.242.231   &lt;none&gt;          443/TCP          11d
</code></pre>

<p><img src="/img/tekton-jx-orchestration/petclinic_tekton.png" alt="Petclinic deployment with Tekton" /></p>

<p>We then confirmed that this kind of pipeline definition and execution in Kubernetes can be extremely powerful in terms of reusability and extensibility. We&rsquo;ve just deployed tasks definitions, ordering and resources descriptions, and running specification. Everything is about playing with Cloud Native resources to deal with CI/CD pipeline objects and executions.</p>

<p>But let&rsquo;s face it. This is not a very easy way of executing pipelines. Powerful, but complex from a conceptual pipeline design.</p>

<h3 id="playing-with-jenkins-x-serverless-pipelines">Playing with Jenkins X Serverless Pipelines</h3>

<p>We can think about orchestrating the previous application pipeline with Tekton to simplify it&rsquo;s usage and make it &ldquo;manageable&rdquo;. Jenkins X is a very good solution to do that, because:</p>

<ul>
<li>Jenkins X <a href="https://jenkins-x.io/architecture/build-packs/">build packs</a> give you pre-set pipelines (among other things) for your applications. You can then start with a prepared and curated conceptual design that is easy to manage.</li>
<li>Jenkins X is not a pipeline engine, it&rsquo;s a CI/CD solution. So all resources like Git repos, users, <code>serviceAccounts</code>, environment <code>namespaces</code>, <code>ingress</code> rules, Docker Registry credentials, etc. are already configured to be able to orchestrate pipeline execution.</li>
<li>Configuring or extending pipeline definitions is just a matter of defining the flow of stages with their steps/containers to be executed. And Tekton is used automatically to decouple run definition and execution. Monolithic abstraction, decoupled execution.</li>
<li><a href="https://jenkins-x.io/architecture/jenkins-x-pipelines/">Jenkins X Pipelines</a> simplify any pipeline process to develop features, release or promote/deploy your code through environments.</li>
</ul>

<p>To understand what we mean about Tekton orchestration, let&rsquo;s try to simulate the same tasks and steps in our previous Tekton example, but defining a basic Jenkins X serverless pipeline without using build packs.</p>

<p><em>NOTE: Don&rsquo;t think about this as a standard way to work with Jenkins X. We are showing first a standalone <code>jenkins-x.yml</code> pipeline to demonstrate how Tekton objects are created and orchestrated automatically by Jenkins X.</em></p>

<p>Jenkins X cluster creation and installation it&rsquo;s just a matter of minutes. So, to start executing pipelines we&rsquo;ll start to deploy a Jenkins X cluster from scracth in GKE after <a href="https://jenkins-x.io/getting-started/install/">installing the Jenkins X CLI</a> (showing parameters instead of values):</p>

<pre><code class="language-bash">jx create cluster gke \
--cluster-name ${JX_CLUSTER} \
--default-admin-password ${MYPWD} \
--environment-git-owner ${GH_ORG} \
--min-num-nodes 3 --max-num-nodes 5 \
--machine-type n1-standard-2 \
--project-id ${GCP_PROJECT} --zone europe-west1-c \
--default-environment-prefix ${JX_CLUSTER} \
--git-provider-kind github \
--git-username ${GH_USER} \
--git-api-token ${GH_APITOKEN} \
--tekton \
--no-tiller \
-b
</code></pre>

<p>Once having the Jenkins X installation (serverless mode with <a href="https://jenkins-x.io/architecture/prow/">Prow</a> and Tekton), you can check that Tekton <code>CRDs</code> are already there:</p>

<pre><code class="language-bash">$ kubectl get crd
NAME                                           CREATED AT
[...]
pipelineresources.tekton.dev                   2019-07-10T11:38:50Z
pipelineruns.tekton.dev                        2019-07-10T11:38:50Z
pipelines.tekton.dev                           2019-07-10T11:38:50Z
[...]
taskruns.tekton.dev                            2019-07-10T11:38:50Z
tasks.tekton.dev                               2019-07-10T11:38:50Z
[...]
</code></pre>

<p>Right now, let&rsquo;s try to simulate the Tekton pipeline that we executed before from a &ldquo;standalone&rdquo; Jenkins X YAML pipeline. This means, as mentioned before, that we are not using any Jenkins X <code>Build Packs</code>.</p>

<p>If we clone the same <a href="https://github.com/dcanadillas/petclinic-kaniko">petclinic-kaniko repo</a> into a local directory <code>petclinic-jenkins-x</code> we can do the following (working from local):</p>

<ul>
<li>Deleting any Git repo reference from the local cloned directory with <code>rm -r ./.git*</code> so we are working for sure from a local copy</li>
<li>Create in the local directory the following <code>jenkins-x.yml</code> file that simulates de <code>maven-build</code>, <code>kaniko-build</code> and <code>kubectl-deploy</code> tasks from our previous Tekton example (for simplicity we are not adding the step used before to check deployment):
<br /></li>
</ul>

<pre><code class="language-yaml">  buildPack: none
  pipelineConfig:
    pipelines:
      release:
        pipeline:
          stages:
            - name: Maven Build
              agent:
                image: maven
              steps:
                - command: mvn
                  args:
                    - clean
                    - install
            - name: Kaniko Build
              agent:
                image: gcr.io/kaniko-project/executor:latest
              steps:
                - command: /kaniko/executor
                  args:
                    - &quot;--dockerfile=Dockerfile&quot;
                    - &quot;--context=/workspace/source&quot;
                    - &quot;--destination=eu.gcr.io/emea-sa-demo/petclinic-kaniko:latest&quot;
            - name: Kubectl Deploy 
              agent:
                image: gcr.io/cloud-builders/kubectl:latest
              steps:
                - command: kubectl
                  args:
                    - apply
                    - -f
                    - https://github.com/dcanadillas/petclinic-kaniko-deploy/blob/master/test-deploy.yaml?raw=true
                    - -n
                    - jx-staging
</code></pre>

<ul>
<li>Import the project with Jenkins X from the local directory:
<br /></li>
</ul>

<pre><code class="language-bash">  $ jx import --git-username dcanadillas --org jx-dcanadillas --name petclinic-jenkins-x -m YAML
  [...]
  ? Git user name: dcanadillas
  The directory /Users/david/Documents/Workspace/Technologists/petclinic-kaniko is not yet using git
  ? Would you like to initialise git now? Yes
  ? Commit message:  Initial import
  [...]
  ? Using organisation: jx-dcanadillas
  ? Enter the new repository name:  petclinic-jenkins-x
  [...]
</code></pre>

<p><em>NOTE: We could have imported first the project and then changed the <code>jenkins-x.yml</code>. In that case Jenkins X would have detected to apply a Maven <code>Build Pack</code> doing the first run with it. But I wanted to force Jenkins X to not recognize any <code>Build Pack</code> from the beggining, so there is no pipeline execution different than the one we want to simulate.</em></p>

<p>Then the first pipeline execution is automatically run, because Jenkins X creates the GitHub repo with its webhook for you. The status of the execution can be seen by:</p>

<pre><code class="language-bash">$ jx get activity -f petclinic -w
[...]
jx-dcanadillas/petclinic-jenkins-x/master #2                   4m24s    4m17s Succeeded
  Maven Build                                                  4m24s    2m10s Succeeded
    Credential Initializer 8xhk4                               4m24s       0s Succeeded
    Working Dir Initializer 7c8l2                              4m23s       0s Succeeded
    Place Tools                                                4m22s       0s Succeeded
    Git Source Jx Dcanadillas Petclinic Jenkin Rhg88 Jhl       4m21s       2s Succeeded https://github.com/jx-dcanadillas/petclinic-jenkins-x
    Git Merge                                                  4m20s       2s Succeeded
    Step2                                                      4m20s     2m5s Succeeded
    Source Mkdir Jx Dcanadillas Petclinic Jenkin Rhg88 K       4m20s     2m6s Succeeded
    Source Copy Jx Dcanadillas Petclinic Jenkin Rhg88 Z2       4m20s     2m6s Succeeded
  Kaniko Build                                                 1m38s      15s Succeeded
    Credential Initializer Lpcwz                               1m38s       0s Succeeded
    Working Dir Initializer Ww5lp                              1m37s       0s Succeeded
    Place Tools                                                1m36s       0s Succeeded
    Create Dir Workspace Zvlmw                                 1m35s       0s Succeeded
    Source Copy Workspace P5n28                                1m35s       2s Succeeded
    Step2                                                      1m34s      10s Succeeded
    Source Mkdir Jx Dcanadillas Petclinic Jenkin Rhg88 R       1m34s      10s Succeeded
    Source Copy Jx Dcanadillas Petclinic Jenkin Rhg88 Rg       1m33s      10s Succeeded
  Kubectl Deploy                                                1m5s      58s Succeeded
    Credential Initializer Hrmkx                                1m5s       0s Succeeded
    Working Dir Initializer Wgqh9                               1m4s       0s Succeeded
    Place Tools                                                 1m3s       0s Succeeded
    Create Dir Workspace J9wgg                                  1m2s       0s Succeeded
    Source Copy Workspace K8nz4                                 1m1s       1s Succeeded
    Step2                                                         9s       2s Succeeded
</code></pre>

<p>As we can see in the activity logs, three stages are executed, which are corresponding to their Tekton <code>Tasks</code>. If we search for Tekton components, we can check that everything is created by Jenkins X for the Tekton engine execution:</p>

<pre><code class="language-bash">$ kubectl get tasks,tasks,taskruns,pipeline,pipelineruns
NAME                                                                      AGE
task.tekton.dev/jx-dcanadillas-petclinic-jenkin-rhg88-kaniko-build-2      7m
task.tekton.dev/jx-dcanadillas-petclinic-jenkin-rhg88-kubectl-deploy-2    7m
task.tekton.dev/jx-dcanadillas-petclinic-jenkin-rhg88-maven-build-2       7m

NAME                                                                               AGE
taskrun.tekton.dev/jx-dcanadillas-petclinic-jenkin-rhg88-2-kaniko-build-kpk8v      4m
taskrun.tekton.dev/jx-dcanadillas-petclinic-jenkin-rhg88-2-kubectl-deploy-scs4r    4m
taskrun.tekton.dev/jx-dcanadillas-petclinic-jenkin-rhg88-2-maven-build-d42f4       7m

NAME                                                          AGE
pipeline.tekton.dev/jx-dcanadillas-petclinic-jenkin-rhg88-2   7m

NAME                                                             AGE
pipelinerun.tekton.dev/jx-dcanadillas-petclinic-jenkin-rhg88-2   7m
</code></pre>

<p>There they are. The same components that were created &ldquo;manually&rdquo; in our previous Tekton example. Truth is that <code>Tasks</code> configurations in this case use more parameters inside (<code>DOCKER_REGISTRY</code>, <code>REPO_OWNER</code>, <code>PIPELINE_KIND</code>, <code>BUILD_NUMBER</code>, <code>APP_NAME</code>, <code>VERSION</code>&hellip; and more) that are configured by Jenkins X. But that is the thing. Just because Jenkins X is orchestrating the pipeline execution from a simpler definition it&rsquo;s abstracting some configurations for you and using some parameters to make it easier.</p>

<p>If we check the pods used by the pipeline execution we will find again three of them (one pod per Tekton task):</p>

<pre><code class="language-bash">$ kubectl get pod -n jx
NAME                                                                      READY   STATUS      RESTARTS   AGE
[...]
jx-dcanadillas-petclinic-jenkin-rhg88-2-kaniko-build-kpk8v-pod-4dd505     0/6     Completed   0          3m41s
jx-dcanadillas-petclinic-jenkin-rhg88-2-kubectl-deploy-scs4r-pod-a2f48a   0/4     Completed   0          2m52s
jx-dcanadillas-petclinic-jenkin-rhg88-2-maven-build-d42f4-pod-432bb5      0/6     Completed   0          6m14s
[...]                                              1/1     Running     0          5d10h
tekton-pipelines-controller-687cfbcc89-69jht                              1/1     Running     0          5d10h
tekton-pipelines-webhook-7fd7f8cdcc-pqv4c                                 1/1     Running     0          5d10h
tide-5f8fb5964c-29pgt                                                     1/1     Running     0          5d10h
</code></pre>

<p>And we can check that same application has been deployed in the namespace <code>jx-staging</code>:</p>

<pre><code class="language-bash">$ kubectl get svc petclinic-service -n jx-staging
NAME                TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)          AGE
petclinic-service   LoadBalancer   10.23.240.64   35.195.126.19   9090:31194/TCP   10m
</code></pre>

<p><img src="/img/tekton-jx-orchestration/petclinic-jx_pipeline.png" alt="Petclinic deployed with JX pipeline" /></p>

<p>We can conclude about the following about simulating the same Tekton configuration with Jenkins X Pipelines:</p>

<ul>
<li>CI/CD pipeline was designed in one YAML file of a couple of lines, instead of defining several YAML files with cross-reference definitions (we could have defined one YAML file for the Tekton example, but would have been very big file with lots of lines of code and not very manageable).</li>
<li>Jenkins X, from that monilithic simple definition, creates automatically all Tekton decoupled components (<code>Tasks</code>, <code>Pipeline</code>, <code>PipelineResources</code>, <code>PipelineRuns</code>, etc.).</li>
<li>There is <strong>no need to define any secret or serviceAccount</strong>. Jenkins X configures the platform parameters automatically when installing, passing those parameters to the pipeline Tekton components at execution creation. You can also change default parameters in the pipeline with an easy commang line execution <code>jx create variable</code>, or just adding them in the YAML file.</li>
<li>It is much easier to create a Jenkins X pipeline to orchestrate Tekton components than creating the isolated components by itself and then deploying the execution.</li>
<li>All GitHub &ldquo;dirty work&rdquo; like webhooks, credentials or updates required are already configured by Jenkins X to work only on code changes.</li>
<li>We could see also that pipeline execution is faster because Jenkins X takes care about artifact caching and optimizing Kubernetes resources used.</li>
</ul>

<p>Let&rsquo;s use the following diagram to show how a Jenkins X pipeline definition is translated into Tekton components, like also happened in our example:</p>

<p><img src="/img/tekton-jx-orchestration/jxpipeline-to-tekton.png" alt="Jenkins X Pipelines to Tekton" /></p>

<h3 id="the-pure-jenkins-x-way">The pure Jenkins X way</h3>

<p>But let&rsquo;s be honest. If we want to take advantadge of a real pipeline orchestration platform like Jenkins X, previous configuration of <code>jenkins-x.yml</code> is not the best way to go. That was intended to understand how a Jenkins X pipeline definition is abstracting Tekton components to run CI/CD pipelines.</p>

<p>The real value of a CI/CD pipeline orchestration platform is about something else than abstracting a powerful decoupled CI/CD engine like Tekton. So let&rsquo;s try to understand what I am talking about by doing CI/CD with the same <a href="https://github.com/dcanadillas/petclinic-kaniko">petclinic-kaniko repo</a> in a <em>pure Jenkins X way</em> using Jenkins X build packs.</p>

<p>As shown before, for demonstration purposes I am cloning first the original repo and then importing from local to to automatically create from Jenkins X a new repo in GitHub. I could import directly from the GitHub repo, creating then a new commit with the changes to continue to do CI/CD with Jenkins X (for example changing my old <code>Jenkinsfile</code> for a new <code>jenkins-x.yaml</code>).</p>

<p>Because I don&rsquo;t want to change the <a href="https://github.com/dcanadillas/petclinic-kaniko">original repo</a>, let&rsquo;s do the following:</p>

<ul>
<li>Clone original repo from the terminal with <code>git clone https://github.com/dcanadillas/petclinic-kaniko petclinic-jx</code>.</li>
<li>Remove git references <code>rm -rf petclinic-jx/.git*</code>.</li>
<li>Now, create Jenkins X project by importing from the local repo into a new GitHub repository:
<br /></li>
</ul>

<pre><code class="language-bash">  jx import --git-username dcanadillas --org jx-dcanadillas --name petclinic-jx -m YAML
</code></pre>

<p>Note that we are using <code>-m YAML</code> to force Jenkins X creating a new Jenkins X serverless pipeline project instead of a Static <code>Jenkinsfile</code> one.</p>

<p>Different things are going on when importing the project with Jenkins X:</p>

<ul>
<li>Creates a local git repo (similar to <code>git init</code>).</li>
<li>Selects a <a href="https://draft.sh/">Draft</a> build pack from Jenkins X. (<em>In this case takes a <a href="https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs/maven">maven build pack</a></em>).</li>
<li>Pushes the new repository with changes applied from the build pack to a repo in the GitHub organization specified in the <code>--org</code> parameter.</li>
<li>Creates a GitHub webhook for Jenkins X to be able to trigger pipelines automatically with any code change.</li>
<li>Runs the pipeline to build the application and promotes to Staging using GitOps.</li>
</ul>

<p>Basically, Jenkins X already configured the CI/CD pipeline just by importing the project. So now there is already a pipeline running. All steps executed by the pipeline build can be seen with <code>jx get activity -f petclinic-jx -w</code> . Once it succeeded:</p>

<pre><code class="language-bash">$ jx get activity -f petclinic-jx --build 1
STEP                                                 STARTED AGO DURATION STATUS
jx-dcanadillas/petclinic-jx/master #1                     55m57s    3m59s Succeeded Version: 0.0.1
  from build pack                                         55m57s    3m59s Succeeded
    Credential Initializer Dk42x                          55m57s       1s Succeeded
    Working Dir Initializer Fqgpx                         55m56s       0s Succeeded
    Place Tools                                           55m55s       0s Succeeded
    Git Source Jx Dcanadillas Petclinic Jx Mas Rt8qq      55m54s       2s Succeeded https://github.com/jx-dcanadillas/petclinic-jx
    Git Merge                                             55m53s       2s Succeeded
    Setup Jx Git Credentials                              55m52s       2s Succeeded
    Build Mvn Deploy                                      55m52s    2m33s Succeeded
    Build Skaffold Version                                55m52s    2m34s Succeeded
    Build Container Build                                 55m52s    2m49s Succeeded
    Build Post Build                                      55m52s    2m50s Succeeded
    Promote Changelog                                     55m51s    2m54s Succeeded
    Promote Helm Release                                  55m51s     3m0s Succeeded
    Promote Jx Promote                                    55m51s    3m53s Succeeded
  Promote: staging                                        52m43s      45s Succeeded
    PullRequest                                           52m43s      45s Succeeded  PullRequest: https://github.com/dcanadillas-kube/environment-dcanadillas-cloudbees-staging/pull/6 Merge SHA: f765c6fe2d91bff40d4fcbc30641cd92b35849a9
    Update                                                51m58s       0s Succeeded
</code></pre>

<p>The pipeline created by Jenkins X has just two Tekton tasks with different steps. The first Tekton task from build pack and a second task <code>Promote: Staging</code>. This just comes from the pipeline defined by Jenkins X:</p>

<pre><code class="language-bash">$ cat jenkins-x.yml
buildPack: maven
</code></pre>

<p>It&rsquo;s a <strong>one line pipeline</strong> that it&rsquo;s just &ldquo;inheriting&rdquo; from the <a href="https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/blob/master/packs/maven/pipeline.yaml">maven build pack pipeline</a>.</p>

<p>Bottom line here is that build packs are templates for building your application. So developers don&rsquo;t need to think about how to conceptually design the CI/CD pipelines, or components needed to manage Kubernetes in order to promote the application. But you can extend using the <code>jenkins-x.yaml</code> or by executing <code>jx create step</code> command. In this case Jenkins X already propose a <a href="https://jenkins-x.io/architecture/build-packs/#lifecycles">pipeline lifecycle</a>, that it is translated on how Tekton decouples the pipeline execution objects.</p>

<p>Coming back to our execution, the application is just automatically deployed and versioned into the <code>Staging</code> environment at version <code>0.0.1</code>:</p>

<pre><code class="language-bash">$ jx get version
APPLICATION      STAGING PODS URL
petclinic                1/1
petclinic-jx     0.0.1        http://petclinic-jx.jx-staging.cbjx.dcanadillas.com
</code></pre>

<p>In this output we can see also our previous <code>petclinic</code> application deployed with our previous example  with the &ldquo;customized&rdquo; Jenkins X pipeline. And our recently deployed Jenkins X application <code>petclinic-jx</code>. Some differences between them are that we needed to define deployment in the first pipeline example (non-pure Jenkins X way), but there is no versioning or promotion lifecycle. In the other hand, our last application build - just by importing it with Jenkins X - it is versioned, deployed in the right environment and with an access url. This means that the Kubernetes <code>service</code>, <code>deployyment</code> and <code>ingress</code> have been configured automatically. And it also used GitOps promotion to track and audit all versioning and promotion environment.</p>

<p>But, the last application deployment wasn&rsquo;t really successful:</p>

<pre><code class="language-bash">$ jx open petclinic-jx -n jx-staging
petclinic-jx: http://petclinic-jx.jx-staging.cbjx.dcanadillas.com
</code></pre>

<p>We get a <code>503</code> error from the application.</p>

<p><img src="/img/tekton-jx-orchestration/petclinic-jx_503.png" alt="Petclinic error 503" /></p>

<p>If we check what is going on:</p>

<pre><code class="language-bash">$ kubectl describe pod $(kubectl get pods -n jx-staging | awk '/petclinic-jx/ {print $1}') -n jx-staging
Name:               jx-petclinic-jx-845cbfcd88-8fvrx
Namespace:          jx-staging

[...]

  Events:
  Type     Reason     Age                   From                                                          Message
  ----     ------     ----                  ----                                                          -------
  Warning  Unhealthy  45m (x20 over 8h)     kubelet, gke-dcanadillas-cloudbee-default-pool-98606002-b9hc  Liveness probe failed: Get http://10.20.0.47:8080/actuator/health: net/http: request canceled (Client.Timeout exceeded while awaiting headers)
  Warning  Unhealthy  35m (x376 over 9h)    kubelet, gke-dcanadillas-cloudbee-default-pool-98606002-b9hc  Liveness probe failed: HTTP probe failed with statuscode: 404
  Normal   Pulled     30m (x138 over 9h)    kubelet, gke-dcanadillas-cloudbee-default-pool-98606002-b9hc  Container image &quot;gcr.io/emea-sa-demo/petclinic-jx:0.0.1&quot; already present on machine
  Warning  Unhealthy  5m35s (x881 over 9h)  kubelet, gke-dcanadillas-cloudbee-default-pool-98606002-b9hc  Readiness probe failed: Get http://10.20.0.47:8080/actuator/health: dial tcp 10.20.0.47:8080: connect: connection refused
  Warning  BackOff    38s (x1719 over 9h)   kubelet, gke-dcanadillas-cloudbee-default-pool-98606002-b9hc  Back-off restarting failed container
</code></pre>

<p>When the project was imported from Jenkins X and the build pack was applied, some components were added to the original project. Like<code>helm charts</code> required to deploy and promote through environments.
Looking at previous error logs in the <code>Pod</code>, the problem seems to be about changing the <code>probePath</code> in the file <code>petclinic-jx/charts/petclinic-jx/values.yaml</code>. Let&rsquo;s fix it. We need to change it from <code>/actuator/health</code> to <code>/</code>:</p>

<pre><code class="language-bash">cat charts/petclinic-jx/values.yaml | sed 's/\/actuator\/health/\//g' | tee charts/petclinic-jx/values.yaml
</code></pre>

<p>Then, we apply the changes to the repo and the pipeline will be triggered automatically:</p>

<pre><code class="language-bash">$ git commit -am &quot;probePath changed to the right value&quot;
$ git push -u origin master
[...]
To https://github.com/jx-dcanadillas/petclinic-jx.git
   b25d36b..234b99a  master -&gt; master
Rama 'master' configurada para hacer seguimiento a la rama remota 'master' de 'origin'.
</code></pre>

<p>The pipeline should start running againg to build the application and promote version <code>0.0.2</code> into <code>Staging</code>.</p>

<pre><code class="language-bash">$ jx get activity -f petclinic-jx -w

[...]

    Git Merge                                                  3m49s       2s Succeeded
    Setup Jx Git Credentials                                   3m49s       3s Succeeded
    Build Mvn Deploy                                           3m48s     2m5s Succeeded
    Build Skaffold Version                                     3m48s     2m6s Succeeded
    Build Container Build                                      3m48s    2m19s Succeeded
    Build Post Build                                           3m48s    2m20s Succeeded
    Promote Changelog                                          3m47s    2m24s Succeeded
    Promote Helm Release                                       3m47s    2m29s Succeeded
    Promote Jx Promote                                         3m47s    3m43s Succeeded
  Promote: staging                                             1m10s     1m6s Succeeded
    PullRequest                                                1m10s     1m6s Succeeded  PullRequest: https://github.com/dcanadillas-kube/environment-dcanadillas-cloudbees-staging/pull/7 Merge SHA: ced0df09abbc18b002114a28640901f5261da6c3
    Update                                                        4s       0s Succeeded
    Promoted                                                      4s       0s Succeeded  Application is at: http://petclinic-jx.jx-staging.cbjx.dcanadillas.com
</code></pre>

<p>Checking then that the new version <code>0.0.2</code> is deployed:</p>

<pre><code class="language-bash">$ jx get version
APPLICATION      STAGING PODS URL
petclinic                1/1
petclinic-jx     0.0.2   1/1  http://petclinic-jx.jx-staging.cbjx.dcanadillas.com
</code></pre>

<p>And opening the new version:</p>

<pre><code class="language-bash">$ jx open petclinic-jx -n jx-staging
</code></pre>

<p><img src="/img/tekton-jx-orchestration/petclinic-jx_staging.png" alt="Petclinic deployed" /></p>

<p>The pure Jenkins X way is about:</p>

<ul>
<li>Applying build pack for adopting pipeline definition experience best practices</li>
<li>Complete pipeline orchestration for build and promote using Tekton native objects</li>
<li>GitOps for promotion process</li>
<li>Kubernetes objects management required for pipeline execution</li>
<li>Pipeline extensions using easy pipeline definition with <code>jenkins-x.yml</code></li>
</ul>

<p>It is a simpler and more complete experience of CI/CD pipelines than trying to deal with Tekton itself. It is a real orchestration of all resources to develop, build and promote from real cloud native environment.</p>

<p>And last, to demonstrate that all Tekton orchestration happened, let&rsquo;s check all components created for all our Jenkins X executions:</p>

<pre><code class="language-bash">$ kubectl get tasks,taskruns,pipeline,pipelinerun,pipelineresources -n jx
NAME                                                                      AGE
task.tekton.dev/dcanadillas-kube-environment-dc-d495w-from-build-pack-7   23m
task.tekton.dev/dcanadillas-kube-environment-dc-hk95j-from-build-pack-1   24m
task.tekton.dev/dcanadillas-kube-environment-dc-wm85w-from-build-pack-6   10h
task.tekton.dev/dcanadillas-kube-environment-dc-wpqsp-from-build-pack-1   10h
task.tekton.dev/jx-dcanadillas-petclinic-jenkin-lg4nw-kaniko-build-3      4h
task.tekton.dev/jx-dcanadillas-petclinic-jenkin-lg4nw-kubectl-deploy-3    4h
task.tekton.dev/jx-dcanadillas-petclinic-jenkin-lg4nw-maven-build-3       4h
task.tekton.dev/jx-dcanadillas-petclinic-jx-mas-from-build-pack-1         10h
task.tekton.dev/jx-dcanadillas-petclinic-jx-mas-ndtfh-from-build-pack-2   27m

NAME                                                                               AGE
taskrun.tekton.dev/dcanadillas-kube-environment-dc-d495w-7-from-build-pack-724xr   23m
taskrun.tekton.dev/dcanadillas-kube-environment-dc-hk95j-1-from-build-pack-w8222   24m
taskrun.tekton.dev/dcanadillas-kube-environment-dc-wm85w-6-from-build-pack-ld44v   10h
taskrun.tekton.dev/dcanadillas-kube-environment-dc-wpqsp-1-from-build-pack-2txtk   10h
taskrun.tekton.dev/jx-dcanadillas-petclinic-jenkin-lg4nw-3-kaniko-build-cjkwd      4h
taskrun.tekton.dev/jx-dcanadillas-petclinic-jenkin-lg4nw-3-kubectl-deploy-kdr9x    4h
taskrun.tekton.dev/jx-dcanadillas-petclinic-jenkin-lg4nw-3-maven-build-x2cjr       4h
taskrun.tekton.dev/jx-dcanadillas-petclinic-jx-mas-1-from-build-pack-x245x         10h
taskrun.tekton.dev/jx-dcanadillas-petclinic-jx-mas-ndtfh-2-from-build-pack-9xkrq   27m

NAME                                                          AGE
pipeline.tekton.dev/dcanadillas-kube-environment-dc-d495w-7   23m
pipeline.tekton.dev/dcanadillas-kube-environment-dc-hk95j-1   24m
pipeline.tekton.dev/dcanadillas-kube-environment-dc-wm85w-6   10h
pipeline.tekton.dev/dcanadillas-kube-environment-dc-wpqsp-1   10h
pipeline.tekton.dev/jx-dcanadillas-petclinic-jenkin-lg4nw-3   4h
pipeline.tekton.dev/jx-dcanadillas-petclinic-jx-mas-1         10h
pipeline.tekton.dev/jx-dcanadillas-petclinic-jx-mas-ndtfh-2   27m

NAME                                                             AGE
pipelinerun.tekton.dev/dcanadillas-kube-environment-dc-d495w-7   23m
pipelinerun.tekton.dev/dcanadillas-kube-environment-dc-hk95j-1   24m
pipelinerun.tekton.dev/dcanadillas-kube-environment-dc-wm85w-6   10h
pipelinerun.tekton.dev/dcanadillas-kube-environment-dc-wpqsp-1   10h
pipelinerun.tekton.dev/jx-dcanadillas-petclinic-jenkin-lg4nw-3   4h
pipelinerun.tekton.dev/jx-dcanadillas-petclinic-jx-mas-1         10h
pipelinerun.tekton.dev/jx-dcanadillas-petclinic-jx-mas-ndtfh-2   27m

NAME                                                                AGE
pipelineresource.tekton.dev/dcanadillas-kube-environment-dc-d495w   23m
pipelineresource.tekton.dev/dcanadillas-kube-environment-dc-hk95j   24m
pipelineresource.tekton.dev/dcanadillas-kube-environment-dc-wm85w   10h
pipelineresource.tekton.dev/dcanadillas-kube-environment-dc-wpqsp   10h
pipelineresource.tekton.dev/jx-dcanadillas-petclinic-jenkin         4d
pipelineresource.tekton.dev/jx-dcanadillas-petclinic-jenkin-lg4nw   4h
pipelineresource.tekton.dev/jx-dcanadillas-petclinic-jenkin-rhg88   3d
pipelineresource.tekton.dev/jx-dcanadillas-petclinic-jx-mas         10h
pipelineresource.tekton.dev/jx-dcanadillas-petclinic-jx-mas-ndtfh   27m
</code></pre>

<h2 id="some-thoughts-and-final-conclusions">Some thoughts and final conclusions</h2>

<p><a href="https://jenkins.io/">Traditional Jenkins</a> has long been one of the best CI/CD pipeline engines in terms of flexibility, standardization and adoption for most DevOps environments. But new challenges for software delivery requires scalable CI/CD architectures. Today that means using Kubernetes as a powerful abstraction of the infrastructure and the use of cloud native platforms to decouple and scale CI/CD pipelines.</p>

<p>Decoupling and scaling pipelines is one of the best approaches for building new modern applications and microservices. But it can be very hard to work on these decoupled objects to define a pipeline, which conceptually is a sequential and &ldquo;monolithic&rdquo; tasks execution.</p>

<p>So, Tekton is demonstrating its power of decoupling CI/CD pipelines and builds to scale. But it also needs a powerful orchestrator to simplify the complexity underneath, and even more when dealing with a platform like Kubernetes, that can also add more complexity.</p>

<p>I like to say that the best way to build and run CI/CD pipelines for modern (and also traditional, why not!) software applications is to make it simple and &ldquo;abstract the abstraction&rdquo;. I believe Jenkins X is about that. It is about applying the simplicity of traditional Jenkins pipelines from a powerful and scalable engine.</p>

<p>Most development companies are realizing that the next iteration of CI/CD is already here and that it&rsquo;s not only about cloud native pipelines, YAML object definintions or Kubernetes containers orchestration. It is about orchestrating all the complexity while adopting best practices. We are talking about build packs templating, simple YAML pipelines, seamless Git integration, scalable team management, flexible and standard deployments, extensible platform engine, standard packaging and promotion, etc.</p>

<p>We&rsquo;ve seen from a very basic point of view in our examples what it means to orchestrate this. And &hellip; it is more than Tekton on steroids.</p>
]]></content>
        </item>
        
        <item>
            <title>Technologists Lightning Talks for DevOps World | Jenkins World 2019 San Francisco</title>
            <link>https://technologists.dev/posts/lightning-talks-dw-jw-2019/</link>
            <pubDate>Sat, 27 Jul 2019 10:50:46 -0400</pubDate>
            
            <guid>https://technologists.dev/posts/lightning-talks-dw-jw-2019/</guid>
            <description>The Technologists will be giving a bunch of lightning talks in the DevOps Theater at this years DevOps World | Jenkins World in San Francisco. Some of the topics we will be covering include (many of which we have already blogged about):
 Self-Updating Jenkins: GitOps for Jenkins Configuration  This Lightning Talk will explore using GitOps to automate config updates for the CloudBees Jenkins Distribution.  Jenkins Plugin Management as Code  Letâ€™s admit it, Jenkins Plugin management can be a pain.</description>
            <content type="html"><![CDATA[<p>The Technologists will be giving a bunch of lightning talks in the DevOps Theater at this years <a href="https://www.cloudbees.com/devops-world/san-francisco/">DevOps World | Jenkins World in San Francisco</a>. Some of the topics we will be covering include (many of which we have <a href="https://cb-technologists.github.io/posts/">already blogged about</a>):</p>

<ul>
<li><a href="https://cb-technologists.github.io/posts/cjd-casc/"><strong>Self-Updating Jenkins: GitOps for Jenkins Configuration</strong></a>

<ul>
<li>This Lightning Talk will explore using GitOps to automate config updates for the CloudBees Jenkins Distribution.</li>
</ul></li>
<li><a href="https://cb-technologists.github.io/posts/jenkins-plugins-good-bad-ugly/"><strong>Jenkins Plugin Management as Code</strong></a>

<ul>
<li>Letâ€™s admit it, Jenkins Plugin management can be a pain. In this talk we will explore using CasC for Jenkins Plugin management. We will also get a preview of some exciting improvements around Plugin Management being built by CloudBees.</li>
</ul></li>
<li><strong>Multi-Cluster/Multi-Cloud/Hybrid Cloud</strong>

<ul>
<li>A brief overview and demonstration of up and coming Kubernetes multi-cluster capabilities in CloudBees Core.</li>
</ul></li>
<li><a href="https://cb-technologists.github.io/posts/jenkins-x-flow-integration/"><strong>Traditional Deployments with Jenkins X</strong></a>

<ul>
<li>Jenkins X isnâ€™t just for Kubernetes deployments. In this talk, weâ€™ll discuss how traditional deployments to non Kubernetes environments can be accomplished using Jenkins X and the power of CloudBees Flow.</li>
</ul></li>
<li><strong>CloudBees Jenkins X Distribution Means Stability for Native Kubernetes CD</strong>

<ul>
<li>Stability has been an issue for Jenkins X as it evolves quickly. This talk will provide an overview of how the CloudBees Jenkins X Distribution provides the stability that companies expect for their CD solution.</li>
</ul></li>
<li><a href="https://cb-technologists.github.io/posts/gitops-series-part-1/"><strong>GitOps for Jenkins Infrastructure</strong></a>

<ul>
<li>Overview of using GitOps to manage your Jenkins infrastructure as code. Using some popular open source tools, we can create a process for provisioning and managing the underlying infrastructure for running Jenkins on Kubernetes.</li>
</ul></li>
<li><strong>Multi Cluster Deployments with Jenkins X</strong>

<ul>
<li>When building and deploying applications in a Kubernetes based environment, a common requirement is the ability to isolate Development, Staging, and Production clusters which may have different security policies configured.  Jenkins X now has the ability to build in your development cluster but deploy your application to a separate cluster.</li>
</ul></li>
<li><strong>Safety First with Snyk and Jenkins</strong>

<ul>
<li>Take advantage of Synkâ€™s dependency and docker vulnerability scanning in your pipelines. Fail builds for critical vulnerabilities. Go from insecure to informed in 15 minutes.</li>
</ul></li>
<li><strong>GitOps for Blogging, Why Not?</strong>

<ul>
<li>Jenkins X isnâ€™t just for deploying micro-services to Kubernetes. The Technologists leverage Jenkins X to provide a GitOps approach to reviewing and deploying their Hugo based blog site.</li>
</ul></li>
</ul>

<p>The Technologists will also always be available in the CloudBees booth. If you are headed to DevOps World | Jenkins World in San Francisco, and we hope you are, we would love to discuss any of these topics, any thing that we have blogged about on this site or any emerging technologies related to continuous delivery with you. Just stop by the CloudBees booth and ask for a CloudBees Technologist.</p>

<p>We are looking forward to talking with you in San Francisco!</p>
]]></content>
        </item>
        
        <item>
            <title>Introduction to GitOps - Part 1</title>
            <link>https://technologists.dev/posts/gitops-series-part-1/</link>
            <pubDate>Tue, 16 Jul 2019 15:00:00 -0400</pubDate>
            
            <guid>https://technologists.dev/posts/gitops-series-part-1/</guid>
            <description>GitOps is a concept that was first coined by Weaveworks in their GitOps - Operations by Pull Request post. The idea itself wasn&amp;rsquo;t anything particularly new, people had been doing automated operations with infrastructure-as-code for years. But now that there was a descriptive new name for this concept, the DevOps community has really started to embrace it. Especially with the ever growing prevalence of Kubernetes.
If you haven&amp;rsquo;t already done so, I&amp;rsquo;d recommend reading that Weaveworks post since it is always good to understand the origination of a concept.</description>
            <content type="html"><![CDATA[

<p>GitOps is a concept that was first coined by Weaveworks in their <a href="https://www.weave.works/blog/gitops-operations-by-pull-request">GitOps - Operations by Pull Request</a> post. The idea itself wasn&rsquo;t anything particularly new, people had been doing automated operations with infrastructure-as-code for years. But now that there was a descriptive new name for this concept, the DevOps community has really started to embrace it. Especially with the ever growing prevalence of Kubernetes.</p>

<p><img src="/img/gitops-series/part-1/trend.png" alt="GitOps Trend" /></p>

<p>If you haven&rsquo;t already done so, I&rsquo;d recommend reading that Weaveworks post since it is always good to understand the origination of a concept. But simply put, GitOps is a way for you to manage your operations from a source code repo. With GitOps you won&rsquo;t be doing any manual steps when you want to change something. On a commit to your master branch, a job will get kicked off and will make the necessary changes.</p>

<h2 id="why-would-you-want-this">Why would you want this?</h2>

<p>If you have any experience on an operations team with little or no automation, you no doubt know the frustration of manual configuration and snowflake servers. In this sort of environment it is easy to quickly get overwhelmed. And when things go wrong, they can spin out of control.</p>

<p>As you move to infrastructure-as-code by using configuration management tools you&rsquo;re able to get away from most of that headache. Now that you have code which describes your desired environment state you have an easy way to manage and maintain the environment. When you need to change something, submit a pull request and have someone review it. Once the change is merged, go ahead and run the automation and the change will propogate. Should something disastrous happen to your environment, you can get your environment back up in no time by rerunning the automation.</p>

<p>But you are still missing something if these commits to master don&rsquo;t automatically kick off a job to make the change. You should always want your environment to 100% match the configuration in your repo. If the automation isn&rsquo;t automatically run, you will drift away from the target state.</p>

<p>I&rsquo;ve personally been guilty of putting off running automation due to fear of something breaking, and I know I&rsquo;m not alone in this. Knowing that merging your pull request is going to trigger a job makes you more careful in your review of pull requests but also gives you confidence in knowing that your environment is always up-to-date.</p>

<h2 id="objective-of-this-series">Objective of this series</h2>

<p>To explore the idea of GitOps I am writing a 3-part series where we&rsquo;ll be building out a fully-functional GitOps process.</p>

<p>In this first part we will take a look at building out the infrastructure automation piece. This will involve provisioning a Kubernetes cluster, setting up certificates and DNS, and more. From here we will fork into two different directions.</p>

<p>In the second part we will add the automation of our <a href="https://www.cloudbees.com/products/cloudbees-jenkins-distribution">CloudBees Jenkins Distribution</a>. This will include plugin management, configuration, and more.</p>

<p>In the final part we will look at <a href="https://www.cloudbees.com/products/cloudbees-core">CloudBees Core</a> and some cool stuff we can do with custom Operations Center and Managed Master images.</p>

<p>Overall, while the goal of this series is educational, I hope it is also useful and that the assets are useable. As I write this I am using this automation daily to make my life easier as I play around with new features and try new configurations.</p>

<p>By necessity the resulting assets are based on my configuration and preferences. While I am trying to keep things as generic as possible, some things like my use of GKE might differ from your situation. The ideas and processes should be transferable to your environment.</p>

<h1 id="time-to-get-down-to-business">Time to get down to business</h1>

<p>With that background out of the way, let&rsquo;s dive right in.</p>

<h2 id="what-s-the-plan">What&rsquo;s the plan?</h2>

<p><img src="/img/gitops-series/part-1/plan.svg" alt="Plan of attack" /></p>

<ol>
<li>Pull latest changes - we&rsquo;ll leave this to Jenkins</li>
<li>Kubernetes cluster - for this I have decided to use <a href="https://www.terraform.io/">Terraform</a> to provision/manage the Kubernetes cluster. We&rsquo;ll be using <a href="https://cloud.google.com/kubernetes-engine/">(GKE) Google Kubernetes Engine</a> since it is my favorite managed Kubernetes platform</li>
<li>Namespace and permissions - will use <code>kubectl</code> to handle this</li>
<li>Ingress controller - Will use the recommended nginx ingress controller</li>
<li>Set DNS record - Will take advantage of the Ansible role to do this easily</li>
<li>Cert manager - Will use Kubectl to install</li>
<li>Ensure CJD/Core are running - This is where we will fork into the next 2 posts</li>
</ol>

<h3 id="already-there-is-an-issue">Already there is an issue</h3>

<p>In order to have a GitOps process that works, we need to have something to actually kick off the jobs. In this case we&rsquo;re going to be using Jenkins, but we don&rsquo;t have it up yet.</p>

<p>There are a couple of ways we could handle this:</p>

<ol>
<li>Have a Jenkins server running outside of the Kubernetes cluster (where is the fun in that?)</li>
<li>Create a seed script which will run everything the first time and setup the Jenkins server on the cluster we just created</li>
</ol>

<p>Since I am trying to minimize the number of things we need to manage, we&rsquo;ll be going with #2.</p>

<h2 id="creating-the-repo">Creating the repo</h2>

<p>In order to keep the separations of concerns pretty straightforward, I&rsquo;ve got the structure of the repo looking like this:</p>

<pre><code>.
â”œâ”€â”€ ansible &lt;- Our ansible playbook will live here
â”œâ”€â”€ cert-manager &lt;- The cert manager configuration lives here
â”œâ”€â”€ scripts &lt;- All other scripts (including our seed script) live here
â””â”€â”€ terraform &lt;- The terraform configuration lives here
</code></pre>

<h2 id="provisioning-the-kubernetes-cluster">Provisioning the Kubernetes cluster</h2>

<p>The first obvious step in building out our project is to be able to spin up the Kubernetes cluster, since after all, that is the platform everything will be running on. For this task I&rsquo;ve chosen to use Terraform for it&rsquo;s quick and easy way to provision cloud resources in an <a href="https://en.wikipedia.org/wiki/Idempotence">idempotent</a> fashion.</p>

<p>Specifically we&rsquo;ll use the Google Kubernetes Engine (GKE) <a href="https://www.terraform.io/docs/providers/google/r/container_cluster.html">provisioner</a>.</p>

<p>If you don&rsquo;t have any experience with Terraform, they have a good <a href="https://learn.hashicorp.com/terraform/">learning site</a> where you can get started. The scope of what we&rsquo;ll be doing is rather limited, so if you don&rsquo;t have any prior experience don&rsquo;t worry, it should be simple enough to follow and understand.</p>

<p>At a minimum you will want to have Terraform <a href="https://learn.hashicorp.com/terraform/getting-started/install">installed locally</a>.</p>

<h3 id="variables-file">Variables file</h3>

<p>When using Terraform I like to split out all of the variables into a separate variables file. This makes it easier when making changes to see all settings at once.</p>

<p>Inside of our <code>terraform/</code> directory will start by creating a <code>variables.tf</code> file.</p>

<p>In Terraform, a variable looks like this:</p>

<pre><code class="language-terraform">variable &quot;cluster_name&quot; {
  default = &quot;ld-cluster-1&quot;
}
</code></pre>

<p>This can then be referenced like this: <code>&quot;${var.cluster_name}&quot;</code></p>

<p>The <code>terraform/variables.tf</code> file is going to look like this:</p>

<pre><code class="language-terraform">variable &quot;project&quot; {
  default = &quot;myproject&quot;
}

variable &quot;region&quot; {
  default = &quot;us-east1-b&quot;
}

variable &quot;cluster_name&quot; {
  default = &quot;my-cluster-name&quot;
}

variable &quot;cluster_zone&quot; {
  default = &quot;us-east1-b&quot;
}

variable &quot;cluster_k8s_version&quot; {
  default = &quot;1.13.6-gke.13&quot;
}

variable &quot;initial_node_count&quot; {
  default = 1
}

variable &quot;autoscaling_min_node_count&quot; {
  default = 1
}

variable &quot;autoscaling_max_node_count&quot; {
  default = 5
}

variable &quot;disk_size_gb&quot; {
  default = 100
}

variable &quot;disk_type&quot; {
  default = &quot;pd-standard&quot;
}

variable &quot;machine_type&quot; {
  default = &quot;n1-standard-2&quot;
}
</code></pre>

<p>Since this is where we are setting the environment specific variables, go ahead and replace those with your own desired state. You&rsquo;ll most likely want to adjust <code>project</code>, <code>region</code>, <code>cluster_zone</code>, and <code>cluster_name</code>. There is also a chance that as you read this the <code>cluster_k8s_version</code> I have listed here is no longer available, so you may need to update that.</p>

<h3 id="cluster-definition-file">Cluster definition file</h3>

<p>Now with the variables out of the way, it&rsquo;s time to build out the actual definition of what the cluster is going to look like. This is the stuff that isn&rsquo;t likely to change as much. If you&rsquo;re following along you shouldn&rsquo;t need to make any changes except for one specific spot I&rsquo;ll point out.</p>

<p>We&rsquo;re going to create a <code>cluster.tf</code> file.</p>

<p>It&rsquo;s going to look like this: <code>terraform/cluster.tf</code></p>

<pre><code class="language-terraform">provider &quot;google&quot; {
  project = &quot;${var.project}&quot;
  region  = &quot;${var.region}&quot;
}

# Change this section
terraform {
  backend &quot;gcs&quot; {
    bucket  = &quot;my-unique-bucket&quot;
    prefix  = &quot;terraform/state&quot;
    project = &quot;my-project&quot;
  }
}

resource &quot;google_container_cluster&quot; &quot;cluster&quot; {
  name               = &quot;${var.cluster_name}&quot;
  location           = &quot;${var.cluster_zone}&quot;
  min_master_version = &quot;${var.cluster_k8s_version}&quot;

  addons_config {
    network_policy_config {
      disabled = true
    }

    http_load_balancing {
      disabled = false
    }

    kubernetes_dashboard {
      disabled = false
    }
  }

  node_pool {
    name               = &quot;default-pool&quot;
    initial_node_count = &quot;${var.initial_node_count}&quot;

    management {
      auto_repair = true
    }

    autoscaling {
      min_node_count = &quot;${var.autoscaling_min_node_count}&quot;
      max_node_count = &quot;${var.autoscaling_max_node_count}&quot;
    }

    node_config {
      preemptible  = false
      disk_size_gb = &quot;${var.disk_size_gb}&quot;
      disk_type    = &quot;${var.disk_type}&quot;

      machine_type = &quot;${var.machine_type}&quot;

      oauth_scopes = [
        &quot;https://www.googleapis.com/auth/devstorage.read_only&quot;,
        &quot;https://www.googleapis.com/auth/logging.write&quot;,
        &quot;https://www.googleapis.com/auth/monitoring&quot;,
        &quot;https://www.googleapis.com/auth/service.management.readonly&quot;,
        &quot;https://www.googleapis.com/auth/servicecontrol&quot;,
        &quot;https://www.googleapis.com/auth/trace.append&quot;,
        &quot;https://www.googleapis.com/auth/compute&quot;,
        &quot;https://www.googleapis.com/auth/cloud-platform&quot;
      ]

    }
  }
}

output &quot;client_certificate&quot; {
  value     = &quot;${google_container_cluster.cluster.master_auth.0.client_certificate}&quot;
  sensitive = true
}

output &quot;client_key&quot; {
  value     = &quot;${google_container_cluster.cluster.master_auth.0.client_key}&quot;
  sensitive = true
}

output &quot;cluster_ca_certificate&quot; {
  value     = &quot;${google_container_cluster.cluster.master_auth.0.cluster_ca_certificate}&quot;
  sensitive = true
}

output &quot;host&quot; {
  value     = &quot;${google_container_cluster.cluster.endpoint}&quot;
  sensitive = true
}
</code></pre>

<p>This may look complicated, but really all we are doing is defining the configuration of the cluster we want to provision. As you can see we are taking full use of the variables we listed in the <code>variables.tf</code> file.</p>

<p>There is one section you will need to modify, it is the following block:</p>

<pre><code class="language-terraform">terraform {
  backend &quot;gcs&quot; {
    bucket  = &quot;my-unique-bucket&quot;
    prefix  = &quot;terraform/state&quot;
    project = &quot;my-project&quot;
  }
}
</code></pre>

<p>By default, when you are using Terraform it stores the state of your environment to the local system. Since we are going to be running this from Jenkins in an ephemeral agent, we don&rsquo;t want this. Instead, this block tells Terraform to store the state to a GCS storage bucket so the state will persist between runs.</p>

<p>If you&rsquo;re following along, you can follow these <a href="https://cloud.google.com/storage/docs/creating-buckets">instructions</a> to create a GCS bucket here.</p>

<h3 id="testing-it-out">Testing it out</h3>

<p>With this configuration all set, we are ready to test it out and see if we can provision a GKE cluster using terraform.</p>

<p>If you <code>cd terraform</code> to change to that directory, you can initialize the Terraform project and pull the requisite plugins by running <code>terraform init</code>. You can then run <code>terraform plan</code> to see the plan that gets generated by Terraform.</p>

<p>If all looks good, you can go ahead and run <code>terraform apply</code>. Unless you specify a specific flag, it is going to prompt you whether you want to perform the actions or not.</p>

<p>Go ahead and type <code>yes</code> when you&rsquo;re ready, then the provisioning process will begin. This should take a few minutes to complete since it has to spin up and configure quite a few resources.</p>

<p>Once the cluster is up and ready to go, we can move on to the next steps.</p>

<h2 id="setting-up-namespace-and-permissions">Setting up namespace and permissions</h2>

<p>These steps we&rsquo;re performing will be put into a script since automation is our goal, but I&rsquo;m going to run through them manually the first time so we can understand what is going on.</p>

<p>First we need to connect to the Kubernetes cluster we created. This is easiest done by running (with your specific parameters):</p>

<p><code>gcloud container clusters get-credentials MYCLUSTER --zone MYZONE --project MYPROJECT</code></p>

<p>You can verify that you&rsquo;re connected by running a kubectl command like <code>kubectl get nodes</code>.</p>

<h3 id="assigning-cluster-admin-role">Assigning cluster-admin role</h3>

<p>Certain components of our setup will need cluster-admin role access so we can easily set that up by running:</p>

<p><code>kubectl create clusterrolebinding cluster-admin-binding  --clusterrole cluster-admin  --user $(gcloud config get-value account)</code></p>

<h3 id="create-the-namespaces">Create the namespaces</h3>

<p>Next we will want to create a namespace for Core or CJD to live in.</p>

<pre><code class="language-bash">kubectl create namespace core
kubectl label namespace core name=core
kubectl config set-context $(kubectl config current-context) --namespace=core
</code></pre>

<p>If you&rsquo;re familiar with kubectl you might be aware that we are going to get an error on subsequent runs of the <code>kubectl create</code> command since it will already exist. We will need to take care of that as part of the Jenkinsfile.</p>

<h2 id="setup-the-ingress-controller">Setup the ingress controller</h2>

<p>In order to get traffic into an application running in Kubernetes we will need to create ingresses for each application. It turns out that manually doing this is a bit of a pain, so the Kubernetes community created the <a href="https://kubernetes.github.io/ingress-nginx/">NGINX Ingress Controller</a> which will do most of the work for us.</p>

<p>There are several different ways to install this, including a simple helm install, all of which can be found <a href="https://kubernetes.github.io/ingress-nginx/deploy/">here</a>.</p>

<p>To avoid having to manage anything else (i.e. helm), I&rsquo;ve opted to just use the yaml file install.</p>

<pre><code class="language-bash">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.24.1/deploy/mandatory.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.24.1/deploy/provider/cloud-generic.yaml
</code></pre>

<p>An important note about this is that it takes several seconds to provision and attach the public ip address to the service. We will need to handle this in the Jenkinsfile.</p>

<h2 id="set-dns-record">Set DNS Record</h2>

<p>Now that we have a public ip address, we can point our domain at it. This is one of those problems that you can tackle 100 different ways. The simplest way is probably to make an api call to your DNS host to update a particular record with the ip address.</p>

<p>I&rsquo;m going to make it a little more complicated in order to make it easier to switch between different DNS hosts.</p>

<p>I&rsquo;ve setup an <a href="https://github.com/ansible/ansible">Ansible</a> playbook which takes advantage of the pre-built DNS provider modules.</p>

<p>Here are the <a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html">Ansible install instructions</a>. If you&rsquo;ve got <code>pip</code> on your system you can simply run <code>pip install ansible</code>.</p>

<p>I created a file <code>ansible/dns.yml</code> which contains the following:</p>

<pre><code class="language-yaml">- hosts: localhost
  tasks:
  - name: Create a managed zone
    gcp_dns_managed_zone:
      name: &quot;my-zone-name&quot;
      dns_name: &quot;my-domain-name.com&quot;
      description: My playground
      project: my-project
      state: present
      auth_kind: serviceaccount
    register: managed_zone

  - name:  Create an A record to point to the Core instance
    gcp_dns_resource_record_set:
      managed_zone: &quot;{{ managed_zone }}&quot;
      name: &quot;core.my-domain-name.com.&quot;
      type: A
      target: 
        - &quot;{{ target_ip }}&quot;
      project: my-project
      auth_kind: serviceaccount
</code></pre>

<p>What we are doing here is taking advantage of the gcp_dns modules (<a href="https://docs.ansible.com/ansible/latest/modules/gcp_dns_managed_zone_module.html"><code>gcp_dns_managed_zone</code></a> &amp; <a href="https://docs.ansible.com/ansible/latest/modules/gcp_dns_resource_record_set_module.html"><code>gcp_dns_resource_record_set</code></a>) to easily set the DNS.</p>

<p>The nice thing about this is should you need to use another DNS host like <a href="https://www.cloudflare.com/">Cloudflare</a> you can easily transition over using the right <a href="https://docs.ansible.com/ansible/latest/modules/cloudflare_dns_module.html#cloudflare-dns-module">module</a>.</p>

<p>Once that is configured, you can run the playbook with (setting the TARGET_IP according to your cluster&rsquo;s ip):</p>

<p><code>ansible-playbook ansible/dns.yml -e target_ip=${TARGET_IP}</code></p>

<h2 id="setting-up-cert-manager">Setting up Cert Manager</h2>

<p>Now we have our ingress controller which has given us a public ip address and we have setup a DNS record to point to the address. We could go ahead and install Core or CJD at this point if we wanted, but we might as well setup SSL certificates to make things more secure.</p>

<p>We&rsquo;re going to use <a href="https://letsencrypt.org/">Let&rsquo;s Encrypt</a> Certificate Authority in order to generate the certs. To do this in an easy and automated fashion, we&rsquo;ll use <a href="https://github.com/jetstack/cert-manager">cert-manager</a>.</p>

<p>Installing it is pretty easy, and you can find the most update instructions <a href="https://docs.cert-manager.io/en/latest/getting-started/install/kubernetes.html">here</a>.</p>

<pre><code class="language-bash">kubectl create namespace cert-manager
kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true
kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.8.0/cert-manager.yaml
</code></pre>

<p>The above commands create a namespace for cert-manager and then deploy the cert-manager application in there.</p>

<p>Next we&rsquo;ll need to create some cert issuers to allow us to actually generate the certs. We&rsquo;ll create two of them in the cert-manager directory.</p>

<p><code>cert-manager/staging-issuer.yaml</code>:</p>

<pre><code class="language-yaml">   apiVersion: certmanager.k8s.io/v1alpha1
   kind: Issuer
   metadata:
     name: letsencrypt-staging
   spec:
     acme:
       # The ACME server URL
       server: https://acme-staging-v02.api.letsencrypt.org/directory
       # Email address used for ACME registration
       email: myemail@example.com # change this
       # Name of a secret used to store the ACME account private key
       privateKeySecretRef:
         name: letsencrypt-staging
       # Enable the HTTP-01 challenge provider
       http01: {}
</code></pre>

<p><code>cert-manager/production-issuer.yaml</code>:</p>

<pre><code class="language-yaml">   apiVersion: certmanager.k8s.io/v1alpha1
   kind: Issuer
   metadata:
     name: letsencrypt-prod
   spec:
     acme:
       # The ACME server URL
       server: https://acme-v02.api.letsencrypt.org/directory
       # Email address used for ACME registration
       email: myemail@example.com
       # Name of a secret used to store the ACME account private key
       privateKeySecretRef:
         name: letsencrypt-prod
       # Enable the HTTP-01 challenge provider
       http01: {}
</code></pre>

<p>The reason we have two of these is because Let&rsquo;s Encrypt has a rate-limiter on how often you can generate certificates. So while you are experimenting with things, it is safer to use the staging issuer. When things are all sorted, you can switch to the production-issuer.</p>

<p>Go ahead and apply these two issuers with:</p>

<pre><code class="language-bash">kubectl apply -f cert-manager/staging-issuer.yaml
kubectl apply -f cert-manager/production-issuer.yaml
</code></pre>

<p>Now when we create an ingress for our applications, we can add some metadata to the ingress definition and the certificates will automatically be generated and stored as <a href="https://kubernetes.io/docs/concepts/configuration/secret/">K8s Secrets</a>.</p>

<h2 id="setting-up-cloudbees-core-or-cjd">Setting up CloudBees Core or CJD</h2>

<p>The final step in this flow is to either setup Core or CJD. We will add this portion, and more in the following posts.</p>

<p>But before concluding this post, let&rsquo;s take a look at how we can put all of these steps into a Jenkinsfile.</p>

<h2 id="putting-it-all-together">Putting it all together.</h2>

<p>Since the whole objective here was to automate this process, it makes sense to use Jenkins to run the process. We can easily achieve GitOps by having every commit to master kick off our pipeline here.</p>

<p>You&rsquo;ll note that we&rsquo;ve added a couple of dependencies along the way that we&rsquo;ll need to make sure Jenkins will have access to. Thankfully, since we&rsquo;ll be running on Kubernetes, we can take advantage of the ephemeral, container-based agents. We can define a <a href="https://jenkins.io/doc/pipeline/steps/kubernetes/">pod template</a> which will describe all of the containers we will need.</p>

<p>In the root directory of my repo, I have created a <code>pod-template.yml</code> file:</p>

<pre><code class="language-yaml">kind: Pod
metadata:
  name: gitops-pod
spec:
  containers:
  - name: terraform
    image: hashicorp/terraform:light
    command:
    - cat
    tty: true
    volumeMounts:
      - name: gcp-credential
        mountPath: /root/
    env:
      - name: GOOGLE_CLOUD_KEYFILE_JSON
        value: &quot;/root/gcp-service.json&quot;
      - name: GCP_SERVICE_ACCOUNT_FILE
        value: &quot;/root/gcp-service.json&quot;
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: &quot;/root/gcp-service.json&quot;
  - name: ansible
    image: ldonleycb/ansible-ci:new
    command:
    - cat
    tty: true
    volumeMounts:
      - name: gcp-credential
        mountPath: /root/
    env:
      - name: GOOGLE_CLOUD_KEYFILE_JSON
        value: &quot;/root/gcp-service.json&quot;
      - name: GCP_SERVICE_ACCOUNT_FILE
        value: &quot;/root/gcp-service.json&quot;
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: &quot;/root/gcp-service.json&quot;
      - name: GCP_PROJECT
        value: &quot;my_project&quot;
      - name: GCP_CLUSTER_NAME
        value: &quot;my_cluster_name&quot;
  - name: kubectl
    image: google/cloud-sdk:252.0.0-slim
    command:
    - cat
    tty: true
    volumeMounts:
      - name: gcp-credential
        mountPath: /home/
    env:
      - name: GOOGLE_CLOUD_KEYFILE_JSON
        value: &quot;/home/gcp-service.json&quot;
      - name: GCP_SERVICE_ACCOUNT_FILE
        value: &quot;/home/gcp-service.json&quot;
      - name: GOOGLE_APPLICATION_CREDENTIALS
        value: &quot;/home/gcp-service.json&quot;
      - name: GCP_PROJECT
        value: &quot;my_project&quot;
      - name: GCP_CLUSTER_NAME
        value: &quot;my_cluster_name&quot;
  volumes:
    - name: gcp-credential
      secret:
        secretName: gcp-credential
</code></pre>

<p>This looks complicated, but it is mostly just bloated by the array of environment variables we need for Google Cloud operations.</p>

<p>The <code>Jenkinsfile</code> in the root directory will look something like this:</p>

<pre><code class="language-groovy">pipeline {
  agent {
    kubernetes {
      label 'gitops'
      yamlFile 'pod-template.yml'
    }
  }
  stages {
    stage('Terraform') {
      steps {
        container('terraform'){
          sh '''
            cd terraform
            terraform init
            terraform apply -input=false -auto-approve
            cd ..
          '''
        }
      }
    }
    stage('Setup ingress controller and namespace') {
      steps {
        container('kubectl'){
          script {
            sh '''
              gcloud auth activate-service-account --key-file=$GCP_SERVICE_ACCOUNT_FILE
              gcloud container clusters get-credentials $GCP_CLUSTER_NAME --zone us-east1-b --project $GCP_PROJECT
            '''
            try {
              sh '''
                kubectl create clusterrolebinding cluster-admin-binding  --clusterrole cluster-admin  --user $(gcloud config get-value account)
              '''
            }
            catch(error) {
              sh &quot;echo ''&quot;
            }

            sh '''
              kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.24.1/deploy/mandatory.yaml
              kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.24.1/deploy/provider/cloud-generic.yaml
              sleep 60s
            '''
            try {
              sh '''
                kubectl create namespace core
                kubectl label namespace core name=core
              '''
            }
            catch(error) {
              sh &quot;echo ''&quot;
            }
            sh '''
              kubectl config set-context $(kubectl config current-context) --namespace=core
            '''
            env.TARGET_IP = sh(returnStdout: true, script: 'kubectl get service ingress-nginx -n ingress-nginx | awk \'END {print $4}\'').trim()
          } 
        }
      }
    }
    stage('Setup DNS') {
      steps {
        container('ansible'){
          sh &quot;&quot;&quot;
            ansible-playbook ansible/dns.yml -e target_ip=${env.TARGET_IP}
          &quot;&quot;&quot;
        }

      }
    }
    stage('Setup cert-manager') {
      steps {
        container('kubectl'){
          sh '''# Install cert-manager
              kubectl create namespace cert-manager
              kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true
              kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v0.8.0/cert-manager.yaml

              sleep 30s

              # Add cert-manager issuers
              kubectl apply -f cert-manager/staging-issuer.yaml
              kubectl apply -f cert-manager/production-issuer.yaml
            '''
        }
      }
    }
  }
}
</code></pre>

<p>This is not a particularly elegant solution at this point, but for an initial attempt it should be sufficient.</p>

<p>In the next parts of this series we will be taking a look at how to extend this to actually deploy and maintain CloudBees Core or CloudBees Jenkins Distribution.</p>
]]></content>
        </item>
        
        <item>
            <title>Self-Updating Jenkins: GitOps for Jenkins Configuration</title>
            <link>https://technologists.dev/posts/cjd-casc/</link>
            <pubDate>Wed, 03 Jul 2019 17:00:00 -0400</pubDate>
            
            <guid>https://technologists.dev/posts/cjd-casc/</guid>
            <description>In this blog post, we&amp;rsquo;ll walk through creating a self-updating instance of the CloudBees Jenkins Distribution, with all configuration stored as code in a GitHub repository.
We&amp;rsquo;ll deploy the CJD master as a StatefulSet in a Kubernetes cluster, configure the master using the Jenkins Configuration as Code plugin, and set up a TLS certificate through cert-manager. Finally, we&amp;rsquo;ll seed a Pipeline job that updates the master upon commit to the Git repository that contains the configuration - enabling GitOps for Jenkins itself.</description>
            <content type="html"><![CDATA[

<p>In this blog post, we&rsquo;ll walk through creating a self-updating instance of the <a href="https://www.cloudbees.com/products/cloudbees-jenkins-distribution">CloudBees Jenkins Distribution</a>, with all configuration stored as code in a GitHub repository.</p>

<p>We&rsquo;ll deploy the CJD master as a <code>StatefulSet</code> in a Kubernetes cluster, configure the master using the <a href="https://github.com/jenkinsci/configuration-as-code-plugin">Jenkins Configuration as Code plugin</a>, and set up a TLS certificate through <a href="https://github.com/jetstack/cert-manager">cert-manager</a>. Finally, we&rsquo;ll seed a Pipeline job that updates the master upon commit to the <a href="https://github.com/cb-technologists/cjd-casc">Git repository</a> that contains the configuration - enabling GitOps for Jenkins itself.</p>

<table>
<thead>
<tr>
<th>UPD (Sep 12, 2019): Jenkins Configuration as Code plugin is now supported in <a href="https://www.cloudbees.com/products/cloudbees-jenkins-distribution">CloudBees Jenkins Distribution</a> and <a href="https://www.cloudbees.com/products/cloudbees-jenkins-support">CloudBees Jenkins Support</a>. See <a href="https://go.cloudbees.com/docs/cloudbees-jenkins-distribution/distro-admin-guide/configuration-as-code/">Administering CJD: Configuration as Code</a> for usage guidelines and quick start. You can also find an official demo <a href="https://github.com/cloudbees-oss/cjd-jcasc-demo">here</a>. For information about other CloudBees products, please see <a href="https://support.cloudbees.com/hc/en-us/articles/360031191471-State-of-Jenkins-Configuration-as-Code-JCasC-support-in-CloudBees-products">this page</a>.</th>
</tr>
</thead>

<tbody>
</tbody>
</table>

<h2 id="deploying-cloudbees-jenkins-distribution-in-kubernetes">Deploying CloudBees Jenkins Distribution in Kubernetes</h2>

<p>First, we&rsquo;ll need to deploy a Jenkins instance into a Kubernetes cluster. In this case, we&rsquo;ll use <a href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a> to deploy a containerized version of CJD. To provision a cluster, we&rsquo;ll follow the Google Cloud documentation <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-cluster">here</a>. (<strong>Note:</strong> this blog post assumes prior installation of and basic familiarity with using <code>kubectl</code> to interact with a Kubernetes cluster.)</p>

<p>Once the cluster has been provisioned and <code>kubectl</code> has been configured, we&rsquo;ll create a dedicated <code>namespace</code> for our CJD resources and update our <code>kubectl config</code> to use it by default:</p>

<pre><code class="language-bash">kubectl create namespace cjd
kubectl config set-context $(kubectl config current-context) --namespace cjd
</code></pre>

<p>We&rsquo;ll also need to ensure an ingress controller is deployed within the cluster. For this post, we&rsquo;ll assume the use of the <a href="https://kubernetes.github.io/ingress-nginx/">NGINX ingress controller</a>. Following the <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/deploy/index.md">Installation Guide</a>, we&rsquo;ll manually deploy using a few <code>kubectl</code> commands:</p>

<pre><code class="language-bash"># grant cluster-admin to user
kubectl create clusterrolebinding cluster-admin-binding \ --clusterrole cluster-admin \ --user $(gcloud config get-value account)
# deploy nginx ingress controller resources
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud-generic.yaml
</code></pre>

<p>Next, let&rsquo;s look at the manifest file that will deploy the necessary resources for CJD using the <a href="https://github.com/cb-technologists/cjd-casc/blob/master/cjd.yaml">cjd.yaml</a> manifest file.</p>

<p>First, we create a <code>ServiceAccount</code>, a <code>Role</code> with the necessary permissions to manage agents and perform the required update actions, and a <code>RoleBinding</code> to connect the two.</p>

<pre><code class="language-yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  name: cjd

---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: cjd
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;pods&quot;,&quot;configmaps&quot;,&quot;services&quot;,&quot;serviceaccounts&quot;]
  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]
- apiGroups: [&quot;&quot;]
  resources: [&quot;pods/exec&quot;]
  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]
- apiGroups: [&quot;&quot;]
  resources: [&quot;pods/log&quot;]
  verbs: [&quot;get&quot;,&quot;list&quot;,&quot;watch&quot;]
- apiGroups: [&quot;&quot;]
  resources: [&quot;secrets&quot;]
  verbs: [&quot;get&quot;]
- apiGroups: [&quot;apps&quot;]
  resources: [&quot;statefulsets&quot;]
  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]
- apiGroups: [&quot;rbac.authorization.k8s.io&quot;]
  resources: [&quot;roles&quot;,&quot;rolebindings&quot;]
  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]
- apiGroups: [&quot;extensions&quot;]
  resources: [&quot;ingresses&quot;]
  verbs: [&quot;create&quot;,&quot;delete&quot;,&quot;get&quot;,&quot;list&quot;,&quot;patch&quot;,&quot;update&quot;,&quot;watch&quot;]

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: cjd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cjd
subjects:
- kind: ServiceAccount
  name: cjd
</code></pre>

<p>Next, we create a <code>Service</code> that exposes ports for access to the CJD web interface and for master-agent communication:</p>

<pre><code class="language-yaml">---
apiVersion: v1
kind: Service
metadata:
  name: cjd
spec:
  selector:
    app: cjd
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  - name: agent
    port: 50000
    protocol: TCP
    targetPort: 50000
  type: ClusterIP
</code></pre>

<p>Next, we set up an <code>Ingress</code> to allow access to our CJD instance from outside of the cluster. We&rsquo;ll examine this in more detail in a later section where we walk through the setup of <code>cert-manager</code>.</p>

<pre><code class="language-yaml">---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: cjd
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    certmanager.k8s.io/issuer: &quot;letsencrypt-prod&quot; # add after cert-manager deploy
    certmanager.k8s.io/acme-challenge-type: http01 # add after cert-manager deploy
spec:
  tls: # cert-manager
  - hosts: # cert-manager
    - cjd.cloudbees.elgin.io # cert-manager
    secretName: cjd-tls # cert-manager
  rules:
  - host: cjd.cloudbees.elgin.io
    http:
      paths:
      - path: /
        backend:
          serviceName: cjd
          servicePort: 80
</code></pre>

<p>We&rsquo;ll also need to make sure that we create a DNS A Record through our hosting provider that maps our <code>host</code> URL to the <code>EXTERNAL-IP</code> of our ingress controller. We can get that IP after deploying our NGINX ingress controller by running:</p>

<pre><code class="language-bash">kubectl get svc -n ingress-nginx
</code></pre>

<p>Finally, we provision the <code>StatefulSet</code> that controls the CJD Pod and <code>PersistentVolumeClaim</code>. The container image we use here is a custom image inheriting from the <a href="https://hub.docker.com/r/cloudbees/cloudbees-jenkins-distribution/">official CJD Docker image</a>. We&rsquo;ll examine the <code>Dockerfile</code> for this image in the next section, when we detail the configuration.</p>

<p>Additionally, you&rsquo;ll notice the creation of a few <code>secretRef</code> environment variables, as well as the setting of the <code>CASC_JENKINS_CONFIG</code> environment variable and the mounting of a <code>jenkins-casc</code> <code>ConfigMap</code> - these again will be expanded upon in the configuration section.</p>

<pre><code class="language-yaml">---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cjd
spec:
  selector:
   matchLabels:
     app: cjd
  serviceName: &quot;cjd&quot;
  template:
    metadata:
      labels:
        app: cjd
    spec:
      containers:
      - name: cjd
        image: gcr.io/melgin/cjd-casc:d176f38b289d0437a2503c83af473f57b25a4d26
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
        - containerPort: 50000
        env:
        - name: CASC_JENKINS_CONFIG
          value: /var/jenkins_config/jenkins-casc.yaml
        envFrom:
          - secretRef:
              name: github
          - secretRef:
              name: url
          - secretRef:
              name: github-oauth
        volumeMounts:
        - name: jenkins-home
          mountPath: /var/jenkins_home/
        - name: jenkins-casc
          mountPath: /var/jenkins_config/
      securityContext:
        fsGroup: 1000
      serviceAccountName: cjd
      volumes:
      - name: jenkins-casc
        configMap:
          name: jenkins-casc
  volumeClaimTemplates:
  - metadata:
      name: jenkins-home
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      resources:
        requests:
          storage: 5Gi
</code></pre>

<h2 id="configuring-with-jenkins-configuration-as-code-plugin">Configuring with Jenkins Configuration-as-Code Plugin</h2>

<p>With the YAML for the required CJD Kubernetes resources laid out, we&rsquo;ll now go into the code handling the configuration of the master. While detailing the <code>StatefulSet</code> above, we mentioned that a custom Docker image is used for the CJD container. The <code>Dockerfile</code> for this image can be found below:</p>

<pre><code class="language-Dockerfile">FROM cloudbees/cloudbees-jenkins-distribution:2.164.3.2

LABEL maintainer &quot;melgin@cloudbees.com&quot;

ENV JAVA_OPTS=&quot;-Djenkins.install.runSetupWizard=false&quot;

USER root

RUN echo 2.0 &gt; /usr/share/jenkins/ref/jenkins.install.UpgradeWizard.state

ENV TZ=&quot;/usr/share/zoneinfo/America/New_York&quot;

ENV JENKINS_UC https://jenkins-updates.cloudbees.com
# add environment variable to point to configuration file
ENV CASC_JENKINS_CONFIG /usr/jenkins_config/jenkins-casc.yaml

# Install plugins
ADD https://raw.githubusercontent.com/jenkinsci/docker/master/install-plugins.sh /usr/local/bin/install-plugins.sh
RUN chmod 755 /usr/local/bin/install-plugins.sh
ADD https://raw.githubusercontent.com/jenkinsci/docker/master/jenkins-support /usr/local/bin/jenkins-support
RUN chmod 755 /usr/local/bin/jenkins-support
COPY plugins.txt /usr/share/jenkins/ref/plugins.txt
RUN bash /usr/local/bin/install-plugins.sh &lt; /usr/share/jenkins/ref/plugins.txt

USER jenkins
</code></pre>

<p>In this <code>Dockerfile</code>, we add custom configuration to the official CJD Docker image. We first set the <code>JENKINS_UC</code> environment variable to use the CloudBees update center, as well as the <code>CASC_JENKINS_CONFIG</code> variable to point to the location we&rsquo;ll mount our configuration file. Finally, we leverage the <a href="https://github.com/jenkinsci/docker#preinstalling-plugins">Jenkins Docker <code>install-plugins.sh</code> script</a> to install a list of plugins from our <code>plugins.txt</code> file. These plugins include:</p>

<pre><code class="language-txt">configuration-as-code:1.20
job-dsl:1.74
kubernetes:1.14.9
kubernetes-credentials:0.4.0
credentials:2.2.0
workflow-multibranch:2.20
github-branch-source:2.4.5
workflow-aggregator:2.5
blueocean:1.10.2
github-oauth:0.32
</code></pre>

<p>This will handle the initial installation of the plugins we need, including resolving any dependencies.</p>

<p>Next, we&rsquo;ll need to use the Configuration as Code plugin to handle the configuration of the master itself. To do so, we&rsquo;ll mount the configuration YAML as a <code>ConfigMap</code> that our CJD <code>StatefulSet</code> will use. Here&rsquo;s what our <code>jenkinsCasc.yaml</code> file looks like:</p>

<pre><code class="language-yaml">---
apiVersion: v1
kind: ConfigMap
metadata:
  name: jenkins-casc
data:
  jenkins-casc.yaml: |
    jenkins:
      agentProtocols:
      - &quot;Diagnostic-Ping&quot;
      - &quot;JNLP4-connect&quot;
      - &quot;Ping&quot;
      crumbIssuer:
        standard:
          excludeClientIPFromCrumb: false
      securityRealm:
        github:
          githubWebUri: &quot;https://github.com&quot;
          githubApiUri: &quot;https://api.github.com&quot;
          clientID: &quot;${CLIENT_ID}&quot;
          clientSecret: &quot;${CLIENT_SECRET}&quot;
          oauthScopes: &quot;read:org,user:email&quot;
      systemMessage: &quot;CJD in Kubernetes configured as code!&quot;
      clouds:
      - kubernetes:
          name: kubernetes
          jenkinsUrl: http://cjd
          containerCapStr: 100
      authorizationStrategy:
        loggedInUsersCanDoAnything:
          allowAnonymousRead: false
    credentials:
      system:
        domainCredentials:
          - credentials:
            - usernamePassword:
                scope: GLOBAL
                id: &quot;github&quot;
                description: &quot;GitHub API token&quot;
                username: ${username}
                password: ${token}
    jobs:
    - script: &gt;
        multibranchPipelineJob('cjd-casc') {
          branchSources {
            github {
              scanCredentialsId('github')
              repoOwner('cb-technologists')
              repository('cjd-casc')
            }
          }
          orphanedItemStrategy {
            discardOldItems {
              numToKeep(5)
            }
          }
        }
    security:
      remotingCLI:
        enabled: false
    unclassified:
      location:
        adminAddress: &quot;address not configured yet &lt;nobody@nowhere&gt;&quot;
        url: &quot;https://cjd.cloudbees.elgin.io/&quot;
</code></pre>

<p>This config file sets up a handful of basic Jenkins settings like allowed agent protocols, security settings, and an example system message.</p>

<p>Three config items in particular are worth additional exploration. First, the security realm is set to use a GitHub organization for authentication (see <a href="https://wiki.jenkins.io/display/JENKINS/GitHub+OAuth+Plugin">the Jenkins GitHub OAuth Plugin page</a> for details on setting up a GitHub OAuth application). To avoid hardcoding our Client ID and Client Secret in our GitHub repository, we take advantage of Kubernetes <code>Secrets</code>.</p>

<p>Recall from our <code>StatefulSet</code> above that we load a few environment variables from <code>Secrets</code>. These include our GitHub OAuth application ID &amp; secret, as well as the username and API token used by our Pipeline job to communicate with our repository.</p>

<p>To create these, we use the following <code>kubectl</code> commands (replacing the placeholder variables with the actual credentials):</p>

<pre><code class="language-bash">kubectl create secret generic github-oauth --from-literal=CLIENT_ID=${CLIENT_ID} --from-literal=CLIENT_SECRET=${CLIENT_SECRET}

kubectl create secret generic github --from-literal=username=${USERNAME} --from-literal=token=${TOKEN}
</code></pre>

<p>The second config item to note is the creation of a simple Kubernetes cloud that our master will use for provisioning pod template agents using the <a href="https://github.com/jenkinsci/kubernetes-plugin">Jenkins Kubernetes plugin</a>.</p>

<p>The third and final detail to call out is the <code>jobs</code> section, which uses the <a href="https://github.com/jenkinsci/job-dsl-plugin">Job DSL plugin</a> to seed a Multibranch Pipeline job. The Jenkinsfile for this Pipeline is stored in the same GitHub repository as the rest of our config files. We&rsquo;ll detail the contents of this Pipeline script in a later section.</p>

<p>To apply this configuration, we apply the <code>ConfigMap</code> manifest file to our cluster:</p>

<pre><code class="language-bash">kubectl apply -f jenkinsCasc.yaml
</code></pre>

<p>With our <code>ConfigMap</code> and related <code>Secrets</code> created, we can now apply the manifest file from the previous section to deploy the remainder of the CJD resources:</p>

<pre><code class="language-bash">kubectl apply -f cjd.yaml
</code></pre>

<h2 id="securing-with-cert-manager">Securing with cert-manager</h2>

<p>At this point, our CJD instance is not accessible through HTTPS. To remedy this and enhance the security of our environment, we&rsquo;ll be using <a href="https://docs.cert-manager.io/en/latest/"><code>cert-manager</code></a>, a Kubernetes tool used to automate the management of certificates within a cluster. In this case, we&rsquo;ll use it to manage our TLS certificate issuance from <a href="https://letsencrypt.org/">Let&rsquo;s Encrypt</a>.</p>

<p>Our setup process for <code>cert-manager</code> loosely follows their <a href="https://github.com/jetstack/cert-manager/blob/master/docs/tutorials/acme/quick-start/index.rst">Quick-Start guide</a>. Because we&rsquo;ve already configured an ingress controller with a corresponding DNS entry along with deploying the CJD resources, we can <a href="https://github.com/jetstack/cert-manager/blob/master/docs/tutorials/acme/quick-start/index.rst#step-0---install-helm-client">ensure Helm</a> <a href="https://github.com/jetstack/cert-manager/blob/master/docs/tutorials/acme/quick-start/index.rst#step-1---installer-tiller">&amp; Tiller</a> are installed on the cluster, then skip to the <a href="https://github.com/jetstack/cert-manager/blob/master/docs/tutorials/acme/quick-start/index.rst#step-5---deploy-cert-manager">step of actually deploying <code>cert-manager</code></a>.</p>

<p>Once <code>cert-manager</code> has been deployed in its new <code>namespace</code>, we&rsquo;ll next need to deploy the <code>Issuer</code> to our <code>cjd</code> <code>namespace</code>.</p>

<blockquote>
<p><strong>Note</strong>: on initial setup of <code>cert-manager</code>, it&rsquo;s probably prudent to heed the Quick-Start&rsquo;s recommendation to create a staging <code>Issuer</code> first to minimize the risk of being rate limited by Let&rsquo;s Encrypt. For brevity, we&rsquo;ll only walk through the production <code>Issuer</code> creation here.</p>
</blockquote>

<p>Using the provided <a href="https://raw.githubusercontent.com/jetstack/cert-manager/release-0.8/docs/tutorials/acme/quick-start/example/production-issuer.yaml">example <code>Issuer</code> manifest file</a>, we&rsquo;ll swap in our actual email address before creating the resource in our <code>cjd</code> <code>namespace</code>:</p>

<pre><code class="language-yaml">apiVersion: certmanager.k8s.io/v1alpha1
kind: Issuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    # The ACME server URL
    server: https://acme-v02.api.letsencrypt.org/directory
    # Email address used for ACME registration
    email: melgin@cloudbees.com
    # Name of a secret used to store the ACME account private key
    privateKeySecretRef:
      name: letsencrypt-prod
    # Enable the HTTP-01 challenge provider
    http01: {}
</code></pre>

<pre><code class="language-bash">kubectl apply -f production-issuer.yaml
</code></pre>

<p>Once created, this <code>Issuer</code> relies on annotations on our <code>Ingress</code> to manage the TLS certificate creation. Recall that we briefly discussed the <code>Ingress</code> manifest in a previous section:</p>

<pre><code class="language-yaml">---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: cjd
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;true&quot;
    certmanager.k8s.io/issuer: &quot;letsencrypt-prod&quot; # add after cert-manager deploy
    certmanager.k8s.io/acme-challenge-type: http01 # add after cert-manager deploy
spec:
  tls: # cert-manager
  - hosts: # cert-manager
    - cjd.cloudbees.elgin.io # cert-manager
    secretName: cjd-tls # cert-manager
  rules:
  - host: cjd.cloudbees.elgin.io
    http:
      paths:
      - path: /
        backend:
          serviceName: cjd
          servicePort: 80
</code></pre>

<p>The lines with comments referencing <code>cert-manager</code> are required for the TLS certificate to be successfully issued. These include specifying the <code>Issuer</code>, the challenge type, as well as the hostname and <code>secretName</code>.</p>

<p>You can confirm that the certificate has been successfully issued by running <code>kubectl get certificate</code> and verifying that <code>READY</code> is <code>True</code> for our <code>cjd-tls</code> certificate. Once this process has been completed, CJD should now be accessible via HTTPS.</p>

<h2 id="writing-the-update-pipeline-job">Writing the update Pipeline job</h2>

<p>With CJD now running in our cluster and accessible via HTTPS, we&rsquo;ll next take a look at the Pipeline script that will handle the update of the master. At a high-level, we need our Pipeline to accomplish two major tasks:</p>

<ol>
<li>build and push our Docker image whenever a change is pushed to the GitHub repository, and</li>
<li>update our Kubernetes resources with the newly built Docker image and any additional changes.</li>
</ol>

<p>We represent these two procedures as stages within our Pipeline script.</p>

<p>For the first stage, we will use <a href="https://github.com/GoogleContainerTools/kaniko">kaniko</a> to build and push our Docker image to <a href="https://cloud.google.com/container-registry/">Google Container Registry</a>. Because we&rsquo;ll be using different agents for each stage, we&rsquo;ll start the Pipeline with <code>agent none</code>. Within the first stage, we define our agent using YAML, which specifies the <a href="https://gcr.io/kaniko-project/executor:debug">Google-provided kaniko image</a> as the container we will use.</p>

<p>To use kaniko, we&rsquo;ll first need to <a href="https://github.com/GoogleContainerTools/kaniko#kubernetes-secret">follow this kaniko documentation</a> to create a Google Cloud service account with appropriate permissions and download the related JSON key. Assuming we&rsquo;ve renamed the key <code>kaniko-secret.json</code>, we can <a href="http://docs.heptio.com/content/private-registries/pr-gcr.html">follow this procedure from Heptio</a> to create another Kubernetes <code>Secret</code> to allow for authentication to Google Container Registry (again replacing the placeholder email with the real service account email address):</p>

<pre><code class="language-bash">kubectl create secret docker-registry gcr-secret \
    --docker-server=https://gcr.io \
    --docker-username=_json_key \
    --docker-email=${SERVICE_ACCOUNT@PROJECT.iam.gserviceaccount.com} \
    --docker-password=&quot;$(cat kaniko-secret.json)&quot;
</code></pre>

<p>Within the <code>step</code> block, we are accomplishing two main things:
1. In the default <code>jnlp</code> container, we store the specific Git commit ID that triggered the build as an environment variable
2. In the <code>kaniko</code> container, we build and push our latest Docker image, tagging it with the commit ID we just stored.</p>

<pre><code class="language-groovy">pipeline {
  agent none
  stages {
    stage('Build and push with kaniko') {
      agent {
        kubernetes {
          label &quot;kaniko-${UUID.randomUUID().toString()}&quot;
          yaml &quot;&quot;&quot;
kind: Pod
metadata:
  name: kaniko
spec:
  serviceAccountName: cjd
  containers:
  - name: kaniko
    image: gcr.io/kaniko-project/executor:debug-v0.10.0
    imagePullPolicy: Always
    command:
    - /busybox/cat
    tty: true
    volumeMounts:
      - name: jenkins-docker-cfg
        mountPath: /kaniko/.docker
  volumes:
  - name: jenkins-docker-cfg
    projected:
      sources:
      - secret:
          name: gcr-secret
          items:
            - key: .dockerconfigjson
              path: config.json
&quot;&quot;&quot;
        }
      }
      environment {
        PATH = &quot;/busybox:/kaniko:$PATH&quot;
      }
      steps {
        container('jnlp') {
          script {
              env.COMMIT_ID = sh(returnStdout: true, script: 'git rev-parse HEAD').trim()
          }
        }
        container(name: 'kaniko', shell: '/busybox/sh') {
          sh &quot;&quot;&quot;#!/busybox/sh
                /kaniko/executor --context `pwd` --destination gcr.io/melgin/cjd-casc:${env.COMMIT_ID} --cache=true
          &quot;&quot;&quot;
        }
      }
    }
</code></pre>

<p>In the subsequent stage, we now apply changes to our CJD configuration to the resources running in our Kubernetes cluster.</p>

<p>First, we use a <code>when</code> directive to ensure we only run this stage when the Pipeline is running off of the <em>master</em> branch. We then use the <a href="https://gcr.io/cloud-builders/kubectl">Google-provided kubectl image</a> for our stage agent pod template. Within this container, we apply changes to our <code>jenkins-casc</code> <code>ConfigMap</code>, the resources specified in <code>cjd.yaml</code>, and finally set the image for our CJD <code>StatefulSet</code> to the latest one we&rsquo;ve just pushed to Google Container Registry:</p>

<pre><code class="language-groovy">    stage('Update CJD') {
      when {
        beforeAgent true
        branch 'master'
      }
      agent {
        kubernetes {
          label &quot;kubectl-${UUID.randomUUID().toString()}&quot;
          yaml &quot;&quot;&quot;
kind: Pod
metadata:
  name: kubectl
spec:
  serviceAccountName: cjd
  containers:
  - name: kubectl
    image: gcr.io/cloud-builders/kubectl@sha256:50de93675e6a9e121aad953658b537d01464cba0e4a3c648dbfc89241bb2085e
    imagePullPolicy: Always
    command:
    - cat
    tty: true
&quot;&quot;&quot;
        }
      }
      steps {
        container('kubectl') {
          sh &quot;&quot;&quot;
            kubectl apply -f jenkinsCasc.yaml
            kubectl apply -f cjd.yaml
            kubectl set image statefulset cjd cjd=gcr.io/melgin/cjd-casc:${env.COMMIT_ID}
          &quot;&quot;&quot;
        }
      }
    }
  }
}
</code></pre>

<p>To ensure the <code>cjd-casc</code> Pipeline job is triggered automatically upon each commit or pull request, we need to ensure a webhook is setup within the GitHub repository following <a href="https://support.cloudbees.com/hc/en-us/articles/224543927-GitHub-Integration-Webhooks">this process</a>.</p>

<p>With this in place, we now have all of our Jenkins configuration stored as code in our GitHub repository, including the process for updating the configuration. Whenever a change is pushed to the repository, those changes will automatically be applied to our Jenkins master.</p>

<p><img src="/img/cjd-casc/cjd-casc-pipeline.png" alt="successful run of cjd-casc Pipeline" /></p>

<h2 id="further-enhancements">Further enhancements</h2>

<p>This approach moves us much closer to the practice of GitOps for our Jenkins configuration. However, there are certainly areas for enhancement going forward. A few immediate examples that come to mind include:</p>

<ul>
<li>Non-master branch Pipeline runs could deploy the CJD resources &amp; config to a staging <code>namespace</code>. This would allow for the vetting of changes in a non-production environment before merging to master - a workflow critical for use in any scenario supporting mission-critical workloads.</li>
<li>Some level of smoke testing should be introduced for either/both of the non-prod/prod <code>namespaces</code> as a third Pipeline stage. This could range from a simple <code>curl</code> command to check the Jenkins system message in order to verify Jenkins is up and running, all the way to more complex cases that verify the latest configuration has been appropriately applied.</li>
<li><code>post</code> blocks could be introduced for notification to the appropriate Slack channel, email list, etc., that a Jenkins update has commenced/succeeded/failed.</li>
<li>Right now, the Docker image is rebuilt on every Pipeline run - even if no changes have been committed to the <code>Dockerfile</code> or related files. While caching is in place, it would be even more efficient to check for changes to those specific files, then selectively skip or run the <code>Build and push with kaniko</code> stage (though this does somewhat complicate the tagging of the Docker image each time a commit triggers a build).</li>
</ul>
]]></content>
        </item>
        
        <item>
            <title>CloudBees&#39; Cross Team Collaboration for Asynchronous DevSecOps</title>
            <link>https://technologists.dev/posts/cloudbees-cross-team-and-dev-sec-ops/</link>
            <pubDate>Mon, 10 Jun 2019 07:50:46 -0400</pubDate>
            
            <guid>https://technologists.dev/posts/cloudbees-cross-team-and-dev-sec-ops/</guid>
            <description>What is Cross Team Collaboration? CloudBees&amp;rsquo; Cross Team Collaboration provides the ability to publish an event from a Jenkins job that triggers any other Jenkins job on the same master or different masters that are listening for that event. It is basically a light-weight PubSub for CloudBees Core Masters connected to CloudBees Operations Center. Jenkins has had the ability to trigger other jobs for quite a while now (and with CloudBees this is even easy to do across Masters), but it always required that the upstream job be aware of the downstream job(s) to be triggered.</description>
            <content type="html"><![CDATA[

<h2 id="what-is-cross-team-collaboration">What is Cross Team Collaboration?</h2>

<p>CloudBees&rsquo; Cross Team Collaboration provides the ability to publish an event from a Jenkins job that triggers any other Jenkins job on the same master or different masters that are listening for that event. It is basically a light-weight <a href="https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern"><strong>PubSub</strong></a> for CloudBees Core Masters connected to <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/operating/#managing-operation-center">CloudBees Operations Center</a>. Jenkins has had the ability to <a href="https://jenkins.io/doc/pipeline/steps/pipeline-build-step/">trigger other jobs</a> for quite a while now (and <a href="https://support.cloudbees.com/hc/en-us/articles/226408088-Trigger-jobs-across-masters">with CloudBees this is even easy to do across Masters</a>), but it always required that the upstream job be aware of the downstream job(s) to be triggered. The Cross Team Collaboration feature provides a loosely coupled link between upstream and downstream Jenkins jobs - so that any job that is interested in a certain event, for whatever reason, can subscribe to that event and get triggered whenever that event is published.</p>

<p>Here are a few good CloudBees&rsquo; blog posts and CloudBees&rsquo; documentation on CloudBees&rsquo; Cross Team Collaboration:</p>

<ul>
<li><a href="https://www.cloudbees.com/blog/cross-team-collaboration-part-1">Cross Team Collaboration (Part 1)</a></li>
<li><a href="https://www.cloudbees.com/blog/cross-team-collaboration-part-2">Cross Team Collaboration (Part 2)</a></li>
<li><a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/cross-team-collaboration/">Cross Team Collaboration documentation</a></li>
</ul>

<h2 id="devsecops">DevSecOps</h2>

<p><a href="https://tech.gsa.gov/guides/understanding_differences_agile_devsecops/">DevSecOps</a> - the idea of shifting security left in your Continuous Delivery pipelines - is becoming a vital component of successful CD. DevSecOps is all about speeding up software delivery, while maintaining, or even improving, the level of security for delivered application code. However, even though you should be shifting automated security left - you still don&rsquo;t want it to impede developers trying to deliver software more quickly. CloudBees&rsquo; Cross Team Collaboration feature is a perfect capability for automating security while at the same time getting out of the way of developers - improving the security and quality of your software delivery while minimizing the impact on delivery speed.</p>

<h2 id="use-case-asynchronously-scan-container-images-for-vulnerabilities-and-compliance">Use Case: Asynchronously Scan Container Images for Vulnerabilities and Compliance</h2>

<p>As containers become a more and more ubiquitous method for delivering your applications, ensuring that your container images don&rsquo;t have security vulnerabilities and/or organization specific security compliance issues is an important aspect of CD for containerized application delivery. However, scanning containers images isn&rsquo;t the fastest process in the world and you don&rsquo;t want to unnecessarily slow down developers trying to get stuff done. You also may not want to depend on individual development teams to configure and manage important securitys steps in their delivery pipelines.</p>

<p>Cross Team Collaboration enables you to publish an event from a <a href="https://jenkins.io/doc/book/pipeline/shared-libraries/">Pipeline Shared Library</a> for <a href="https://github.com/cloudbees-days/pipeline-library/blob/master/vars/kanikoBuildPush.groovy">securely building container images</a> and then asynchronously triggering <em>not-so-quick</em> security related jobs listening for events, making it very easy to provide security as part of the CD pipelines for an entire organization.</p>

<p>So, this container scan job can be run on different Jenkins Masters (or as we at CloudBees refer to them: <a href="https://www.cloudbees.com/blog/team-masters-continuous-delivery">Team Masters</a>) and are able to run automatically thanks to the <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/cross-team-collaboration/#cross-team-config">Cross Team Collaboration queue</a> managed by the <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/operating/#managing-operation-center">Operations Center</a> in CloudBees Core. Pipelines for building containers and checking vulnerabilities are then decoupled, but they run any time you build a container in an upstream job (e.g. every time engineering teams build containers, the vulnerabilities and compliance will be checked, but running this container scan doesn&rsquo;t require building the container again).</p>

<h3 id="cross-team-collaboration-events">Cross Team Collaboration Events</h3>

<p>There are basically <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/cross-team-collaboration/#cross-team-event-types">two types of Cross Team Collaboration events</a>:</p>

<p><strong>Simple Event:</strong></p>

<pre><code class="language-groovy">publishEvent simpleEvent(&quot;${dockerReg}/helloworld-nodejs:${repoName}-${BUILD_NUMBER}&quot;)
</code></pre>

<p><strong>JSON Event:</strong></p>

<pre><code class="language-groovy">publishEvent event:jsonEvent(&quot;{'eventType':'containerImagePush', 'image':'${dockerReg}/helloworld-nodejs:${repoName}-${BUILD_NUMBER}'}&quot;), verbose: true
</code></pre>

<p>For this example we will be using the more verbose JSON event. The problem with the <strong>Simple Event</strong> approach is that the triggered job would have to subscribe to a single <code>string</code> value and in this case a specific container <code>image</code>. But what we really want is to run an Anchore scan for all container images being pushed to our DEV container registry. The <strong>JSON Event</strong> approach allows us to subscribe to a more generic event, <code>containerImagePush</code>, while passing the exact container image being pushed as an additional JSON value for the key <code>image</code>.  But to use this approach the triggered job(s) must retrieve the value of the <code>image</code> key from the event payload.</p>

<h3 id="capturing-the-cross-team-collaboration-event-payload">Capturing the Cross Team Collaboration Event Payload</h3>

<p>Now let&rsquo;s compare using groovy code vs a <code>curl</code> call against the <a href="https://wiki.jenkins.io/display/JENKINS/Remote+access+API">Jenkins REST API</a> to get the JSON event payload:</p>

<ul>
<li>You could get the event JSON with the following: <code>currentBuild.getBuildCauses()[0].event.toString()</code>. But that will run on the Jenkins Master, not the Jenkins agent and will impact performance when you are scanning hundreds or even thousands of container images.</li>
<li>A better approach is to use the <code>sh</code> step with a <code>curl</code> call against the Jenkins REST API with a <a href="https://jenkins.io/blog/2018/07/02/new-api-token-system/">Jenkins API token</a> to get the JSON representation of the current build, and then piping the JSON response to <a href="https://stedolan.github.io/jq/"><strong>jq</strong></a> to get the value for the <code>image</code> key from the event payload in a Jenkins Pipeline triggered by the <code>EventTriggerCause</code>: <code>curl -u 'beedemo-admin':$TOKEN --silent ${BUILD_URL}/api/json| jq '.actions[0].causes[0].event.image'</code>. The advantages of this approach are:

<ul>
<li>The <code>sh</code> step will run on the agent, not the Jenkins Master, allowing you to scale across as many agents as needed for your container scans with very little impact on the performance of the Jenkins Master.</li>
<li>Using lightweight shell scripts provide easier testing and more portability of your CD pipelines to other platforms.</li>
</ul></li>
</ul>

<p>| NOTE: <code>BUILD_URL</code> is one of many <a href="https://jenkins.io/doc/book/pipeline/getting-started/#global-variable-reference">Pipeline global variables</a> available to all Jenkins Pipeline jobs.</p>

<h3 id="anchore-inline-scan">Anchore Inline Scan</h3>

<p>Earlier this year, <a href="https://anchore.com/">Anchore</a> provided some new tools and scripts to make it easier to execute Anchore scans without constantly running an Anchore Engine. The <a href="https://anchore.com/inline-scanning-with-anchore-engine/">Anchore <strong>inline scan</strong></a> provides the same analysis/vulnerability/policy evaluation and reporting as a statically managed Anchore engine and is used in this example to highlight how easy and fast you can add container security scanning to your own CD pipelines. However, a better long-term approach would be to stand-up your own centralized, managed and stable Anchore engine to use across all of you dev teams. The advantages of a static, always running Anchore Engine include:</p>

<ul>
<li><strong>Faster scans:</strong> since you don&rsquo;t have to wait for the Anchore engine to start-up for each job.</li>
<li><strong>Reduced infrastructure costs:</strong> if you only do a few scans a day then this is less of an advantage as you will have a constant infrastructure cost for the static Anchore engine. But if you are doing 100s of scan per day then you will definitely realize savings with this approach.</li>
<li><strong>More secure:</strong> as we will see in the <strong>inline scan</strong> example below, the Anchore <code>inline_scan</code> script requires access to a Docker daemon. And in this example we are using the <a href="https://github.com/jenkinsci/kubernetes-plugin">Jenkins Kubernetes plugin</a> to provide dynamic and ephemeral agent pods for the Anchore inline scan job. A quick and dirty approach - that has a number of security implications - for providing a K8s pod agent access to the Docker daemon is to mount the Docker socket as a <code>volume</code> on the pod.</li>
</ul>

<p>But again, we will use the newer Anchore <strong>inline scan</strong> in this example to highlight how fast you can add container scans to your own Jenkins Pipelines.</p>

<p><em>Anchore inline scan Pod</em> - <code>dockerClientPod.yml</code></p>

<pre><code class="language-yaml">apiVersion: v1
kind: Pod
spec:
  containers:
  - name: docker-client
    image: gcr.io/technologists/docker-client:0.0.3
    command: ['cat']
    tty: true
    volumeMounts:
    - name: dockersock
      mountPath: /var/run/docker.sock
  volumes:
  - name: dockersock
    hostPath:
      path: /var/run/docker.sock
</code></pre>

<p>Even though there is an <a href="https://plugins.jenkins.io/anchore-container-scanner">Anchore plugin for Jenkins</a>, there is no reason to install another plugin when you can accomplish the same thing with a straightforward <code>sh</code> step. As mentioned in my <a href="./jenkins-plugins-good-bad-ugly/">last post here on the Technologists site</a> - using fewer Jenkins plugins is a <strong>good</strong> thing.</p>

<pre><code class="language-groovy">container('docker-client'){
  sh &quot;curl -s https://ci-tools.anchore.io/inline_scan-v0.3.3 \
  | bash -s -- -f -b ./.anchore_policy.json -p ${containerImage}&quot;
}
</code></pre>

<p>Again, the only thing required to run the scan above is a Docker daemon. So you could just as easily run that command on your laptop running Docker as on a Jenkins agent that has access to a Docker daemon.</p>

<h3 id="putting-it-all-together">Putting It All Together</h3>

<p><em>CloudBees&rsquo; Pipeline Template Catalog, Pipeline Shared Library, and Cross Team Collaboration</em></p>

<p>By combining the new <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/pipeline/#_setting_up_a_pipeline_template_catalog">CloudBees&rsquo; Pipeline Template Catalogs</a> with a Pipeline Shared Library and CloudBees&rsquo; Cross Team Collaboration we are able to provide robust DevSecOps application delivery Pipelines that are very easy for development teams to adopt quickly.</p>

<p>First we have the Pipeline Shared Library for building our container images with <a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a>:</p>

<p><em>pipeline shared library</em> - <code>kanikoBuildPush.groovy</code></p>

<pre><code class="language-groovy">def call(String imageName, String imageTag = env.BUILD_NUMBER, String gcpProject = &quot;core-workshop&quot;, String target = &quot;.&quot;, String dockerFile=&quot;Dockerfile&quot;, Closure body) {
  def dockerReg = &quot;gcr.io/${gcpProject}&quot;
  imageName = &quot;helloworld-nodejs&quot;
  def label = &quot;kaniko-${UUID.randomUUID().toString()}&quot;
  def podYaml = libraryResource 'podtemplates/dockerBuildPush.yml'
  podTemplate(name: 'kaniko', label: label, yaml: podYaml, inheritFrom: 'default-jnlp', nodeSelector: 'type=agent') {
    node(label) {
      body()
      imageNameTag()
      gitShortCommit()
      def repoName = env.IMAGE_REPO.toLowerCase()
      container(name: 'kaniko', shell: '/busybox/sh') {
        withEnv(['PATH+EXTRA=/busybox:/kaniko']) {
          sh &quot;&quot;&quot;#!/busybox/sh
            /kaniko/executor -f ${pwd()}/${dockerFile} -c ${pwd()} --build-arg context=${repoName} --build-arg buildNumber=${BUILD_NUMBER} --build-arg shortCommit=${env.SHORT_COMMIT} --build-arg commitAuthor=${env.COMMIT_AUTHOR} -d ${dockerReg}/helloworld-nodejs:${repoName}-${BUILD_NUMBER}
          &quot;&quot;&quot;
        }
      }
      publishEvent event:jsonEvent(&quot;{'eventType':'containerImagePush', 'image':'${dockerReg}/helloworld-nodejs:${repoName}-${BUILD_NUMBER}'}&quot;), verbose: true
    }
  }
}
</code></pre>

<p>Note the <code>publishEvent</code> step at the end - after the container image has been successfully built and pushed to our <strong>dev</strong> container registry it will <strong>publish</strong> the <code>containerImagePush</code> event.</p>

<p><em>The JSON output for the <code>publishEvent</code> step - note the <code>image</code> key value is the container image just built and pushed by Kaniko:</em></p>

<pre><code class="language-json">{
  &quot;eventType&quot;: &quot;containerImagePush&quot;,
  &quot;image&quot;: &quot;gcr.io/core-workshop/helloworld-nodejs:beeops-cb-days-7&quot;,
  &quot;source&quot;:     {
      &quot;type&quot;: &quot;JenkinsTeamBuild&quot;,
      &quot;buildInfo&quot;:         {
          &quot;build&quot;: 7,
          &quot;job&quot;: &quot;template-jobs/beedemo-admin-helloworld-nodejs/master&quot;,
          &quot;jenkinsUrl&quot;: &quot;https://********/teams-sec/&quot;,
          &quot;instanceId&quot;: &quot;d37a81cc1906b6fe684f253a8a07834c&quot;,
          &quot;team&quot;: &quot;sec&quot;
      }
  }
}
</code></pre>

<p>Next, the <code>kanikoBuildPush</code> shared library is consumed by a <a href="https://github.com/cloudbees-days/pipeline-template-catalog">Pipeline Template Catalog</a> template. In this case a <a href="https://github.com/cloudbees-days/pipeline-template-catalog/tree/master/templates/nodejs-app">template for Node.js applications</a>:</p>

<p><a href="https://github.com/cloudbees-days/pipeline-template-catalog/blob/master/templates/nodejs-app/Jenkinsfile"><em>Pipeline Template</em></a> - <strong>Build and Push Image</strong> <code>stage</code></p>

<pre><code class="language-groovy">    stage('Build and Push Image') {
      when {
        beforeAgent true
        branch 'master'
      }
      steps {  
        echo &quot;${repoOwner}&quot;
        kanikoBuildPush(env.IMAGE_NAME, env.IMAGE_TAG, &quot;${gcpProject}&quot;) {
          checkout scm
        }
      }
      post {
        success {
          slackSend message: &quot;${JOB_NAME} pipeline job is awaiting approval at: ${RUN_DISPLAY_URL}&quot;
        }
      }
    }
</code></pre>

<p>Again, if the <code>kanikoBuildPush</code> library step is successful it will publish a <code>containerImagePush</code> event.</p>

<p>Finally, we set-up a job on our <strong>Security</strong> Jenkins Master to listen for the <code>containerImagePush</code> event:</p>

<p><a href="https://github.com/cloudbees-days/anchore-scan/blob/master/Jenkinsfile"><strong>anchore-scan</strong> <code>Jenkinsfile</code></a></p>

<pre><code class="language-groovy">def containerImage
pipeline {
  agent none

  triggers {
      eventTrigger jmespathQuery(&quot;eventType=='containerImagePush'&quot;)
  }
  
  stages {
    stage('Anchore Scan') {
      agent {
        kubernetes {
          label 'docker-client'
          yamlFile 'dockerClientPod.yml'
        }
      }
      when { 
        triggeredBy 'EventTriggerCause' 
        beforeAgent true
      }
      environment {
        TOKEN = credentials('beedemo-admin-api-key')
      }
      steps {
        script {
          containerImage = sh(script: &quot;&quot;&quot;
             curl -u 'beedemo-admin':$TOKEN --silent ${BUILD_URL}/api/json| jq '.actions[0].causes[0].event.image'
          &quot;&quot;&quot;, returnStdout: true)
        }
        echo containerImage
        container('docker-client'){
          sh &quot;curl -s https://ci-tools.anchore.io/inline_scan-v0.3.3 | bash -s -- -f -b ./.anchore_policy.json -p ${containerImage}&quot;
        }
      }
    }
  }
}
</code></pre>

<p>Note the <code>eventTrigger</code> step uses <code>jmespathQuery</code> to listen for the <code>containerImagePush</code> <code>eventType</code>. Also note the <code>triggeredBy</code> condition <code>EventTriggerCause</code> in the <a href="https://jenkins.io/doc/book/pipeline/syntax/#when"><code>when</code> directive</a> - this will result in the <code>Anchore Scan</code> stage only running (and the provisioning of a K8s pod based agent used for the scan) if this job is triggered by a Cross Team Collaboration event.</p>

<p>If the newly built container image doesn&rsquo;t pass all of the policies specified in the <a href="https://github.com/cloudbees-days/anchore-scan/blob/master/.anchore_policy.json"><code>.anchore_policy.json</code></a> file then the job will fail.</p>

<p>Here is an example Anchore report for a failed <code>anchore-scan</code> job:</p>

<pre><code class="language-console">Image Digest: sha256:e03d86b75d38d1d18035b58e9e43088c9d0d5dd6e49f2c507d949937174f3465
Full Tag: anchore-engine:5000/helloworld-nodejs:beeops-cb-days-5
Image ID: 0b22d7798cd24465252335d602059fea88128244b623bc4af20926eeec8f9b4c
Status: fail
Last Eval: 2019-06-07T12:56:03Z
Policy ID: custom-anchore-policy-nodejs
Final Action: stop
Final Action Reason: policy_evaluation

Gate              Trigger               Detail                                                                                     Status        
dockerfile        effective_user        User root found as effective user, which is explicity not allowed list                     stop          
dockerfile        instruction           Dockerfile directive 'HEALTHCHECK' not found, matching condition 'not_exists' check        warn          

Image Digest: sha256:e03d86b75d38d1d18035b58e9e43088c9d0d5dd6e49f2c507d949937174f3465
Full Tag: anchore-engine:5000/helloworld-nodejs:beeops-cb-days-5
Status: fail
Last Eval: 2019-06-07T12:56:04Z
Policy ID: custom-anchore-policy-nodejs
</code></pre>

<p>As you can see from the above output the scan failed because of the <code>effective_user</code> trigger - <a href="https://github.com/nodejs/docker-node/blob/master/10/alpine/Dockerfile">the official <code>node</code> container image we are using from DockerHub runs as <code>root</code></a> and <a href="https://snyk.io/blog/10-docker-image-security-best-practices/">this is a very bad security practice</a> as it allows <strong>container breakouts</strong> where the container user is able to escape the container namespace and interact with other processes on the host.</p>

<h3 id="some-improvements">Some Improvements</h3>

<ul>
<li>One improvement would be to run this without mounting the Docker socket in the <a href="https://github.com/cloudbees-days/anchore-scan/blob/declarative/dockerClientPod.yml"><code>docker-client</code> container</a>. The Anchore inline-scan script runs a number of Docker commands that requires a Docker daemon - but this is not good security. Using a static Anchore engine would allow us to do container scans without mounting the Docker socket.</li>
<li>Another improvement would be to extend the <code>anchore-scan</code> job to push the container image to a <strong>Prod</strong> container registry on success and notify interested dev teams that their image is now available for production deployments.</li>
</ul>

<h3 id="casc-for-cross-team-collaboration-configuration-for-your-cloudbees-core-v2-masters">CasC for Cross Team Collaboration Configuration for your CloudBees Core v2 Masters</h3>

<p>In order for all of this to work you have to turn on Cross Team Collaboration for all of your Core v2 Masters that you want to publish and subscribe to events. I am a big proponent of CasC for everything so here is an <a href="https://wiki.jenkins.io/display/JENKINS/Post-initialization+script"><code>init.groovy.d</code></a> script to set-up CasC to automatically enable Cross Team Collaboration notifications for your CloudBees Core v2 Masters on start-up:</p>

<p><a href="https://github.com/kypseli/cb-core-mm-workshop/blob/master/quickstart/init_61_notification_api.groovy"><em>cb-core-mm-workshop/quickstart/init_61_notification_api.groovy</em></a>:</p>

<pre><code class="language-groovy">import jenkins.model.Jenkins
import hudson.ExtensionList

import com.cloudbees.jenkins.plugins.notification.api.NotificationConfiguration
import com.cloudbees.jenkins.plugins.notification.spi.Router
import com.cloudbees.opscenter.plugins.notification.OperationsCenterRouter

jenkins = Jenkins.getInstance()

NotificationConfiguration config = ExtensionList.lookupSingleton(NotificationConfiguration.class);
Router r = new OperationsCenterRouter();
        config.setRouter(r);
        config.setEnabled(true);
        config.onLoaded();
</code></pre>

<p>I&rsquo;m also a big fan of the Jenkins Config-as-Code plugin. However, currently, the CloudBees&rsquo; plugins for Cross Team Collaboration do not yet support <a href="https://github.com/jenkinsci/configuration-as-code-plugin">JCasC</a>.</p>

<table>
<thead>
<tr>
<th>UPD (Sep 12, 2019): Jenkins Configuration as Code plugin is now supported in <a href="https://www.cloudbees.com/products/cloudbees-jenkins-distribution">CloudBees Jenkins Distribution</a> and <a href="https://www.cloudbees.com/products/cloudbees-jenkins-support">CloudBees Jenkins Support</a>. Support for CloudBees Core and Cross Team Collaboration is coming soon. For information about the current support status, please see <a href="https://support.cloudbees.com/hc/en-us/articles/360031191471-State-of-Jenkins-Configuration-as-Code-JCasC-support-in-CloudBees-products">this page</a>.</th>
</tr>
</thead>

<tbody>
</tbody>
</table>

<h2 id="add-devsecops-to-your-cd-with-cloudbees-now">Add DevSecOps to Your CD with CloudBees Now</h2>

<p>So there&rsquo;s really no excuse NOT to add asynchronous container security scans to your container image CD pipelines with CloudBees Core v2, our Cross Team Collaboration feature and the Anchore <strong>inline scan</strong> - when it is as easy as this!</p>
]]></content>
        </item>
        
        <item>
            <title>Jenkins Plugins: The Good, the Bad and the Ugly</title>
            <link>https://technologists.dev/posts/jenkins-plugins-good-bad-ugly/</link>
            <pubDate>Thu, 30 May 2019 05:50:46 -0400</pubDate>
            
            <guid>https://technologists.dev/posts/jenkins-plugins-good-bad-ugly/</guid>
            <description>There are over 1600 Jenkins plugins and that is both a blessing and a curse. Of those 1600 plugins only a small percentage are well maintained and tested, and even fewer (140 of 1600+) are part of the CloudBees Assurance Program (CAP) as verified and/or compatible plugins - well tested to interoperate with the rest of the CAP plugins (and their dependencies) and with a specific LTS version of Jenkins.</description>
            <content type="html"><![CDATA[

<p>There are over <a href="http://updates.jenkins.io/pluginCount.txt">1600 Jenkins plugins</a> and that is both a blessing and a curse. Of those 1600 plugins only a small percentage are well maintained and tested, and even fewer (140 of 1600+) are part of the <a href="https://go.cloudbees.com/docs/cloudbees-documentation/assurance-program/">CloudBees Assurance Program (CAP)</a> as verified and/or compatible plugins - well tested to interoperate with the rest of the CAP plugins (and their dependencies) and with a specific LTS version of Jenkins. Problems can arise when you use plugins that aren&rsquo;t part of CAP, or a plugin that isn&rsquo;t well maintained or tested to work with all of the other plugins you are using and the specific version of Jenkins that you are using. But the extensibility offered by plugins has helped make Jenkins the most popular CI tool on the planet.</p>

<p>I typically like to end posts on a good note, so I will start with <em>The Ugly</em> and end with <em>The Good</em> - and then offer some opinionated ideas/best practices on Jenkins plugin management and usage.</p>

<h1 id="the-ugly">The Ugly</h1>

<p>There are almost always a number of Jenkins plugins that have security vulnerabilities. Over 55 plugins were listed as part of the <a href="https://jenkins.io/security/advisory/2019-04-03/">2019-04-03 Jenkins Security Advisory</a>. Even worse is when you find out that a plugin that you are using has a security vulnerability and you also find out that the plugin is not maintained anymore. You could search the 1600+ Jenkins plugins to see if there is another plugin that is maintained and that does what you need, or you could become a plugin maintainer - not exactly what you intended to sign up for when you first started using Jenkins. Are you developing your own applications or are you looking to become a Jenkins plugin developer?</p>

<p>Another <em>ugly</em> issue arises when you have numerous Jenkins masters in your organization. These Jenkins instances are often snowflakes comprised of many different plug-ins. So managing more than one Jenkins master with disparate sets of plugins can become very ugly, very quickly. CloudBees can certainly help you with this through CAP and something we call <a href="https://go.cloudbees.com/docs/cloudbees-documentation/admin-cje/cje-ux/#_when_to_use_a_team_master_when_to_use_a_managed_master">Team Masters - easily provisioned and managed team specific Jenkins masters</a> with an opinionated set of very stable and tested plugins. However, there is nothing stopping individual Jenkins master admins from manually installing a plugin and sometimes ending up with an unusable Jenkins master.</p>

<h1 id="the-bad">The Bad</h1>

<p>Installing a lot of plugins can result in maintenance hell and sometimes your Jenkins master doesn&rsquo;t even restart successfully after upgrading a plugin.</p>


    <img src="/img/jenkins-plugins-good-bad-ugly/jenkins_devil.png"  alt="Jenkins Devil"  class="left"  />



<p>And although the Jenkins Devil makes for a very cool sticker, it isn&rsquo;t something you ever want to see on <strong>your</strong> Jenkins Master, especially after restarting Jenkins for a plugin update. Backing out a plugin update that causes Jenkins to crash is not a fun thing to deal with and will slow down your software delivery.</p>

<p>Dependency hell is another <em>bad</em> thing that Jenkins admins have to deal with all the time. Sometimes upgrading just one plugin results in the need to update dozens others, and many Jenkins admins do this directly on their production Jenkins master. Blue Ocean, while a noble attempt at a new UI for Jenkins Pipelines, requires dozens of dependencies, many of which you probably have no use for - for example the Blue Ocean plugin suite requires both the <em>Bitbucket Pipeline for Blue Ocean</em> and the <em>GitHub Pipeline for Blue Ocean</em> plugins even if you don&rsquo;t use either Bitbucket or GitHub for source control.</p>

<p>Too many plugins that do the same thing - how do you choose? Search the Jenkins plugin site for <em>Docker</em> and you get 26 results. If I want Docker based agents should I use the <strong>Docker plugin</strong> or <strong>Yet Another Docker plugin</strong>? With 1600+ plugins, sometimes it can be hard to choose the right one.</p>

<h1 id="the-good">The Good</h1>

<p>The extensibility and integrations provided by Jenkins plugins are amazing. I don&rsquo;t believe that there is any other CI platform that integrates with as many source control tools/platforms as Jenkins. Without Jenkins&rsquo; extensive plugin ecosystem it would not be the CI automation tool of choice that it has become. Jenkins is by far the most flexible CI platform available, bar none, and the Jenkins plugin ecosystem is a big reason why.</p>

<p>There are a lot of very <em>good</em>, and even necessary, plugins. Like plugins for credentials and for source control - Jenkins has awesome integration with GitHub and Bitbucket for example. And the Jenkins Pipeline plugin suite (although another example of dependency hell) provides a <a href="https://jenkins.io/doc/book/pipeline/syntax/#declarative-pipeline">Declarative approach to building you CI/CD pipelines</a> that can be <a href="https://jenkins.io/doc/book/pipeline/jenkinsfile/">easily managed as-code in source control</a>. And finally, the <a href="https://jenkins.io/projects/jcasc/">JCasC plugin</a> makes it easier than ever to manager your Jenkins master configuration as-code in source control.</p>

<p>So there are some very <em>good</em> reasons to use <strong>some</strong> plugins.</p>

<h1 id="so-what-to-do">So What to Do</h1>

<p>CloudBees can certainly help. All of the CloudBees distributions, including the <a href="https://www.cloudbees.com/products/cloudbees-jenkins-distribution">free CloudBees Jenkins Distribution</a>, include CAP with Beekeeper. I have managed a few demo/workshop environments for the CloudBees Solution Architecture team for the last 4 years and update those environments almost every month. I have yet to have an update that has resulted in the Jenkins Devil - ok maybe one.</p>

<p>There are a few other things you can do right <strong>now</strong> , whether you use a CloudBees Distro or not, to make using Jenkins Plugins easier to manage and less impactful to your production Jenkins master - allowing you to focus on CD for the applications you are delivering instead of spending too much time managing Jenkins.</p>

<h2 id="use-jenkins-pipeline">Use Jenkins Pipeline</h2>

<p>Although Jenkins Pipeline does require a <a href="https://plugins.jenkins.io/workflow-aggregator">number of plugins and plugin dependencies</a> its advantages far outweigh the disadvantages of using Jenkins without Pipeline jobs. Using Jenkins Pipelines with a Jenkinsfile in source control and <a href="https://jenkins.io/doc/book/pipeline/shared-libraries/">Pipeline Shared Libraries</a> can greatly reduce the number of additional plugins you need to install and manage. For example if you need to send a Slack message, just run a simple <code>curl</code> command in a lightweight container instead of installing the <a href="https://github.com/jenkinsci/slack-plugin/issues">Jenkins Slack plugin</a>:</p>

<pre><code class="language-bash">curl -X POST -H 'Content-type: application/json' --data '{&quot;text&quot;:&quot;The build is broken :(&quot;}' YOUR_WEBHOOK_URL
</code></pre>

<p>This is actually considered a best practice for Jenkins Pipelines as any <code>step</code> that is run from a plugin will actually run on the Jenkins master, not on the agent (other than the <code>sh</code>, <code>bat</code> and <code>pwsh</code> steps). This will result in worse performance for your Jenkins master and may even bring your Jenkins master down - once again slowing down your application delivery.</p>

<p>Another big plus with replacing Jenkins Pipeline plugin based steps with lightweight shell scripts is that it provides easier testing and more portability of your CD pipelines to other platforms. For example, Jenkins X Pipelines with Tekton runs every pipeline step as a command in a container - adopting that approach with Jenkins Pipelines now will make it much easier to migrate to better emerging solutions in the future.</p>

<h2 id="use-fewer-plugins">Use Fewer Plugins</h2>

<p>Using fewer plugs will reduce the amount of pain you will incur from many of the <em>ugly</em> and <em>bad</em> issues mentioned above. Migrating as many Jenkins Pipeline <code>steps</code> from plugins to <code>sh</code> steps running in containers not only reduces the <em>bad</em> and <em>ugly</em> above, it also makes it easier to test and reduce dependencies on the less than stellar plugin maintainers (like me), and provides better portability to other emerging CD technologies - like <a href="https://kurtmadel.com/posts/native-kubernetes-continuous-delivery/jenkins-x-goes-native/#re-tooling-with-tekton">Jenkins X Pipelines with Tekton</a>.</p>

<p>Do you really need the Docker plugin and the Yet Another Docker plugin? Or the Chuck Noris plugin? The fewer plugins that you install, the fewer plugins you have to manage and the less chance that they will have security issues or even worse, bring your Jenkins master down - Jenkins Devil and all.</p>

<h2 id="test">Test</h2>

<p>Always test any new plugin or plugin update before you put it into your production Jenkins master(s). Running Jenkins as a container can certainly make this easier - and is what I suggest - but there is no reason why you can&rsquo;t use Jenkins to automate this kind of testing regardless of how you deploy Jenkins. Just spin up a Jenkins master with a few <em>fake</em> jobs that use the plugins in a similar way to how you use them in your <em>real</em> jobs. All of this can be automated with Jenkins itself.</p>

<p>The <a href="https://github.com/jenkins-x/jenkins-x-serverless-filerunner">Jenkins X ephemeral masters</a> basically went with this approach - extensive testing whenever a new <a href="https://github.com/jenkins-x/jenkins-x-serverless-filerunner/blob/master/pom.xml#L32">plugin was added to the the CasC Master container image</a>.</p>

<h2 id="manage-plugins-with-casc">Manage Plugins with CasC</h2>

<p>Never use the Jenkins UI to install plugins. Maintain your plugins as code in source control, where every new plugin and plugin upgrade can be tracked as commits. The easiest and best way to do this, in my opinion, is to use a customized Docker image that includes the plugins you <strong>absolutely need</strong> - in addition to other configuration via JCasC (and if necessary, <a href="https://wiki.jenkins.io/display/JENKINS/Post-initialization+script"><code>init</code> scripts</a>). If you have read any of my other posts you will know that I am a big fan of containers - and have always run Jenkins with containers since I started at CloudBees back in 2015. The Jenkins GitHub Org <em>docker</em> project <a href="https://github.com/jenkinsci/docker/blob/master/install-plugins.sh">provides a script</a> for <a href="https://github.com/jenkinsci/docker#preinstalling-plugins">preinstalling plugins</a> from a simple <code>plugins.txt</code> file so your Jenkins master container image has all the plugins you need on startup. This makes it easier to test plugin changes and all of your plugin changes are captured as code commits - and a tool like Git (GitHub, BitBucket, even GitLab) is much better at tracking/auditing/controlling such changes than Jenkins was ever meant to be. Here is a simple <code>plugins.txt</code> file and <code>Dockerfile</code> to get you started:</p>

<p><em>plugins.txt</em></p>

<pre><code class="language-txt">configuration-as-code:1.19
credentials:2.2.0
</code></pre>

<p>Yes, only two plugins. The reason why we only need these two plugins is because the <a href="https://www.cloudbees.com/blog/cloudbees-jenkins-distribution-adds-stability-and-security-your-jenkins-environment">CloudBees Jenkins Distribution</a> already contains a curated set of plugins for Jenkins Pipeline, Blue Ocean, source control management and everything else we need - all well tested for us already.</p>

<p>This version of the Credentials plugin is an exception, because the recent version of the plugin with JCasC support has not been integrated into CAP yet (coming soon!).</p>

<table>
<thead>
<tr>
<th>UPD (Sep 12, 2019): Jenkins Configuration as Code plugin is now supported in <a href="https://www.cloudbees.com/products/cloudbees-jenkins-distribution">CloudBees Jenkins Distribution</a> and <a href="https://www.cloudbees.com/products/cloudbees-jenkins-support">CloudBees Jenkins Support</a>. See the <a href="https://go.cloudbees.com/docs/cloudbees-jenkins-distribution/distro-admin-guide/configuration-as-code/">Administering CJD: Configuration as Code</a> for usage guidelines and quick start. You can also find an official demo <a href="https://github.com/cloudbees-oss/cjd-jcasc-demo">here</a>. For information about other CloudBees products, please see <a href="https://support.cloudbees.com/hc/en-us/articles/360031191471-State-of-Jenkins-Configuration-as-Code-JCasC-support-in-CloudBees-products">this page</a>.</th>
</tr>
</thead>

<tbody>
</tbody>
</table>

<p><em>Extending the CloudBees Jenkins Distribution container image with plugins and JCasC</em></p>

<pre><code class="language-Dockerfile">FROM cloudbees/cloudbees-jenkins-distribution:2.164.3.2

# optional, but you might want to let everyone know who is responsible for their Jenkins ;)
LABEL maintainer &quot;kmadel@cloudbees.com&quot;

#set java opts variable to skip setup wizard; plugins will be installed via license activated script
ENV JAVA_OPTS=&quot;-Djenkins.install.runSetupWizard=false&quot;
#skip setup wizard; per https://github.com/jenkinsci/docker/tree/master#preinstalling-plugins
RUN echo 2.0 &gt; /usr/share/jenkins/ref/jenkins.install.UpgradeWizard.state

# diable cli
ENV JVM_OPTS -Djenkins.CLI.disabled=true -server
# set your timezone
ENV TZ=&quot;/usr/share/zoneinfo/America/New_York&quot;

#config-as-code plugin configuration
COPY config-as-code.yml /usr/share/jenkins/config-as-code.yml
ENV CASC_JENKINS_CONFIG /usr/share/jenkins/config-as-code.yml

# use CloudBees' update center to ensure you don't allow any really bad plugins
ENV JENKINS_UC http://jenkins-updates.cloudbees.com

#install suggested and additional plugins
COPY plugins.txt /usr/share/jenkins/ref/plugins.txt
COPY jenkins-support /usr/local/bin/jenkins-support
COPY install-plugins.sh /usr/local/bin/install-plugins.sh
RUN bash /usr/local/bin/install-plugins.sh &lt; /usr/share/jenkins/ref/plugins.txt
</code></pre>

<h1 id="use-plugins-you-need-and-no-more">Use Plugins You Need and No More</h1>

<p>So, don&rsquo;t avoid Jenkins plugins - they are an important part of what makes Jenkins great and add critical features to the way you will use Jenkins - but be smart about the plugins you use and keep your application delivery your primary focus - not your CI tool.</p>
]]></content>
        </item>
        
        <item>
            <title>Extending Jenkins X for Traditional Deployments with CloudBees Flow</title>
            <link>https://technologists.dev/posts/jenkins-x-flow-integration/</link>
            <pubDate>Wed, 29 May 2019 12:47:46 -0400</pubDate>
            
            <guid>https://technologists.dev/posts/jenkins-x-flow-integration/</guid>
            <description>Jenkins X is quickly becoming the de facto standard for high performing teams wanting to do CI/CD in a highly scalable and fault tolerant environment. For those who havenâ€™t gotten the opportunity to try out Jenkins X, it allows teams to run CI/CD workloads natively in a Kubernetes environment while taking advantage of modern operating patterns like GitOps and serverless architectures. For teams wanting to modernize their continuous integration and continuous deployment capabilities, Jenkins X is the go to solution.</description>
            <content type="html"><![CDATA[

<p><a href="https://jenkins-x.io">Jenkins X</a> is quickly becoming the de facto standard for high performing teams wanting to do CI/CD in a highly scalable and fault tolerant environment. For those who havenâ€™t gotten the opportunity to try out Jenkins X, it allows teams to run CI/CD workloads natively in a Kubernetes environment while taking advantage of modern operating patterns like GitOps and serverless architectures. For teams wanting to modernize their continuous integration and continuous deployment capabilities, Jenkins X is the go to solution.</p>

<p>In todayâ€™s heterogenous technology environment, most organizations tend to have a mix of modern cloud native architectures as well as more traditional workloads which get deployed either on-prem or within the cloud. In the latter case, a combination of Jenkins X (performing CI steps) and CloudBees Flow (handling the deployment) can add a huge amount of flexibility and power to a Continuous Delivery process.  The combination of Jenkins X and CloudBees Flow also brings improved visibility and tracability across the application landscape.</p>

<p>Jenkins X can be easily extended to accommodate any type of workload required - it can be a full end to end CI/CD tool for building, deploying, and running applications all within a Kubernetes cluster, or it can handle CI while offloading other release and deployment tasks to another solution.  In this blog post weâ€™re going to cover how Jenkins X can be extended to offload release/deployment tasks to <a href="https://www.cloudbees.com/cloudbees-acquires-electric-cloud">CloudBees Flow</a>.  We will accomplish this by extending the maven Jenkins X build pack in order to call the CloudBees Flow REST API as part of the Jenkins X pipeline execution.</p>

<h1 id="extending-jenkins-x">Extending Jenkins X</h1>

<p>For the purposes of this blog, weâ€™re going to be focusing on the Jenkins X serverless pipeline execution engine with Tekton (See <a href="https://jenkins-x.io/architecture/jenkins-x-pipelines/">https://jenkins-x.io/architecture/jenkins-x-pipelines/</a>). There are two main ways to customize a Jenkins X pipeline in order to integrate with CloudBees Flow.  The first and simplest would be to modify the jenkins-x.yml (more information on Jenkins X pipelines: <a href="https://jenkins-x.io/architecture/jenkins-x-pipelines/#differences-to-jenkins-pipelines">https://jenkins-x.io/architecture/jenkins-x-pipelines/#differences-to-jenkins-pipelines</a> and the jenkins-x.yml file) pipeline file in the source code repo for the project weâ€™re going to build.  The other way is to extend the <a href="https://jenkins-x.io/architecture/build-packs/">Jenkins X build packs</a> and modify the build pack for the language/build tool you want to use.  Both will work, but by forking the build packs you can get reuse across multiple projects using the build pack you extend. In this example, weâ€™ll walk through how to extend the Jenkins X build packs.</p>

<h2 id="creating-our-cluster-and-installing-jenkins-x">Creating our Cluster and Installing Jenkins X</h2>

<p>To start, weâ€™ll fork the Jenkins X Kubernetes build packs into our own repository: <a href="https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes">https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes</a>.  Later we&rsquo;ll be extending the maven build pack to support a REST API call into CloudBees Flow.</p>

<p>Now itâ€™s time to create a Kubernetes cluster on GKE using Jenkins X and <a href="https://github.com/tektoncd/pipeline">Tekton</a>.  In this case, we&rsquo;re starting by creating a cluster from scratch, but Jenkins X can also be installed into an existing Kubernetes cluster if you already have one available by using the <code>jx install</code> command:</p>

<pre><code class="language-bash">jx create cluster gke --tekton --no-tiller
</code></pre>

<p>Fill out the options.  For example:</p>

<pre><code class="language-shell">$  jx create cluster gke
Your browser has been opened to visit:

    https://accounts.google.com/o/oauth2/auth?redirect_uri=....


? Google Cloud Project: jhendrick-ckcd
Updated property [core/project].
Lets ensure we have container and compute enabled on your project
No apis need to be enable as they are already enabled: container compute
No cluster name provided so using a generated one: crownprong
? What type of cluster would you like to create Zonal
? Google Cloud Zone: us-west1-a
? Google Cloud Machine Type: n1-standard-4
? Minimum number of Nodes (per zone) 3
? Maximum number of Nodes 5
? Would you like use preemptible VMs? No
? Would you like to access Google Cloud Storage / Google Container Registry? No
Creating cluster...
Initialising cluster ...
? Select Jenkins installation type: Serverless Jenkins X Pipelines with Tekton
Setting the dev namespace to: jx
Namespace jx created 
</code></pre>

<p>Create an ingress controller if one doesnâ€™t exist and setup the domain or use the default *.nip.io address if you donâ€™t have one.  Go through the prompts and then configure your GitHub credentials.  Create an API token using the URL provided if you donâ€™t have one:</p>

<pre><code class="language-shell">If you don't have a wildcard DNS setup then setup a DNS (A) record and point it at: 35.197.85.1 then use the DNS domain in the next input...
? Domain 35.197.85.1.nip.io
nginx ingress controller installed and configured
? Would you like to enable Long Term Storage? A bucket for provider gke will be created No
Lets set up a Git user name and API token to be able to perform CI/CD

Creating a local Git user for GitHub server
? GitHub user name: jhendrick
To be able to create a repository on GitHub we need an API Token
Please click this URL https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo

Then COPY the token and enter in into the form below:

? API Token: ****************************************
Select the CI/CD pipelines Git server and user
? Do you wish to use GitHub as the pipelines Git server: Yes
Creating a pipelines Git user for GitHub server
To be able to create a repository on GitHub we need an API Token
Please click this URL https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo

Then COPY the token and enter in into the form below:

? API Token: ****************************************
Setting the pipelines Git server https://github.com and user name jhendrick.
Saving the Git authentication configuration
</code></pre>

<p>In the setup weâ€™re going to choose the Kubernetes workloads option and later modify the kubernetes workload build packs to include the CloudBees Flow specific steps:</p>

<pre><code class="language-shell">? Pick default workload build pack: [Use arrows to move, space to select, type to filter]
&gt; Kubernetes Workloads: Automated CI+CD with GitOps Promotion
Library Workloads: CI+Release but no CD
</code></pre>

<h2 id="editing-the-build-packs">Editing the Build Packs</h2>

<p>You can use your favorite IDE but in this case, we&rsquo;ll modify the Jenkins X build packs in VS Code with the YAML Language extension installed (<a href="https://jenkins-x.io/architecture/jenkins-x-pipelines/#editing-in-vs-code">https://jenkins-x.io/architecture/jenkins-x-pipelines/#editing-in-vs-code</a>) for validation as recommended by the Jenkins X team.</p>

<p>This example is going to focus on a sample Spring Boot application using Maven.  To start we&rsquo;ll modified the maven build pack in our forked build pack repo (<a href="https://github.com/jhendrickCB/jenkins-x-kubernetes/blob/master/packs/maven/pipeline.yaml):">https://github.com/jhendrickCB/jenkins-x-kubernetes/blob/master/packs/maven/pipeline.yaml):</a></p>

<pre><code class="language-yaml">extends:
 import: classic
 file: maven/pipeline.yaml
pipelines:
 release:
   build:
     steps:
     - sh: jx step post build --image $DOCKER_REGISTRY/$ORG/$APP_NAME:\$(cat VERSION)
       name: post-build
   promote:
     steps:
     - sh: jx step changelog --version v\$(cat ../../VERSION)
       name: changelog
     - comment: call CloudBees Flow to run a release
       sh: &gt;
         curl -X POST --header &quot;Authorization: Basic $(jx step credential -s flow-token -k token)&quot; --header &quot;Content-Type: application/json&quot; --header &quot;Accept: application/json&quot; -d &quot;{}&quot; &quot;https://ps9.ecloud-kdemo.com/rest/v1.0/pipelines?pipelineName=my_pipeline&amp;projectName=my_project&quot; --insecure
       name: cloudbees-flow-release
</code></pre>

<p>Compare the original build pack for maven found here: <a href="https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs/maven">https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs/maven</a> vs. our forked build pack.  Weâ€™ve removed all the references to skaffold, watch, and helm since weâ€™re no longer having Jenkinâ€™s X handle the deployment to our Kubernetes cluster.  Weâ€™ve also updated the pipeline file to make an API call into our CloudBees Flow server using <a href="https://curl.haxx.se/">cURL</a>:</p>

<pre><code class="language-bash">curl -X POST --header &quot;Authorization: Basic $(jx step credential -s flow-token -k token)&quot; --header &quot;Content-Type: application/json&quot; --header &quot;Accept: application/json&quot; -d &quot;{}&quot; &quot;https://ps9.ecloud-kdemo.com/rest/v1.0/pipelines?pipelineName=my_pipeline&amp;projectName=my_project&quot; --insecure
</code></pre>

<p>The above API call into CloudBees Flow tells Flow to run a pipeline called <code>my_pipeline</code> within a project called <code>my_project</code>.</p>

<p>Youâ€™ll also notice that weâ€™re using a Jenkins X feature (<code>jx step credential</code>) to get our secret, <code>flow-token</code>, which we created previously so that we can authenticate to the Flow Rest API. Note that there are many other possible ways to call into the CloudBees Flow APIâ€™s besides cURL such as the command line tool <a href="http://docs.electric-cloud.com/eflow_doc/9_0/API/HTML/FlowAPI_Guide_9_0.htm#EFlow_api/usingAPI.htm?Highlight=ectool">ectool</a> as well as <a href="http://docs.electric-cloud.com/eflow_doc/9_0/API/HTML/FlowAPI_Guide_9_0.htm#EFlow_api/usingAPI.htm%3FTocPath%3DUsing%2520the%25C2%25A0ElectricFlow%2520Perl%2520API%7C_____0">perl</a> or <a href="http://docs.electric-cloud.com/eflow_doc/9_0/API/HTML/FlowAPI_Guide_9_0.htm#EFlow_api/UsingGroovy.htm%3FTocPath%3D_____11">groovy libraries</a>.  Also note that for a production environment we would want to setup the proper certificates rather than using the <code>--insecure parameter</code>.</p>

<p>Next, we need to tell Jenkins X to use our new build pack:</p>

<pre><code class="language-shell">$ jx edit buildpack -u https://github.com/jhendrickCB/jenkins-x-kubernetes -r master -b

Setting the team build pack to  repo: https://github.com/jhendrickCB/jenkins-x-kubernetes ref: master
</code></pre>

<p>Since we have to authenticate when calling the <a href="http://docs.electric-cloud.com/eflow_doc/9_0/API/HTML/FlowAPI_Guide_9_0.htm">Flow REST API</a>, weâ€™ll create a Kubernetes secret to store our username/password basic authentication token:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
 name: flow-token
type: Opaque
data:
 token: &lt;Basic Auth Token&gt;
</code></pre>

<p>Note: In this case, the <code>&lt;Basic Auth Token&gt;</code> will take the form of <code>username:password</code> base64 encoded.  Take note that weâ€™ll actually need to base64 encode our username:password token twice as it will get base64 decoded automatically when we access it later.</p>

<p>To apply the secret in our Kubernetes cluster, we can save our secret to a file called <code>flow-token-secret.yaml</code> and run the command:</p>

<pre><code class="language-bash">kubectl apply -f flow-token-secret.yaml
</code></pre>

<h1 id="creating-a-sample-spring-boot-project">Creating a Sample Spring Boot Project</h1>

<p>To test out our new build pack, weâ€™ll use Jenkins Xâ€™s capability to create a quick start project for a Spring Boot microservice:</p>

<pre><code class="language-bash">jx create spring -d web -d actuator
</code></pre>

<p>Follow the prompts to create the Spring Boot project and setup the repository on your GitHub account:</p>

<pre><code class="language-shell">$ jx create spring -d web -d actuator
Using Git provider GitHub at https://github.com
? Do you wish to use jhendrick as the Git user name? Yes


About to create repository  on server https://github.com with user jhendrick
? Which organisation do you want to use? jhendrickCB
? Enter the new repository name:  jx-spring-flowdemo


Creating repository jhendrickCB/jx-spring-flowdemo
? Language: java
? Group: com.example
Created Spring Boot project at /Users/jhendrick/Cloudbees/jx-spring-flowdemo
The directory /Users/jhendrick/Cloudbees/jx-spring-flowdemo is not yet using git
? Would you like to initialise git now? Yes
? Commit message:  Initial import

Git repository created
selected pack: /Users/jhendrick/.jx/draft/packs/github.com/jhendrickCB/jenkins-x-kubernetes/packs/maven

replacing placeholders in directory /Users/jhendrick/Cloudbees/cloudbees-days/kops-cluster/jx-spring-flowdemo
app name: jx-spring-flowdemo, git server: github.com, org: jhendrickcb, Docker registry org: jhendrickcb
skipping directory &quot;/Users/jhendrick/Cloudbees/jx-spring-flowdemo/.git&quot;
skipping ignored file &quot;/Users/jhendrick/Cloudbees/jx-spring-flowdemo/HELP.md&quot;
Pushed Git repository to https://github.com/jhendrickCB/jx-spring-flowdemo

Creating GitHub webhook for jhendrickCB/jx-spring-flowdemo for url http://hook.jx.35.197.85.1.nip.io/hook

Watch pipeline activity via:    jx get activity -f jx-spring-flowdemo -w
Browse the pipeline log via:    jx get build logs jhendrickCB/jx-spring-flowdemo/master
Open the Jenkins console via    jx console
You can list the pipelines via: jx get pipelines
When the pipeline is complete:  jx get applications

For more help on available commands see: https://jenkins-x.io/developing/browsing/

Note that your first pipeline may take a few minutes to start while the necessary images get downloaded!
</code></pre>

<p>Once created, the project should build and run automatically.  If everything worked, we should see our Spring Boot project built with Maven, artifacts uploaded automatically to our Nexus repository and then our CloudBees Flow pipeline executed within our CloudBees Flow server.</p>

<p>If for some reason, we made a mistake, the pipeline can be re-run by using:</p>

<pre><code class="language-bash">jx start pipeline
</code></pre>

<p>To debug, build logs can be checked with:</p>

<pre><code class="language-bash">jx get build logs 
</code></pre>

<p>Or more specifically with our project name:</p>

<pre><code class="language-bash">jx get build logs jhendrickCB/jx-spring-flowdemo/master
</code></pre>

<p>We can get build activity with:</p>

<pre><code class="language-bash">jx get activity -w
</code></pre>

<p>Or more specifically:</p>

<pre><code class="language-bash">jx get activity -f jx-spring-flowdemo -w
</code></pre>

<h1 id="in-conclusion">In Conclusion</h1>

<p>In the above example we were able to use Jenkins X to build our application as well as store the built artifacts, and then utilize CloudBees flow to handle execution of our release pipeline.  This allows us to take advantage of the scalability and efficiency of Jenkins X while leveraging the power and control of CloudBees Flow for managing the release.</p>

<p>For organizations who want to take advantage of modern CI/CD on Jenkins X but are not yet &ldquo;all in&rdquo; on Kubernetes and still deploying traditional applications, this provides a very solid approach to achieving Continuous Delivery.</p>
]]></content>
        </item>
        
        <item>
            <title>Introducing the Technologists, A CloudBees Solution Architecture Team</title>
            <link>https://technologists.dev/posts/introducing-technologists/</link>
            <pubDate>Thu, 23 May 2019 19:10:46 -0400</pubDate>
            
            <guid>https://technologists.dev/posts/introducing-technologists/</guid>
            <description>The Technologists is a new team of CloudBees Solution Architects. Technologists have a passion for emerging technologies, continuously learning and teaching through thought leadership, providing technical direction within CloudBees and the broader tech community, and driving the best technical solutions for customers.
 Technical integrity is of the utmost importance for a Technologist - always providing the RIGHT solution. We are Technologists focused on providing best practices, solutions, and adoption paths to organizations navigating software delivery transformations with leading edge technologies.</description>
            <content type="html"><![CDATA[<p>The Technologists is a new team of CloudBees Solution Architects. Technologists have a passion for emerging technologies, continuously learning and teaching through thought leadership, providing technical direction within CloudBees and the broader tech community, and driving the best technical solutions for customers.</p>

<ul>
<li>Technical integrity is of the utmost importance for a Technologist - always providing the RIGHT solution.</li>
<li>We are Technologists focused on providing best practices, solutions, and adoption paths to organizations navigating software delivery transformations with leading edge technologies.</li>
<li>Technologists are Thought Leaders internally at CloudBees and in the DevOps and wider Tech community - writing blog posts, speaking at conferences and meetups, contributing to open source projects on technologies and technical practices related to CloudBees&rsquo; products and to the DevOps space in general.</li>
</ul>

<p>From Cloud Native CD to microservice and even nanoservice architecture to service meshes and API gateways, Technologists are early adopters of the best of the best emerging technologies related to software delivery. Look to this website for interesting posts on a number of technical subjects related to CloudBees&rsquo; products and DevOps in general, like:</p>

<ul>
<li>Native Kubernetes Continuous Delivery</li>
<li>Jenkins Plugins: The Good, the Bad and the Ugly</li>
<li>The State of DevOps Analytics</li>
</ul>

<p>Stay tuned for more&hellip;</p>
]]></content>
        </item>
        
    </channel>
</rss>
