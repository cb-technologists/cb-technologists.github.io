<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on CloudBees Technologists</title>
        <link>https://cb-technologists.github.io/posts/</link>
        <description>Recent content in Posts on CloudBees Technologists</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Mon, 10 Jun 2019 07:50:46 -0400</lastBuildDate>
        <atom:link href="https://cb-technologists.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>CloudBees&#39; Cross Team Collaboration for Asynchronous DevSecOps</title>
            <link>https://cb-technologists.github.io/posts/cloudbees-cross-team-and-dev-sec-ops/</link>
            <pubDate>Mon, 10 Jun 2019 07:50:46 -0400</pubDate>
            
            <guid>https://cb-technologists.github.io/posts/cloudbees-cross-team-and-dev-sec-ops/</guid>
            <description>What is Cross Team Collaboration? CloudBees&amp;rsquo; Cross Team Collaboration provides the ability to publish an event from a Jenkins job that triggers any other Jenkins job on the same master or different masters that are listening for that event. It is basically a light-weight PubSub for CloudBees Core Masters connected to CloudBees Operations Center. Jenkins has had the ability to trigger other jobs for quite a while now (and with CloudBees this is even easy to do across Masters), but it always required that the upstream job be aware of the downstream job(s) to be triggered.</description>
            <content type="html"><![CDATA[

<h2 id="what-is-cross-team-collaboration">What is Cross Team Collaboration?</h2>

<p>CloudBees&rsquo; Cross Team Collaboration provides the ability to publish an event from a Jenkins job that triggers any other Jenkins job on the same master or different masters that are listening for that event. It is basically a light-weight <a href="https://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern"><strong>PubSub</strong></a> for CloudBees Core Masters connected to <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/operating/#managing-operation-center">CloudBees Operations Center</a>. Jenkins has had the ability to <a href="https://jenkins.io/doc/pipeline/steps/pipeline-build-step/">trigger other jobs</a> for quite a while now (and <a href="https://support.cloudbees.com/hc/en-us/articles/226408088-Trigger-jobs-across-masters">with CloudBees this is even easy to do across Masters</a>), but it always required that the upstream job be aware of the downstream job(s) to be triggered. The Cross Team Collaboration feature provides a loosely coupled link between upstream and downstream Jenkins jobs - so that any job that is interested in a certain event, for whatever reason, can subscribe to that event and get triggered whenever that event is published.</p>

<p>Here are a few good CloudBees&rsquo; blog posts and CloudBees&rsquo; documentation on CloudBees&rsquo; Cross Team Collaboration:</p>

<ul>
<li><a href="https://www.cloudbees.com/blog/cross-team-collaboration-part-1">Cross Team Collaboration (Part 1)</a></li>
<li><a href="https://www.cloudbees.com/blog/cross-team-collaboration-part-2">Cross Team Collaboration (Part 2)</a></li>
<li><a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/cross-team-collaboration/">Cross Team Collaboration documentation</a></li>
</ul>

<h2 id="devsecops">DevSecOps</h2>

<p><a href="https://tech.gsa.gov/guides/understanding_differences_agile_devsecops/">DevSecOps</a> - the idea of shifting security left in your Continuous Delivery pipelines - is becoming a vital component of successful CD. DevSecOps is all about speeding up software delivery, while maintaining, or even improving, the level of security for delivered application code. However, even though you should be shifting automated security left - you still don&rsquo;t want it to impede developers trying to deliver software more quickly. CloudBees&rsquo; Cross Team Collaboration feature is a perfect capability for automating security while at the same time getting out of the way of developers - improving the security and quality of your software delivery while minimizing the impact on delivery speed.</p>

<h2 id="use-case-asynchronously-scan-container-images-for-vulnerabilities-and-compliance">Use Case: Asynchronously Scan Container Images for Vulnerabilities and Compliance</h2>

<p>As containers become a more and more ubiquitous method for delivering your applications, ensuring that your container images don&rsquo;t have security vulnerabilities and/or organization specific security compliance issues is an important aspect of CD for containerized application delivery. However, scanning containers images isn&rsquo;t the fastest process in the world and you don&rsquo;t want to unnecessarily slow down developers trying to get stuff done. You also may not want to depend on individual development teams to configure and manage important securitys steps in their delivery pipelines.</p>

<p>Cross Team Collaboration enables you to publish an event from a <a href="https://jenkins.io/doc/book/pipeline/shared-libraries/">Pipeline Shared Library</a> for <a href="https://github.com/cloudbees-days/pipeline-library/blob/master/vars/kanikoBuildPush.groovy">securely building container images</a> and then asynchronously triggering <em>not-so-quick</em> security related jobs listening for events, making it very easy to provide security as part of the CD pipelines for an entire organization.</p>

<p>So, this container scan job can be run on different Jenkins Masters (or as we at CloudBees refer to them: <a href="https://www.cloudbees.com/blog/team-masters-continuous-delivery">Team Masters</a>) and are able to run automatically thanks to the <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/cross-team-collaboration/#cross-team-config">Cross Team Collaboration queue</a> managed by the <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/operating/#managing-operation-center">Operations Center</a> in CloudBees Core. Pipelines for building containers and checking vulnerabilities are then decoupled, but they run any time you build a container in an upstream job (e.g. every time engineering teams build containers, the vulnerabilities and compliance will be checked, but running this container scan doesn&rsquo;t require building the container again).</p>

<h3 id="cross-team-collaboration-events">Cross Team Collaboration Events</h3>

<p>There are basically <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/cross-team-collaboration/#cross-team-event-types">two types of Cross Team Collaboration events</a>:</p>

<p><strong>Simple Event:</strong></p>

<pre><code class="language-groovy">publishEvent simpleEvent(&quot;${dockerReg}/helloworld-nodejs:${repoName}-${BUILD_NUMBER}&quot;)
</code></pre>

<p><strong>JSON Event:</strong></p>

<pre><code class="language-groovy">publishEvent event:jsonEvent(&quot;{'eventType':'containerImagePush', 'image':'${dockerReg}/helloworld-nodejs:${repoName}-${BUILD_NUMBER}'}&quot;), verbose: true
</code></pre>

<p>For this example we will be using the more verbose JSON event. The problem with the <strong>Simple Event</strong> approach is that the triggered job would have to subscribe to a single <code>string</code> value and in this case a specific container <code>image</code>. But what we really want is to run an Anchore scan for all container images being pushed to our DEV container registry. The <strong>JSON Event</strong> approach allows us to subscribe to a more generic event, <code>containerImagePush</code>, while passing the exact container image being pushed as an additional JSON value for the key <code>image</code>.  But to use this approach the triggered job(s) must retrieve the value of the <code>image</code> key from the event payload.</p>

<h3 id="capturing-the-cross-team-collaboration-event-payload">Capturing the Cross Team Collaboration Event Payload</h3>

<p>Now let&rsquo;s compare using groovy code vs a <code>curl</code> call against the <a href="https://wiki.jenkins.io/display/JENKINS/Remote+access+API">Jenkins REST API</a> to get the JSON event payload:</p>

<ul>
<li>You could get the event JSON with the following: <code>currentBuild.getBuildCauses()[0].event.toString()</code>. But that will run on the Jenkins Master, not the Jenkins agent and will impact performance when you are scanning hundreds or even thousands of container images.</li>
<li>A better approach is to use the <code>sh</code> step with a <code>curl</code> call against the Jenkins REST API with a <a href="https://jenkins.io/blog/2018/07/02/new-api-token-system/">Jenkins API token</a> to get the JSON representation of the current build, and then piping the JSON response to <a href="https://stedolan.github.io/jq/"><strong>jq</strong></a> to get the value for the <code>image</code> key from the event payload in a Jenkins Pipeline triggered by the <code>EventTriggerCause</code>: <code>curl -u 'beedemo-admin':$TOKEN --silent ${BUILD_URL}/api/json| jq '.actions[0].causes[0].event.image'</code>. The advantages of this approach are:

<ul>
<li>The <code>sh</code> step will run on the agent, not the Jenkins Master, allowing you to scale across as many agents as needed for your container scans with very little impact on the performance of the Jenkins Master.</li>
<li>Using lightweight shell scripts provide easier testing and more portability of your CD pipelines to other platforms.</li>
</ul></li>
</ul>

<p>| NOTE: <code>BUILD_URL</code> is one of many <a href="https://jenkins.io/doc/book/pipeline/getting-started/#global-variable-reference">Pipeline global variables</a> available to all Jenkins Pipeline jobs.</p>

<h3 id="anchore-inline-scan">Anchore Inline Scan</h3>

<p>Earlier this year, <a href="https://anchore.com/">Anchore</a> provided some new tools and scripts to make it easier to execute Anchore scans without constantly running an Anchore Engine. The <a href="https://anchore.com/inline-scanning-with-anchore-engine/">Anchore <strong>inline scan</strong></a> provides the same analysis/vulnerability/policy evaluation and reporting as a statically managed Anchore engine and is used in this example to highlight how easy and fast you can add container security scanning to your own CD pipelines. However, a better long-term approach would be to stand-up your own centralized, managed and stable Anchore engine to use across all of you dev teams. The advantages of a static, always running Anchore Engine include:</p>

<ul>
<li><strong>Faster scans:</strong> since you don&rsquo;t have to wait for the Anchore engine to start-up for each job.</li>
<li><strong>Reduced infrastructure costs:</strong> if you only do a few scans a day then this is less of an advantage as you will have a constant infrastructure cost for the static Anchore engine. But if you are doing 100s of scan per day then you will definitely realize savings with this approach.</li>
<li><strong>More secure:</strong> as we will see in the <strong>inline scan</strong> example below, the Anchore <code>inline_scan</code> script requires access to a Docker daemon. And in this example we are using the <a href="https://github.com/jenkinsci/kubernetes-plugin">Jenkins Kubernetes plugin</a> to provide dynamic and ephemeral agent pods for the Anchore inline scan job. A quick and dirty approach - that has a number of security implications - for providing a K8s pod agent access to the Docker daemon is to mount the Docker socket as a <code>volume</code> on the pod.</li>
</ul>

<p>But again, we will use the newer Anchore <strong>inline scan</strong> in this example to highlight how fast you can add container scans to your own Jenkins Pipelines.</p>

<p><em>Anchore inline scan Pod</em> - <code>dockerClientPod.yml</code></p>

<pre><code class="language-yaml">apiVersion: v1
kind: Pod
spec:
  containers:
  - name: docker-client
    image: gcr.io/technologists/docker-client:0.0.3
    command: ['cat']
    tty: true
    volumeMounts:
    - name: dockersock
      mountPath: /var/run/docker.sock
  volumes:
  - name: dockersock
    hostPath:
      path: /var/run/docker.sock
</code></pre>

<p>Even though there is an <a href="https://plugins.jenkins.io/anchore-container-scanner">Anchore plugin for Jenkins</a>, there is no reason to install another plugin when you can accomplish the same thing with a straightforward <code>sh</code> step. As mentioned in my <a href="./jenkins-plugins-good-bad-ugly/">last post here on the Technologists site</a> - using fewer Jenkins plugins is a <strong>good</strong> thing.</p>

<pre><code class="language-groovy">container('docker-client'){
  sh &quot;curl -s https://ci-tools.anchore.io/inline_scan-v0.3.3 \
  | bash -s -- -f -b ./.anchore_policy.json -p ${containerImage}&quot;
}
</code></pre>

<p>Again, the only thing required to run the scan above is a Docker daemon. So you could just as easily run that command on your laptop running Docker as on a Jenkins agent that has access to a Docker daemon.</p>

<h3 id="putting-it-all-together">Putting It All Together</h3>

<p><em>CloudBees&rsquo; Pipeline Template Catalog, Pipeline Shared Library, and Cross Team Collaboration</em></p>

<p>By combining the new <a href="https://go.cloudbees.com/docs/cloudbees-core/cloud-admin-guide/pipeline/#_setting_up_a_pipeline_template_catalog">CloudBees&rsquo; Pipeline Template Catalogs</a> with a Pipeline Shared Library and CloudBees&rsquo; Cross Team Collaboration we are able to provide robust DevSecOps application delivery Pipelines that are very easy for development teams to adopt quickly.</p>

<p>First we have the Pipeline Shared Library for building our container images with <a href="https://github.com/GoogleContainerTools/kaniko">Kaniko</a>:</p>

<p><em>pipeline shared library</em> - <code>kanikoBuildPush.groovy</code></p>

<pre><code class="language-groovy">def call(String imageName, String imageTag = env.BUILD_NUMBER, String gcpProject = &quot;core-workshop&quot;, String target = &quot;.&quot;, String dockerFile=&quot;Dockerfile&quot;, Closure body) {
  def dockerReg = &quot;gcr.io/${gcpProject}&quot;
  imageName = &quot;helloworld-nodejs&quot;
  def label = &quot;kaniko-${UUID.randomUUID().toString()}&quot;
  def podYaml = libraryResource 'podtemplates/dockerBuildPush.yml'
  podTemplate(name: 'kaniko', label: label, yaml: podYaml, inheritFrom: 'default-jnlp', nodeSelector: 'type=agent') {
    node(label) {
      body()
      imageNameTag()
      gitShortCommit()
      def repoName = env.IMAGE_REPO.toLowerCase()
      container(name: 'kaniko', shell: '/busybox/sh') {
        withEnv(['PATH+EXTRA=/busybox:/kaniko']) {
          sh &quot;&quot;&quot;#!/busybox/sh
            /kaniko/executor -f ${pwd()}/${dockerFile} -c ${pwd()} --build-arg context=${repoName} --build-arg buildNumber=${BUILD_NUMBER} --build-arg shortCommit=${env.SHORT_COMMIT} --build-arg commitAuthor=${env.COMMIT_AUTHOR} -d ${dockerReg}/helloworld-nodejs:${repoName}-${BUILD_NUMBER}
          &quot;&quot;&quot;
        }
      }
      publishEvent event:jsonEvent(&quot;{'eventType':'containerImagePush', 'image':'${dockerReg}/helloworld-nodejs:${repoName}-${BUILD_NUMBER}'}&quot;), verbose: true
    }
  }
}
</code></pre>

<p>Note the <code>publishEvent</code> step at the end - after the container image has been successfully built and pushed to our <strong>dev</strong> container registry it will <strong>publish</strong> the <code>containerImagePush</code> event.</p>

<p><em>The JSON output for the <code>publishEvent</code> step - note the <code>image</code> key value is the container image just built and pushed by Kaniko:</em></p>

<pre><code class="language-json">{
  &quot;eventType&quot;: &quot;containerImagePush&quot;,
  &quot;image&quot;: &quot;gcr.io/core-workshop/helloworld-nodejs:beeops-cb-days-7&quot;,
  &quot;source&quot;:     {
      &quot;type&quot;: &quot;JenkinsTeamBuild&quot;,
      &quot;buildInfo&quot;:         {
          &quot;build&quot;: 7,
          &quot;job&quot;: &quot;template-jobs/beedemo-admin-helloworld-nodejs/master&quot;,
          &quot;jenkinsUrl&quot;: &quot;https://********/teams-sec/&quot;,
          &quot;instanceId&quot;: &quot;d37a81cc1906b6fe684f253a8a07834c&quot;,
          &quot;team&quot;: &quot;sec&quot;
      }
  }
}
</code></pre>

<p>Next, the <code>kanikoBuildPush</code> shared library is consumed by a <a href="https://github.com/cloudbees-days/pipeline-template-catalog">Pipeline Template Catalog</a> template. In this case a <a href="https://github.com/cloudbees-days/pipeline-template-catalog/tree/master/templates/nodejs-app">template for Node.js applications</a>:</p>

<p><a href="https://github.com/cloudbees-days/pipeline-template-catalog/blob/master/templates/nodejs-app/Jenkinsfile"><em>Pipeline Template</em></a> - <strong>Build and Push Image</strong> <code>stage</code></p>

<pre><code class="language-groovy">    stage('Build and Push Image') {
      when {
        beforeAgent true
        branch 'master'
      }
      steps {  
        echo &quot;${repoOwner}&quot;
        kanikoBuildPush(env.IMAGE_NAME, env.IMAGE_TAG, &quot;${gcpProject}&quot;) {
          checkout scm
        }
      }
      post {
        success {
          slackSend message: &quot;${JOB_NAME} pipeline job is awaiting approval at: ${RUN_DISPLAY_URL}&quot;
        }
      }
    }
</code></pre>

<p>Again, if the <code>kanikoBuildPush</code> library step is successful it will publish a <code>containerImagePush</code> event.</p>

<p>Finally, we set-up a job on our <strong>Security</strong> Jenkins Master to listen for the <code>containerImagePush</code> event:</p>

<p><a href="https://github.com/cloudbees-days/anchore-scan/blob/master/Jenkinsfile"><strong>anchore-scan</strong> <code>Jenkinsfile</code></a></p>

<pre><code class="language-groovy">def containerImage
pipeline {
  agent none

  triggers {
      eventTrigger jmespathQuery(&quot;eventType=='containerImagePush'&quot;)
  }
  
  stages {
    stage('Anchore Scan') {
      agent {
        kubernetes {
          label 'docker-client'
          yamlFile 'dockerClientPod.yml'
        }
      }
      when { 
        triggeredBy 'EventTriggerCause' 
        beforeAgent true
      }
      environment {
        TOKEN = credentials('beedemo-admin-api-key')
      }
      steps {
        script {
          containerImage = sh(script: &quot;&quot;&quot;
             curl -u 'beedemo-admin':$TOKEN --silent ${BUILD_URL}/api/json| jq '.actions[0].causes[0].event.image'
          &quot;&quot;&quot;, returnStdout: true)
        }
        echo containerImage
        container('docker-client'){
          sh &quot;curl -s https://ci-tools.anchore.io/inline_scan-v0.3.3 | bash -s -- -f -b ./.anchore_policy.json -p ${containerImage}&quot;
        }
      }
    }
  }
}
</code></pre>

<p>Note the <code>eventTrigger</code> step uses <code>jmespathQuery</code> to listen for the <code>containerImagePush</code> <code>eventType</code>. Also note the <code>triggeredBy</code> condition <code>EventTriggerCause</code> in the <a href="https://jenkins.io/doc/book/pipeline/syntax/#when"><code>when</code> directive</a> - this will result in the <code>Anchore Scan</code> stage only running (and the provisioning of a K8s pod based agent used for the scan) if this job is triggered by a Cross Team Collaboration event.</p>

<p>If the newly built container image doesn&rsquo;t pass all of the policies specified in the <a href="https://github.com/cloudbees-days/anchore-scan/blob/master/.anchore_policy.json"><code>.anchore_policy.json</code></a> file then the job will fail.</p>

<p>Here is an example Anchore report for a failed <code>anchore-scan</code> job:</p>

<pre><code class="language-console">Image Digest: sha256:e03d86b75d38d1d18035b58e9e43088c9d0d5dd6e49f2c507d949937174f3465
Full Tag: anchore-engine:5000/helloworld-nodejs:beeops-cb-days-5
Image ID: 0b22d7798cd24465252335d602059fea88128244b623bc4af20926eeec8f9b4c
Status: fail
Last Eval: 2019-06-07T12:56:03Z
Policy ID: custom-anchore-policy-nodejs
Final Action: stop
Final Action Reason: policy_evaluation

Gate              Trigger               Detail                                                                                     Status        
dockerfile        effective_user        User root found as effective user, which is explicity not allowed list                     stop          
dockerfile        instruction           Dockerfile directive 'HEALTHCHECK' not found, matching condition 'not_exists' check        warn          

Image Digest: sha256:e03d86b75d38d1d18035b58e9e43088c9d0d5dd6e49f2c507d949937174f3465
Full Tag: anchore-engine:5000/helloworld-nodejs:beeops-cb-days-5
Status: fail
Last Eval: 2019-06-07T12:56:04Z
Policy ID: custom-anchore-policy-nodejs
</code></pre>

<p>As you can see from the above output the scan failed because of the <code>effective_user</code> trigger - <a href="https://github.com/nodejs/docker-node/blob/master/10/alpine/Dockerfile">the official <code>node</code> container image we are using from DockerHub runs as <code>root</code></a> and <a href="https://snyk.io/blog/10-docker-image-security-best-practices/">this is a very bad security practice</a> as it allows <strong>container breakouts</strong> where the container user is able to escape the container namespace and interact with other processes on the host.</p>

<h3 id="some-improvements">Some Improvements</h3>

<ul>
<li>One improvement would be to run this without mounting the Docker socket in the <a href="https://github.com/cloudbees-days/anchore-scan/blob/declarative/dockerClientPod.yml"><code>docker-client</code> container</a>. The Anchore inline-scan script runs a number of Docker commands that requires a Docker daemon - but this is not good security. Using a static Anchore engine would allow us to do container scans without mounting the Docker socket.</li>
<li>Another improvement would be to extend the <code>anchore-scan</code> job to push the container image to a <strong>Prod</strong> container registry on success and notify interested dev teams that their image is now available for production deployments.</li>
</ul>

<h3 id="casc-for-cross-team-collaboration-configuration-for-your-cloudbees-core-v2-masters">CasC for Cross Team Collaboration Configuration for your CloudBees Core v2 Masters</h3>

<p>In order for all of this to work you have to turn on Cross Team Collaboration for all of your Core v2 Masters that you want to publish and subscribe to events. I am a big proponent of CasC for everything so here is an <a href="https://wiki.jenkins.io/display/JENKINS/Post-initialization+script"><code>init.groovy.d</code></a> script to set-up CasC to automatically enable Cross Team Collaboration notifications for your CloudBees Core v2 Masters on start-up:</p>

<p><a href="https://github.com/kypseli/cb-core-mm-workshop/blob/master/quickstart/init_61_notification_api.groovy"><em>cb-core-mm-workshop/quickstart/init_61_notification_api.groovy</em></a>:</p>

<pre><code class="language-groovy">import jenkins.model.Jenkins
import hudson.ExtensionList

import com.cloudbees.jenkins.plugins.notification.api.NotificationConfiguration
import com.cloudbees.jenkins.plugins.notification.spi.Router
import com.cloudbees.opscenter.plugins.notification.OperationsCenterRouter

jenkins = Jenkins.getInstance()

NotificationConfiguration config = ExtensionList.lookupSingleton(NotificationConfiguration.class);
Router r = new OperationsCenterRouter();
        config.setRouter(r);
        config.setEnabled(true);
        config.onLoaded();
</code></pre>

<p>I&rsquo;m also a big fan of the Jenkins Config-as-Code plugin. However, currently, the CloudBees&rsquo; plugins for Cross Team Collaboration do not yet support <a href="https://github.com/jenkinsci/configuration-as-code-plugin">JCasC</a> (but support for JCasC is coming soon).</p>

<h2 id="add-devsecops-to-your-cd-with-cloudbees-now">Add DevSecOps to Your CD with CloudBees Now</h2>

<p>So there&rsquo;s really no excuse NOT to add asynchronous container security scans to your container image CD pipelines with CloudBees Core v2, our Cross Team Collaboration feature and the Anchore <strong>inline scan</strong> - when it is as easy as this!</p>
]]></content>
        </item>
        
        <item>
            <title>Jenkins Plugins: The Good, the Bad and the Ugly</title>
            <link>https://cb-technologists.github.io/posts/jenkins-plugins-good-bad-ugly/</link>
            <pubDate>Thu, 30 May 2019 05:50:46 -0400</pubDate>
            
            <guid>https://cb-technologists.github.io/posts/jenkins-plugins-good-bad-ugly/</guid>
            <description>There are over 1600 Jenkins plugins and that is both a blessing and a curse. Of those 1600 plugins only a small percentage are well maintained and tested, and even fewer (140 of 1600+) are part of the CloudBees Assurance Program (CAP) as verified and/or compatible plugins - well tested to interoperate with the rest of the CAP plugins (and their dependencies) and with a specific LTS version of Jenkins.</description>
            <content type="html"><![CDATA[

<p>There are over <a href="http://updates.jenkins.io/pluginCount.txt">1600 Jenkins plugins</a> and that is both a blessing and a curse. Of those 1600 plugins only a small percentage are well maintained and tested, and even fewer (140 of 1600+) are part of the <a href="https://go.cloudbees.com/docs/cloudbees-documentation/assurance-program/">CloudBees Assurance Program (CAP)</a> as verified and/or compatible plugins - well tested to interoperate with the rest of the CAP plugins (and their dependencies) and with a specific LTS version of Jenkins. Problems can arise when you use plugins that aren&rsquo;t part of CAP, or a plugin that isn&rsquo;t well maintained or tested to work with all of the other plugins you are using and the specific version of Jenkins that you are using. But the extensibility offered by plugins has helped make Jenkins the most popular CI tool on the planet.</p>

<p>I typically like to end posts on a good note, so I will start with <em>The Ugly</em> and end with <em>The Good</em> - and then offer some opinionated ideas/best practices on Jenkins plugin management and usage.</p>

<h1 id="the-ugly">The Ugly</h1>

<p>There are almost always a number of Jenkins plugins that have security vulnerabilities. Over 55 plugins were listed as part of the <a href="https://jenkins.io/security/advisory/2019-04-03/">2019-04-03 Jenkins Security Advisory</a>. Even worse is when you find out that a plugin that you are using has a security vulnerability and you also find out that the plugin is not maintained anymore. You could search the 1600+ Jenkins plugins to see if there is another plugin that is maintained and that does what you need, or you could become a plugin maintainer - not exactly what you intended to sign up for when you first started using Jenkins. Are you developing your own applications or are you looking to become a Jenkins plugin developer?</p>

<p>Another <em>ugly</em> issue arises when you have numerous Jenkins masters in your organization. These Jenkins instances are often snowflakes comprised of many different plug-ins. So managing more than one Jenkins master with disparate sets of plugins can become very ugly, very quickly. CloudBees can certainly help you with this through CAP and something we call <a href="https://go.cloudbees.com/docs/cloudbees-documentation/admin-cje/cje-ux/#_when_to_use_a_team_master_when_to_use_a_managed_master">Team Masters - easily provisioned and managed team specific Jenkins masters</a> with an opinionated set of very stable and tested plugins. However, there is nothing stopping individual Jenkins master admins from manually installing a plugin and sometimes ending up with an unusable Jenkins master.</p>

<h1 id="the-bad">The Bad</h1>

<p>Installing a lot of plugins can result in maintenance hell and sometimes your Jenkins master doesn&rsquo;t even restart successfully after upgrading a plugin.</p>


    <img src="/img/jenkins-plugins-good-bad-ugly/jenkins_devil.png"  alt="Jenkins Devil"  class="left"  />



<p>And although the Jenkins Devil makes for a very cool sticker, it isn&rsquo;t something you ever want to see on <strong>your</strong> Jenkins Master, especially after restarting Jenkins for a plugin update. Backing out a plugin update that causes Jenkins to crash is not a fun thing to deal with and will slow down your software delivery.</p>

<p>Dependency hell is another <em>bad</em> thing that Jenkins admins have to deal with all the time. Sometimes upgrading just one plugin results in the need to update dozens others, and many Jenkins admins do this directly on their production Jenkins master. Blue Ocean, while a noble attempt at a new UI for Jenkins Pipelines, requires dozens of dependencies, many of which you probably have no use for - for example the Blue Ocean plugin suite requires both the <em>Bitbucket Pipeline for Blue Ocean</em> and the <em>GitHub Pipeline for Blue Ocean</em> plugins even if you don&rsquo;t use either Bitbucket or GitHub for source control.</p>

<p>Too many plugins that do the same thing - how do you choose? Search the Jenkins plugin site for <em>Docker</em> and you get 26 results. If I want Docker based agents should I use the <strong>Docker plugin</strong> or <strong>Yet Another Docker plugin</strong>? With 1600+ plugins, sometimes it can be hard to choose the right one.</p>

<h1 id="the-good">The Good</h1>

<p>The extensibility and integrations provided by Jenkins plugins are amazing. I don&rsquo;t believe that there is any other CI platform that integrates with as many source control tools/platforms as Jenkins. Without Jenkins&rsquo; extensive plugin ecosystem it would not be the CI automation tool of choice that it has become. Jenkins is by far the most flexible CI platform available, bar none, and the Jenkins plugin ecosystem is a big reason why.</p>

<p>There are a lot of very <em>good</em>, and even necessary, plugins. Like plugins for credentials and for source control - Jenkins has awesome integration with GitHub and Bitbucket for example. And the Jenkins Pipeline plugin suite (although another example of dependency hell) provides a <a href="https://jenkins.io/doc/book/pipeline/syntax/#declarative-pipeline">Declarative approach to building you CI/CD pipelines</a> that can be <a href="https://jenkins.io/doc/book/pipeline/jenkinsfile/">easily managed as-code in source control</a>. And finally, the <a href="https://jenkins.io/projects/jcasc/">JCasC plugin</a> makes it easier than ever to manager your Jenkins master configuration as-code in source control.</p>

<p>So there are some very <em>good</em> reasons to use <strong>some</strong> plugins.</p>

<h1 id="so-what-to-do">So What to Do</h1>

<p>CloudBees can certainly help. All of the CloudBees distributions, including the <a href="https://www.cloudbees.com/products/cloudbees-jenkins-distribution">free CloudBees Jenkins Distribution</a>, include CAP with Beekeeper. I have managed a few demo/workshop environments for the CloudBees Solution Architecture team for the last 4 years and update those environments almost every month. I have yet to have an update that has resulted in the Jenkins Devil - ok maybe one.</p>

<p>There are a few other things you can do right <strong>now</strong> , whether you use a CloudBees Distro or not, to make using Jenkins Plugins easier to manage and less impactful to your production Jenkins master - allowing you to focus on CD for the applications you are delivering instead of spending too much time managing Jenkins.</p>

<h2 id="use-jenkins-pipeline">Use Jenkins Pipeline</h2>

<p>Although Jenkins Pipeline does require a <a href="https://plugins.jenkins.io/workflow-aggregator">number of plugins and plugin dependencies</a> its advantages far outweigh the disadvantages of using Jenkins without Pipeline jobs. Using Jenkins Pipelines with a Jenkinsfile in source control and <a href="https://jenkins.io/doc/book/pipeline/shared-libraries/">Pipeline Shared Libraries</a> can greatly reduce the number of additional plugins you need to install and manage. For example if you need to send a Slack message, just run a simple <code>curl</code> command in a lightweight container instead of installing the <a href="https://github.com/jenkinsci/slack-plugin/issues">Jenkins Slack plugin</a>:</p>

<pre><code class="language-bash">curl -X POST -H 'Content-type: application/json' --data '{&quot;text&quot;:&quot;The build is broken :(&quot;}' YOUR_WEBHOOK_URL
</code></pre>

<p>This is actually considered a best practice for Jenkins Pipelines as any <code>step</code> that is run from a plugin will actually run on the Jenkins master, not on the agent (other than the <code>sh</code>, <code>bat</code> and <code>pwsh</code> steps). This will result in worse performance for your Jenkins master and may even bring your Jenkins master down - once again slowing down your application delivery.</p>

<p>Another big plus with replacing Jenkins Pipeline plugin based steps with lightweight shell scripts is that it provides easier testing and more portability of your CD pipelines to other platforms. For example, Jenkins X Pipelines with Tekton runs every pipeline step as a command in a container - adopting that approach with Jenkins Pipelines now will make it much easier to migrate to better emerging solutions in the future.</p>

<h2 id="use-fewer-plugins">Use Fewer Plugins</h2>

<p>Using fewer plugs will reduce the amount of pain you will incur from many of the <em>ugly</em> and <em>bad</em> issues mentioned above. Migrating as many Jenkins Pipeline <code>steps</code> from plugins to <code>sh</code> steps running in containers not only reduces the <em>bad</em> and <em>ugly</em> above, it also makes it easier to test and reduce dependencies on the less than stellar plugin maintainers (like me), and provides better portability to other emerging CD technologies - like <a href="https://kurtmadel.com/posts/native-kubernetes-continuous-delivery/jenkins-x-goes-native/#re-tooling-with-tekton">Jenkins X Pipelines with Tekton</a>.</p>

<p>Do you really need the Docker plugin and the Yet Another Docker plugin? Or the Chuck Noris plugin? The fewer plugins that you install, the fewer plugins you have to manage and the less chance that they will have security issues or even worse, bring your Jenkins master down - Jenkins Devil and all.</p>

<h2 id="test">Test</h2>

<p>Always test any new plugin or plugin update before you put it into your production Jenkins master(s). Running Jenkins as a container can certainly make this easier - and is what I suggest - but there is no reason why you can&rsquo;t use Jenkins to automate this kind of testing regardless of how you deploy Jenkins. Just spin up a Jenkins master with a few <em>fake</em> jobs that use the plugins in a similar way to how you use them in your <em>real</em> jobs. All of this can be automated with Jenkins itself.</p>

<p>The <a href="https://github.com/jenkins-x/jenkins-x-serverless-filerunner">Jenkins X ephemeral masters</a> basically went with this approach - extensive testing whenever a new <a href="https://github.com/jenkins-x/jenkins-x-serverless-filerunner/blob/master/pom.xml#L32">plugin was added to the the CasC Master container image</a>.</p>

<h2 id="manage-plugins-with-casc">Manage Plugins with CasC</h2>

<p>Never use the Jenkins UI to install plugins. Maintain your plugins as code in source control, where every new plugin and plugin upgrade can be tracked as commits. The easiest and best way to do this, in my opinion, is to use a customized Docker image that includes the plugins you <strong>absolutely need</strong> - in addition to other configuration via JCasC (and if necessary, <a href="https://wiki.jenkins.io/display/JENKINS/Post-initialization+script"><code>init</code> scripts</a>). If you have read any of my other posts you will know that I am a big fan of containers - and have always run Jenkins with containers since I started at CloudBees back in 2015. The Jenkins GitHub Org <em>docker</em> project <a href="https://github.com/jenkinsci/docker/blob/master/install-plugins.sh">provides a script</a> for <a href="https://github.com/jenkinsci/docker#preinstalling-plugins">preinstalling plugins</a> from a simple <code>plugins.txt</code> file so your Jenkins master container image has all the plugins you need on startup. This makes it easier to test plugin changes and all of your plugin changes are captured as code commits - and a tool like Git (GitHub, BitBucket, even GitLab) is much better at tracking/auditing/controlling such changes than Jenkins was ever meant to be. Here is a simple <code>plugins.txt</code> file and <code>Dockerfile</code> to get you started:</p>

<p><em>plugins.txt</em></p>

<pre><code class="language-txt">configuration-as-code:1.19
credentials:2.2.0
</code></pre>

<p>Yes, only two plugins. The reason why we only need these two plugins is because the <a href="https://www.cloudbees.com/blog/cloudbees-jenkins-distribution-adds-stability-and-security-your-jenkins-environment">CloudBees Jenkins Distribution</a> already contains a curated set of plugins for Jenkins Pipeline, Blue Ocean, source control management and everything else we need - all well tested for us already.</p>

<p>This version of the Credentials plugin is an exception, because the recent version of the plugin with JCasC support has not been integrated into CAP yet (coming soon!).</p>

<p><em>Extending the CloudBees Jenkins Distribution container image with plugins and JCasC</em></p>

<pre><code class="language-Dockerfile">FROM cloudbees/cloudbees-jenkins-distribution:2.164.3.2

# optional, but you might want to let everyone know who is responsible for their Jenkins ;)
LABEL maintainer &quot;kmadel@cloudbees.com&quot;

#set java opts variable to skip setup wizard; plugins will be installed via license activated script
ENV JAVA_OPTS=&quot;-Djenkins.install.runSetupWizard=false&quot;
#skip setup wizard; per https://github.com/jenkinsci/docker/tree/master#preinstalling-plugins
RUN echo 2.0 &gt; /usr/share/jenkins/ref/jenkins.install.UpgradeWizard.state

# diable cli
ENV JVM_OPTS -Djenkins.CLI.disabled=true -server
# set your timezone
ENV TZ=&quot;/usr/share/zoneinfo/America/New_York&quot;

#config-as-code plugin configuration
COPY config-as-code.yml /usr/share/jenkins/config-as-code.yml
ENV CASC_JENKINS_CONFIG /usr/share/jenkins/config-as-code.yml

# use CloudBees' update center to ensure you don't allow any really bad plugins
ENV JENKINS_UC http://jenkins-updates.cloudbees.com

#install suggested and additional plugins
COPY plugins.txt /usr/share/jenkins/ref/plugins.txt
COPY jenkins-support /usr/local/bin/jenkins-support
COPY install-plugins.sh /usr/local/bin/install-plugins.sh
RUN bash /usr/local/bin/install-plugins.sh &lt; /usr/share/jenkins/ref/plugins.txt
</code></pre>

<h1 id="use-plugins-you-need-and-no-more">Use Plugins You Need and No More</h1>

<p>So, don&rsquo;t avoid Jenkins plugins - they are an important part of what makes Jenkins great and add critical features to the way you will use Jenkins - but be smart about the plugins you use and keep your application delivery your primary focus - not your CI tool.</p>
]]></content>
        </item>
        
        <item>
            <title>Extending Jenkins X for Traditional Deployments with CloudBees Flow</title>
            <link>https://cb-technologists.github.io/posts/jenkins-x-flow-integration/</link>
            <pubDate>Wed, 29 May 2019 12:47:46 -0400</pubDate>
            
            <guid>https://cb-technologists.github.io/posts/jenkins-x-flow-integration/</guid>
            <description>Jenkins X is quickly becoming the de facto standard for high performing teams wanting to do CI/CD in a highly scalable and fault tolerant environment. For those who haven’t gotten the opportunity to try out Jenkins X, it allows teams to run CI/CD workloads natively in a Kubernetes environment while taking advantage of modern operating patterns like GitOps and serverless architectures. For teams wanting to modernize their continuous integration and continuous deployment capabilities, Jenkins X is the go to solution.</description>
            <content type="html"><![CDATA[

<p><a href="https://jenkins-x.io">Jenkins X</a> is quickly becoming the de facto standard for high performing teams wanting to do CI/CD in a highly scalable and fault tolerant environment. For those who haven’t gotten the opportunity to try out Jenkins X, it allows teams to run CI/CD workloads natively in a Kubernetes environment while taking advantage of modern operating patterns like GitOps and serverless architectures. For teams wanting to modernize their continuous integration and continuous deployment capabilities, Jenkins X is the go to solution.</p>

<p>In today’s heterogenous technology environment, most organizations tend to have a mix of modern cloud native architectures as well as more traditional workloads which get deployed either on-prem or within the cloud. In the latter case, a combination of Jenkins X (performing CI steps) and CloudBees Flow (handling the deployment) can add a huge amount of flexibility and power to a Continuous Delivery process.  The combination of Jenkins X and CloudBees Flow also brings improved visibility and tracability across the application landscape.</p>

<p>Jenkins X can be easily extended to accommodate any type of workload required - it can be a full end to end CI/CD tool for building, deploying, and running applications all within a Kubernetes cluster, or it can handle CI while offloading other release and deployment tasks to another solution.  In this blog post we’re going to cover how Jenkins X can be extended to offload release/deployment tasks to <a href="https://www.cloudbees.com/cloudbees-acquires-electric-cloud">CloudBees Flow</a>.  We will accomplish this by extending the maven Jenkins X build pack in order to call the CloudBees Flow REST API as part of the Jenkins X pipeline execution.</p>

<h1 id="extending-jenkins-x">Extending Jenkins X</h1>

<p>For the purposes of this blog, we’re going to be focusing on the Jenkins X serverless pipeline execution engine with Tekton (See <a href="https://jenkins-x.io/architecture/jenkins-x-pipelines/">https://jenkins-x.io/architecture/jenkins-x-pipelines/</a>). There are two main ways to customize a Jenkins X pipeline in order to integrate with CloudBees Flow.  The first and simplest would be to modify the jenkins-x.yml (more information on Jenkins X pipelines: <a href="https://jenkins-x.io/architecture/jenkins-x-pipelines/#differences-to-jenkins-pipelines">https://jenkins-x.io/architecture/jenkins-x-pipelines/#differences-to-jenkins-pipelines</a> and the jenkins-x.yml file) pipeline file in the source code repo for the project we’re going to build.  The other way is to extend the <a href="https://jenkins-x.io/architecture/build-packs/">Jenkins X build packs</a> and modify the build pack for the language/build tool you want to use.  Both will work, but by forking the build packs you can get reuse across multiple projects using the build pack you extend. In this example, we’ll walk through how to extend the Jenkins X build packs.</p>

<h2 id="creating-our-cluster-and-installing-jenkins-x">Creating our Cluster and Installing Jenkins X</h2>

<p>To start, we’ll fork the Jenkins X Kubernetes build packs into our own repository: <a href="https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes">https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes</a>.  Later we&rsquo;ll be extending the maven build pack to support a REST API call into CloudBees Flow.</p>

<p>Now it’s time to create a Kubernetes cluster on GKE using Jenkins X and <a href="https://github.com/tektoncd/pipeline">Tekton</a>.  In this case, we&rsquo;re starting by creating a cluster from scratch, but Jenkins X can also be installed into an existing Kubernetes cluster if you already have one available by using the <code>jx install</code> command:</p>

<pre><code class="language-bash">jx create cluster gke --tekton --no-tiller
</code></pre>

<p>Fill out the options.  For example:</p>

<pre><code class="language-shell">$  jx create cluster gke
Your browser has been opened to visit:

    https://accounts.google.com/o/oauth2/auth?redirect_uri=....


? Google Cloud Project: jhendrick-ckcd
Updated property [core/project].
Lets ensure we have container and compute enabled on your project
No apis need to be enable as they are already enabled: container compute
No cluster name provided so using a generated one: crownprong
? What type of cluster would you like to create Zonal
? Google Cloud Zone: us-west1-a
? Google Cloud Machine Type: n1-standard-4
? Minimum number of Nodes (per zone) 3
? Maximum number of Nodes 5
? Would you like use preemptible VMs? No
? Would you like to access Google Cloud Storage / Google Container Registry? No
Creating cluster...
Initialising cluster ...
? Select Jenkins installation type: Serverless Jenkins X Pipelines with Tekton
Setting the dev namespace to: jx
Namespace jx created 
</code></pre>

<p>Create an ingress controller if one doesn’t exist and setup the domain or use the default *.nip.io address if you don’t have one.  Go through the prompts and then configure your GitHub credentials.  Create an API token using the URL provided if you don’t have one:</p>

<pre><code class="language-shell">If you don't have a wildcard DNS setup then setup a DNS (A) record and point it at: 35.197.85.1 then use the DNS domain in the next input...
? Domain 35.197.85.1.nip.io
nginx ingress controller installed and configured
? Would you like to enable Long Term Storage? A bucket for provider gke will be created No
Lets set up a Git user name and API token to be able to perform CI/CD

Creating a local Git user for GitHub server
? GitHub user name: jhendrick
To be able to create a repository on GitHub we need an API Token
Please click this URL https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo

Then COPY the token and enter in into the form below:

? API Token: ****************************************
Select the CI/CD pipelines Git server and user
? Do you wish to use GitHub as the pipelines Git server: Yes
Creating a pipelines Git user for GitHub server
To be able to create a repository on GitHub we need an API Token
Please click this URL https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo

Then COPY the token and enter in into the form below:

? API Token: ****************************************
Setting the pipelines Git server https://github.com and user name jhendrick.
Saving the Git authentication configuration
</code></pre>

<p>In the setup we’re going to choose the Kubernetes workloads option and later modify the kubernetes workload build packs to include the CloudBees Flow specific steps:</p>

<pre><code class="language-shell">? Pick default workload build pack: [Use arrows to move, space to select, type to filter]
&gt; Kubernetes Workloads: Automated CI+CD with GitOps Promotion
Library Workloads: CI+Release but no CD
</code></pre>

<h2 id="editing-the-build-packs">Editing the Build Packs</h2>

<p>You can use your favorite IDE but in this case, we&rsquo;ll modify the Jenkins X build packs in VS Code with the YAML Language extension installed (<a href="https://jenkins-x.io/architecture/jenkins-x-pipelines/#editing-in-vs-code">https://jenkins-x.io/architecture/jenkins-x-pipelines/#editing-in-vs-code</a>) for validation as recommended by the Jenkins X team.</p>

<p>This example is going to focus on a sample Spring Boot application using Maven.  To start we&rsquo;ll modified the maven build pack in our forked build pack repo (<a href="https://github.com/jhendrickCB/jenkins-x-kubernetes/blob/master/packs/maven/pipeline.yaml):">https://github.com/jhendrickCB/jenkins-x-kubernetes/blob/master/packs/maven/pipeline.yaml):</a></p>

<pre><code class="language-yaml">extends:
 import: classic
 file: maven/pipeline.yaml
pipelines:
 release:
   build:
     steps:
     - sh: jx step post build --image $DOCKER_REGISTRY/$ORG/$APP_NAME:\$(cat VERSION)
       name: post-build
   promote:
     steps:
     - sh: jx step changelog --version v\$(cat ../../VERSION)
       name: changelog
     - comment: call CloudBees Flow to run a release
       sh: &gt;
         curl -X POST --header &quot;Authorization: Basic $(jx step credential -s flow-token -k token)&quot; --header &quot;Content-Type: application/json&quot; --header &quot;Accept: application/json&quot; -d &quot;{}&quot; &quot;https://ps9.ecloud-kdemo.com/rest/v1.0/pipelines?pipelineName=my_pipeline&amp;projectName=my_project&quot; --insecure
       name: cloudbees-flow-release
</code></pre>

<p>Compare the original build pack for maven found here: <a href="https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs/maven">https://github.com/jenkins-x-buildpacks/jenkins-x-kubernetes/tree/master/packs/maven</a> vs. our forked build pack.  We’ve removed all the references to skaffold, watch, and helm since we’re no longer having Jenkin’s X handle the deployment to our Kubernetes cluster.  We’ve also updated the pipeline file to make an API call into our CloudBees Flow server using <a href="https://curl.haxx.se/">cURL</a>:</p>

<pre><code class="language-bash">curl -X POST --header &quot;Authorization: Basic $(jx step credential -s flow-token -k token)&quot; --header &quot;Content-Type: application/json&quot; --header &quot;Accept: application/json&quot; -d &quot;{}&quot; &quot;https://ps9.ecloud-kdemo.com/rest/v1.0/pipelines?pipelineName=my_pipeline&amp;projectName=my_project&quot; --insecure
</code></pre>

<p>The above API call into CloudBees Flow tells Flow to run a pipeline called <code>my_pipeline</code> within a project called <code>my_project</code>.</p>

<p>You’ll also notice that we’re using a Jenkins X feature (<code>jx step credential</code>) to get our secret, <code>flow-token</code>, which we created previously so that we can authenticate to the Flow Rest API. Note that there are many other possible ways to call into the CloudBees Flow API’s besides cURL such as the command line tool <a href="http://docs.electric-cloud.com/eflow_doc/9_0/API/HTML/FlowAPI_Guide_9_0.htm#EFlow_api/usingAPI.htm?Highlight=ectool">ectool</a> as well as <a href="http://docs.electric-cloud.com/eflow_doc/9_0/API/HTML/FlowAPI_Guide_9_0.htm#EFlow_api/usingAPI.htm%3FTocPath%3DUsing%2520the%25C2%25A0ElectricFlow%2520Perl%2520API%7C_____0">perl</a> or <a href="http://docs.electric-cloud.com/eflow_doc/9_0/API/HTML/FlowAPI_Guide_9_0.htm#EFlow_api/UsingGroovy.htm%3FTocPath%3D_____11">groovy libraries</a>.  Also note that for a production environment we would want to setup the proper certificates rather than using the <code>--insecure parameter</code>.</p>

<p>Next, we need to tell Jenkins X to use our new build pack:</p>

<pre><code class="language-shell">$ jx edit buildpack -u https://github.com/jhendrickCB/jenkins-x-kubernetes -r master -b

Setting the team build pack to  repo: https://github.com/jhendrickCB/jenkins-x-kubernetes ref: master
</code></pre>

<p>Since we have to authenticate when calling the <a href="http://docs.electric-cloud.com/eflow_doc/9_0/API/HTML/FlowAPI_Guide_9_0.htm">Flow REST API</a>, we’ll create a Kubernetes secret to store our username/password basic authentication token:</p>

<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
 name: flow-token
type: Opaque
data:
 token: &lt;Basic Auth Token&gt;
</code></pre>

<p>Note: In this case, the <code>&lt;Basic Auth Token&gt;</code> will take the form of <code>username:password</code> base64 encoded.  Take note that we’ll actually need to base64 encode our username:password token twice as it will get base64 decoded automatically when we access it later.</p>

<p>To apply the secret in our Kubernetes cluster, we can save our secret to a file called <code>flow-token-secret.yaml</code> and run the command:</p>

<pre><code class="language-bash">kubectl apply -f flow-token-secret.yaml
</code></pre>

<h1 id="creating-a-sample-spring-boot-project">Creating a Sample Spring Boot Project</h1>

<p>To test out our new build pack, we’ll use Jenkins X’s capability to create a quick start project for a Spring Boot microservice:</p>

<pre><code class="language-bash">jx create spring -d web -d actuator
</code></pre>

<p>Follow the prompts to create the Spring Boot project and setup the repository on your GitHub account:</p>

<pre><code class="language-shell">$ jx create spring -d web -d actuator
Using Git provider GitHub at https://github.com
? Do you wish to use jhendrick as the Git user name? Yes


About to create repository  on server https://github.com with user jhendrick
? Which organisation do you want to use? jhendrickCB
? Enter the new repository name:  jx-spring-flowdemo


Creating repository jhendrickCB/jx-spring-flowdemo
? Language: java
? Group: com.example
Created Spring Boot project at /Users/jhendrick/Cloudbees/jx-spring-flowdemo
The directory /Users/jhendrick/Cloudbees/jx-spring-flowdemo is not yet using git
? Would you like to initialise git now? Yes
? Commit message:  Initial import

Git repository created
selected pack: /Users/jhendrick/.jx/draft/packs/github.com/jhendrickCB/jenkins-x-kubernetes/packs/maven

replacing placeholders in directory /Users/jhendrick/Cloudbees/cloudbees-days/kops-cluster/jx-spring-flowdemo
app name: jx-spring-flowdemo, git server: github.com, org: jhendrickcb, Docker registry org: jhendrickcb
skipping directory &quot;/Users/jhendrick/Cloudbees/jx-spring-flowdemo/.git&quot;
skipping ignored file &quot;/Users/jhendrick/Cloudbees/jx-spring-flowdemo/HELP.md&quot;
Pushed Git repository to https://github.com/jhendrickCB/jx-spring-flowdemo

Creating GitHub webhook for jhendrickCB/jx-spring-flowdemo for url http://hook.jx.35.197.85.1.nip.io/hook

Watch pipeline activity via:    jx get activity -f jx-spring-flowdemo -w
Browse the pipeline log via:    jx get build logs jhendrickCB/jx-spring-flowdemo/master
Open the Jenkins console via    jx console
You can list the pipelines via: jx get pipelines
When the pipeline is complete:  jx get applications

For more help on available commands see: https://jenkins-x.io/developing/browsing/

Note that your first pipeline may take a few minutes to start while the necessary images get downloaded!
</code></pre>

<p>Once created, the project should build and run automatically.  If everything worked, we should see our Spring Boot project built with Maven, artifacts uploaded automatically to our Nexus repository and then our CloudBees Flow pipeline executed within our CloudBees Flow server.</p>

<p>If for some reason, we made a mistake, the pipeline can be re-run by using:</p>

<pre><code class="language-bash">jx start pipeline
</code></pre>

<p>To debug, build logs can be checked with:</p>

<pre><code class="language-bash">jx get build logs 
</code></pre>

<p>Or more specifically with our project name:</p>

<pre><code class="language-bash">jx get build logs jhendrickCB/jx-spring-flowdemo/master
</code></pre>

<p>We can get build activity with:</p>

<pre><code class="language-bash">jx get activity -w
</code></pre>

<p>Or more specifically:</p>

<pre><code class="language-bash">jx get activity -f jx-spring-flowdemo -w
</code></pre>

<h1 id="in-conclusion">In Conclusion</h1>

<p>In the above example we were able to use Jenkins X to build our application as well as store the built artifacts, and then utilize CloudBees flow to handle execution of our release pipeline.  This allows us to take advantage of the scalability and efficiency of Jenkins X while leveraging the power and control of CloudBees Flow for managing the release.</p>

<p>For organizations who want to take advantage of modern CI/CD on Jenkins X but are not yet &ldquo;all in&rdquo; on Kubernetes and still deploying traditional applications, this provides a very solid approach to achieving Continuous Delivery.</p>
]]></content>
        </item>
        
        <item>
            <title>Introducing the Technologists, A CloudBees Solution Architecture Team</title>
            <link>https://cb-technologists.github.io/posts/introducing-technologists/</link>
            <pubDate>Thu, 23 May 2019 19:10:46 -0400</pubDate>
            
            <guid>https://cb-technologists.github.io/posts/introducing-technologists/</guid>
            <description>The Technologists is a new team of CloudBees Solution Architects. Technologists have a passion for emerging technologies, continuously learning and teaching through thought leadership, providing technical direction within CloudBees and the broader tech community, and driving the best technical solutions for customers.
 Technical integrity is of the utmost importance for a Technologist - always providing the RIGHT solution. We are Technologists focused on providing best practices, solutions, and adoption paths to organizations navigating software delivery transformations with leading edge technologies.</description>
            <content type="html"><![CDATA[<p>The Technologists is a new team of CloudBees Solution Architects. Technologists have a passion for emerging technologies, continuously learning and teaching through thought leadership, providing technical direction within CloudBees and the broader tech community, and driving the best technical solutions for customers.</p>

<ul>
<li>Technical integrity is of the utmost importance for a Technologist - always providing the RIGHT solution.</li>
<li>We are Technologists focused on providing best practices, solutions, and adoption paths to organizations navigating software delivery transformations with leading edge technologies.</li>
<li>Technologists are Thought Leaders internally at CloudBees and in the DevOps and wider Tech community - writing blog posts, speaking at conferences and meetups, contributing to open source projects on technologies and technical practices related to CloudBees&rsquo; products and to the DevOps space in general.</li>
</ul>

<p>From Cloud Native CD to microservice and even nanoservice architecture to service meshes and API gateways, Technologists are early adopters of the best of the best emerging technologies related to software delivery. Look to this website for interesting posts on a number of technical subjects related to CloudBees&rsquo; products and DevOps in general, like:</p>

<ul>
<li>Native Kubernetes Continuous Delivery</li>
<li>Jenkins Plugins: The Good, the Bad and the Ugly</li>
<li>The State of DevOps Analytics</li>
</ul>

<p>Stay tuned for more&hellip;</p>
]]></content>
        </item>
        
    </channel>
</rss>
